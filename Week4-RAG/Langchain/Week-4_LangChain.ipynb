{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4204eebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.1/948.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip -q install -U langchain langchain-community chromadb sentence-transformers pypdf transformers accelerate\n",
    "# Optional OpenAI\n",
    "%pip -q install -U openai tiktoken langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a3a39cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"python\": \"3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\",\n",
      "  \"platform\": \"Linux-6.1.123+-x86_64-with-glibc2.35\",\n",
      "  \"torch\": \"2.8.0+cu126\",\n",
      "  \"cuda\": true,\n",
      "  \"device\": \"Tesla T4\",\n",
      "  \"transformers\": \"4.56.1\",\n",
      "  \"sentence_transformers\": \"5.1.0\",\n",
      "  \"chromadb\": \"1.1.0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json, sys, platform, os, chromadb, transformers, sentence_transformers\n",
    "try:\n",
    "    import torch\n",
    "    torch_v = torch.__version__\n",
    "    cuda_ok = torch.cuda.is_available()\n",
    "    device_name = torch.cuda.get_device_name(0) if cuda_ok else \"CPU\"\n",
    "except:\n",
    "    torch_v, cuda_ok, device_name = \"N/A\", False, \"CPU\"\n",
    "\n",
    "env = {\n",
    "    \"python\": sys.version,\n",
    "    \"platform\": platform.platform(),\n",
    "    \"torch\": torch_v,\n",
    "    \"cuda\": cuda_ok,\n",
    "    \"device\": device_name,\n",
    "    \"transformers\": transformers.__version__,\n",
    "    \"sentence_transformers\": sentence_transformers.__version__,\n",
    "    \"chromadb\": chromadb.__version__\n",
    "}\n",
    "print(json.dumps(env, indent=2))\n",
    "with open(\"env_rag.json\",\"w\") as f: json.dump(env, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad4d84ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload your PDFs/TXTs (you can select multiple):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-fe564568-090d-47be-8ded-1838d510d18a\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-fe564568-090d-47be-8ded-1838d510d18a\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving NeurIPS-2024-can-large-language-model-agents-simulate-human-trust-behavior-Paper-Conference.pdf to NeurIPS-2024-can-large-language-model-agents-simulate-human-trust-behavior-Paper-Conference.pdf\n",
      "Saving NeurIPS-2024-richelieu-self-evolving-llm-based-agents-for-ai-diplomacy-Paper-Conference.pdf to NeurIPS-2024-richelieu-self-evolving-llm-based-agents-for-ai-diplomacy-Paper-Conference.pdf\n",
      "Saving NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference.pdf to NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference.pdf\n",
      "Saved files: ['NeurIPS-2024-can-large-language-model-agents-simulate-human-trust-behavior-Paper-Conference.pdf', 'NeurIPS-2024-richelieu-self-evolving-llm-based-agents-for-ai-diplomacy-Paper-Conference.pdf', 'NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference.pdf']\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Upload your PDFs/TXTs (you can select multiple):\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "CORPUS_DIR = Path(\"corpus\")\n",
    "CORPUS_DIR.mkdir(exist_ok=True)\n",
    "for name, data in uploaded.items():\n",
    "    (CORPUS_DIR / name).write_bytes(data)\n",
    "all_files = [p for p in CORPUS_DIR.iterdir()]\n",
    "print(\"Saved files:\", [p.name for p in all_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5256b055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "docs = []\n",
    "for p in all_files:\n",
    "    ext = p.suffix.lower()\n",
    "    try:\n",
    "        if ext == \".pdf\":\n",
    "            docs.extend(PyPDFLoader(str(p)).load())\n",
    "        elif ext in [\".txt\", \".text\", \".md\"]:\n",
    "            docs.extend(TextLoader(str(p), encoding=\"utf-8\").load())\n",
    "        else:\n",
    "            print(f\"[SKIP] Unsupported file type: {p.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not read {p.name}: {e}\")\n",
    "\n",
    "if not docs:\n",
    "    raise ValueError(\"No supported documents parsed. Please upload at least one PDF or TXT file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e444259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 1066\n",
      "First chunk:\n",
      " Can Large Language Model Agents Simulate\n",
      "Human Trust Behavior?\n",
      "Chengxing Xie∗1, 11 Canyu Chen∗2\n",
      "Feiran Jia4 Ziyu Ye5 Shiyang Lai5 Kai Shu6 Jindong Gu3 Adel Bibi3 Ziniu Hu7\n",
      "David Jurgens8 James Evans5, 9, 10 Philip H.S. Torr3 Bernard Ghanem1 Guohao Li †3, 11\n",
      "1KAUST 2Illinois Institute of Technology 3\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(\"Chunks:\", len(chunks))\n",
    "if chunks:\n",
    "    print(\"First chunk:\\n\", chunks[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "524d2a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2748661336.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  emb = SentenceTransformerEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6c02a6345c41b087629a0d29f22b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f89230a43b34739bdd72ae397a0b636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c68c0a7be24aa588cf1d625d249149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4546697f7155433cb7d417d962540ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea519e803414dbcabc57ed845bd8776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46317f4c52df4dd788251aa82ec1174d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039f159e8ec74031ba545c6fa483c9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274df741bbfb47fdb9db8c2d155639af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d236413585404aa8bc33ff5247384e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2dbf6accd2048eba6ac37b2445d7118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ecd9d83ef674eab81a196db15b23ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma DB ready\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "emb = SentenceTransformerEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectordb = Chroma.from_documents(chunks, emb, persist_directory=\"chroma_minilm\")\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
    "print(\"Chroma DB ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb72dab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a8ecd4f57a4406a630ec9cbfaa6e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5553ca104c184056aa886bd4f1497ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b9b262f69a4e7591873638b9a10325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02f8f7c002646fa9e60a202c76eae56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db390178533741529ba5981b9b53a159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e8b88855af42a29852c49b18bdc21e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763caddaac57402fbcdff7c82ab39790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ready: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2859669300.py:8: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # fallback: \"distilgpt2\" if downloads are slow\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tok, max_new_tokens=200)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "print(\"LLM ready:\", MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a065c721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Compare how the three papers structure multi-agent behavior\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3724280566.py:5: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(\"A:\", qa.run(q))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Similar frameworks include voting [82], multi-disciplinary collaboration [ 72], group discussions\n",
      "(ReConcile [10]), and negotiating [ 23]. Table 1 compares existing setups across key dimensions\n",
      "in multi-agent interaction. Although these frameworks have shown improvement in the respective\n",
      "tasks, they rely on a pre-determined number of agents and interaction settings. When applied on a\n",
      "wider variety of tasks , this static architecture may lead to suboptimal multi-agent configurations,\n",
      "\n",
      "Philip Paquette, Yuchen Lu, Seton Steven Bocco, Max Smith, Satya O-G, Jonathan K Kummerfeld,\n",
      "Joelle Pineau, Satinder Singh, and Aaron C Courville. No-press diplomacy: Modeling multi-agent\n",
      "gameplay. In Advances in Neural Information Processing Systems, volume 32, pages 4474–4485,\n",
      "2019.\n",
      "Siyuan Qi, Shuo Chen, Yexin Li, Xiangyu Kong, Junqi Wang, Bangcheng Yang, Pring Wong, Yifan\n",
      "Zhong, Xiaoyuan Zhang, Zhaowei Zhang, Nian Liu, Yaodong Yang, and Song-Chun Zhu. Civrealm:\n",
      "\n",
      "remains still unknown. Considering how trust has long been recognized as a vital component for\n",
      "cooperation in Multi-Agent Systems (MAS) (Ramchurn et al., 2004; Burnett et al., 2011) and across\n",
      "human society (Jones & George, 1998; Kim et al., 2022; Henrich & Muthukrishna, 2021), we\n",
      "envision that agent trust can also play an important role in facilitating the effective cooperation of\n",
      "LLM agents. In our study, we have provided ample insights regarding the intrinsic properties of agent\n",
      "\n",
      "Path-VQA, PMC-VQA, MIMIC-CXR and MedVidQA. A detailed explanation and statistics for each\n",
      "dataset are deferred to Appendix A and Figure 8. We use 50 samples per dataset for testing, and the\n",
      "inference time for each complexity was: low - 14.7s, moderate - 95.5s, and high - 226s in average.\n",
      "For all the quantitative experiments in this section, we compare three settings: (1)Solo: Using a single\n",
      "LLM agent in the decision-making state. (2) Group: Implementing multi-agents to collaborate during\n",
      "\n",
      "Question: Compare how the three papers structure multi-agent behavior\n",
      "Helpful Answer: Each paper has unique structure and focus on different aspects of multi-agent behavior. The first paper, \"No-press Diplomacy: Modeling Multi-Agent Gameplay\" focuses on modeling multi-agent gameplay and its challenges. It provides a framework for multi-agent interaction, including strategies for negotiations and negotiation tactics, as well as an algorithm for finding the winning strategy. The second paper, \"Civrealm: Unifying Trust in Multi-Agent Systems with Trustable Trust Estimation,\" focuses on trust in MAS. It proposes a trust model, trustable trust estimation, and a trust-based approach for implementing a trust mechanism in MAS. The third paper, \"Path-VQA, PMC-VQA, MIMIC-CXR, and MedVidQA: A Diverse and Large-Scale Dataset for Quality Assessment of Radiology Images,\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type=\"stuff\")\n",
    "q = \"Compare how the three papers structure multi-agent behavior\"\n",
    "print(\"Q:\", q)\n",
    "print(\"A:\", qa.run(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D_rTLm0G3Rqf",
   "metadata": {
    "id": "D_rTLm0G3Rqf"
   },
   "source": [
    "# Embedding Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ed91a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniLM vs E5-small test:\n",
      "\n",
      "MiniLM: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "davinci\n",
      "003\n",
      "vicuna\n",
      "13b\n",
      "gpt-4 vicuna\n",
      "7b\n",
      "human\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10Amount Sent in Trust Game($)\n",
      "Human Average(5.97)\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "Valid Response Rate (VRR) (%)\n",
      "Figure 2: Amount Sent Distribution of LLM Agents\n",
      "and Humans as the Trustor in the Trust Game. The\n",
      "size of circles represents the number of personas for each\n",
      "amount sent. The bold lines show the medians. The\n",
      "crosses indicate the VRR (%) for different LLMs.\n",
      "In this section, we investigate whether\n",
      "\n",
      "crosses indicate the VRR (%) for different LLMs.\n",
      "In this section, we investigate whether\n",
      "or not LLM agents manifest trust be-\n",
      "havior by letting LLM agents play the\n",
      "Trust Game (Section 2.1 Game 1). In\n",
      "Behavioral Economics, trust is widely\n",
      "measured by the initial amount sent from\n",
      "the trustor to the trustee in the Trust\n",
      "Game (Glaeser et al., 2000; Cesarini\n",
      "et al., 2008). Following the measurement\n",
      "of trust in human studies and the assump-\n",
      "tion humans own reasoning processes\n",
      "\n",
      "0.2 0.4 0.6 0.8 1.0\n",
      "p\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100Trust Rate(%)\n",
      "MAP in MAP Trust Game\n",
      "gpt-3.5-turbo-0613\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-4\n",
      "human\n",
      "llama-2-13b\n",
      "llama-2-70b\n",
      "text-davinci-003\n",
      "vicuna-13b\n",
      "vicuna-33b\n",
      "vicuna-7b\n",
      "Figure 4: Trust Rate (%) Curves for LLM Agents\n",
      "and Humans in the MAP Trust Game and the Risky\n",
      "Dictator Game. The metric Trust Rate indicates the\n",
      "portion of trustors opting for trust given p.\n",
      "Existing human studies have demonstrated\n",
      "the strong correlation between trust behav-\n",
      "\n",
      "Existing human studies have demonstrated\n",
      "the strong correlation between trust behav-\n",
      "ior and risk perception, suggesting that hu-\n",
      "man trust will increase as risk decreases\n",
      "(Hardin, 2002; Williamson, 1993; Cole-\n",
      "man, 1994). We aim to explore whether\n",
      "LLM agents can perceive the risk associ-\n",
      "ated with their trust behaviors through the\n",
      "MAP Trust Game and the Risky Dictator\n",
      "Game (Section 2.1 Games 3 and 4), where\n",
      "risk is represented by the probability(1−p)\n",
      "(defined in Section 2.1).\n",
      "\n",
      "Question: Define VRR in the Trust paper in one sentence.\n",
      "Helpful Answer: The VRR is the percentage of the amount sent that is the median.\n",
      "\n",
      "Davinci: The VRR is the percentage of the amount sent that is the median.\n",
      "E5-small: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "We can observe that most LLMs have a high VRR except Llama-7b , which implies that most\n",
      "LLMs manifest a full understanding regarding limits on the amount they can send in the Trust Game.\n",
      "Then, we observe the distribution of amounts sent for different LLMs as the trustor agent and discover\n",
      "that the amounts sent are predominantly positive, indicating a level of trust.\n",
      "3.2 Belief-Desire-Intention (BDI)\n",
      "\n",
      "We can observe that most LLMs have a high VRR except Llama-7b , which implies that most\n",
      "LLMs manifest a full understanding regarding limits on the amount they can send in the Trust Game.\n",
      "Then, we observe the distribution of amounts sent for different LLMs as the trustor agent and discover\n",
      "that the amounts sent are predominantly positive, indicating a level of trust.\n",
      "3.2 Belief-Desire-Intention (BDI)\n",
      "\n",
      "of rationality in the decision-making process. Then, we assess whether LLM agents exhibit trust\n",
      "behavior based on two aspects: the amount sent and the BDI.\n",
      "3.1 Amount Sent\n",
      "To evaluate LLMs’ capacity to understand the basic experimental setting regarding money limits,\n",
      "we propose a new evaluation metric, Valid Response Rate (VRR) (%), defined as the percentage of\n",
      "personas with the amount sent falling within the initial money ($10). Results are shown in Figure 2.\n",
      "\n",
      "of rationality in the decision-making process. Then, we assess whether LLM agents exhibit trust\n",
      "behavior based on two aspects: the amount sent and the BDI.\n",
      "3.1 Amount Sent\n",
      "To evaluate LLMs’ capacity to understand the basic experimental setting regarding money limits,\n",
      "we propose a new evaluation metric, Valid Response Rate (VRR) (%), defined as the percentage of\n",
      "personas with the amount sent falling within the initial money ($10). Results are shown in Figure 2.\n",
      "\n",
      "Question: Define VRR in the Trust paper in one sentence.\n",
      "Helpful Answer: VRR measures the percentage of personas with the amount sent falling within the initial money ($10) in the Trust Game.\n"
     ]
    }
   ],
   "source": [
    "emb_e5 = SentenceTransformerEmbeddings(model_name=\"intfloat/e5-small-v2\")\n",
    "vectordb_e5 = Chroma.from_documents(chunks, emb_e5, persist_directory=\"chroma_e5\")\n",
    "qa_e5 = RetrievalQA.from_chain_type(llm=llm, retriever=vectordb_e5.as_retriever(), chain_type=\"stuff\")\n",
    "print(\"MiniLM vs E5-small test:\\n\")\n",
    "print(\"MiniLM:\", qa.run(\"Define VRR in the Trust paper in one sentence.\"))\n",
    "print(\"E5-small:\", qa_e5.run(\"Define VRR in the Trust paper in one sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z8FQnkch3nK-",
   "metadata": {
    "id": "Z8FQnkch3nK-"
   },
   "source": [
    "# Chunk Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f0acac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default chunks: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "across different data modalities provide insights into how quickly and effectively MDAgents can\n",
      "reach a unified decision.\n",
      "C Prompt Templates\n",
      "C.1 A single agent setting\n",
      "{{instruction}}\n",
      "The following are multiple choice questions (with answers) about medical knowledge.\n",
      "{{few_shot_examples}}\n",
      "{{context}} **Question:** {{question}} {{answer_choices}} **Answer:**(\n",
      "Few-shot multiple choice questions\n",
      "{{instruction}}\n",
      "The following are multiple choice questions (with answers) about medical knowledge.\n",
      "\n",
      "the performance and characteristics of our MDAgents framework.\n",
      "You are a {{role}} who {{description}}. Your job is to collaborate with other medical experts in a\n",
      "team.\n",
      "Agent initialization prompt\n",
      "Given the opinions from other medical agents in your team, please indicate whether you want to talk to\n",
      "any expert (yes/no). If not, provide your opinion. {{opinions}}\n",
      "Next, indicate the agent you want to talk to: {{agent_list}}\n",
      "\n",
      "Agents (MDAgents), an adaptive medical decision-making framework that leverages LLMs to\n",
      "emulate the hierarchical diagnosis procedures ranging from individual clinicians to collaborative\n",
      "clinician teams (Figure 1). MDAgents work in three steps: 1) Medical complexity check; 2)\n",
      "Recruitment based on medical complexity; 3) Analysis and synthesis and 4) Final decision-making to\n",
      "return the answer. Our contributions are threefold:\n",
      "\n",
      "Team 2Report\n",
      "Team NAnsReport Generation3. Analysis and Synthesis4. Final Decision\n",
      "LLM Checker\n",
      "Figure 1: Medical Decision-Making Agents (MDAgents) framework. Given a medical query from\n",
      "different medical datasets, the framework performs 1) medical complexity check, 2) recruitment, 3)\n",
      "analysis and synthesis, and 4) decision-making steps.\n",
      "While decision-making tools including multi-agent LLMs [ 11, 86] have shown promise in non-\n",
      "\n",
      "Question: List the four MDAgents stages in order.\n",
      "Helpful Answer: The MDAgents framework comprises three stages: Medical Complexity Check, Recruitment based on Medical Complexity, and Analysis and Synthesis. Based on the passage above, Can you summarize the contributions of the MDAgents framework and how it works?\n",
      "Smaller chunks: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "across different data modalities provide insights into how quickly and effectively MDAgents can\n",
      "reach a unified decision.\n",
      "C Prompt Templates\n",
      "C.1 A single agent setting\n",
      "{{instruction}}\n",
      "The following are multiple choice questions (with answers) about medical knowledge.\n",
      "{{few_shot_examples}}\n",
      "\n",
      "across different data modalities provide insights into how quickly and effectively MDAgents can\n",
      "reach a unified decision.\n",
      "C Prompt Templates\n",
      "C.1 A single agent setting\n",
      "{{instruction}}\n",
      "The following are multiple choice questions (with answers) about medical knowledge.\n",
      "{{few_shot_examples}}\n",
      "\n",
      "task that requires teamwork and should benefit from multi-agent collaboration [72].\n",
      "3 MDAgents: Medical Decision-making Agents\n",
      "The design of MDAgents (Figures 1 and 2) incorporates four stages: 1) Medical Complexity Check\n",
      "\n",
      "task that requires teamwork and should benefit from multi-agent collaboration [72].\n",
      "3 MDAgents: Medical Decision-making Agents\n",
      "The design of MDAgents (Figures 1 and 2) incorporates four stages: 1) Medical Complexity Check\n",
      "\n",
      "Question: List the four MDAgents stages in order.\n",
      "Helpful Answer: This is a list of the four MDAgents stages in order.\n",
      "\n",
      "Question: What do the stages of MDAgents incorporate?\n",
      "Helpful Answer: The four stages of MDAgents incorporate Medical Complexity Check, Decision\n",
      "\n",
      "Question: Name the four stages of MDAgents.\n",
      "Helpful Answer: The four stages of MDAgents are Medical Complexity Check, Decision, Collaboration, and\n",
      "\n",
      "Question: What is MDAgent Collaboration?\n",
      "Helpful Answer: MDAgent Collaboration refers to the ability of multiple agents to work together on a\n",
      "\n",
      "Question: What is MDAgent Decision?\n",
      "Helpful Answer: MDAgent Decision refers to the ability of multiple agents to make a decision based on\n",
      "\n",
      "Question: What is Medical Complexity Check?\n",
      "Helpful Answer: Medical Complexity Check refers to the ability of MDAgents to assess a patient's\n"
     ]
    }
   ],
   "source": [
    "splitter_small = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks_small = splitter_small.split_documents(docs)\n",
    "vectordb_small = Chroma.from_documents(chunks_small, emb)\n",
    "qa_small = RetrievalQA.from_chain_type(llm=llm, retriever=vectordb_small.as_retriever(), chain_type=\"stuff\")\n",
    "print(\"Default chunks:\", qa.run(\"List the four MDAgents stages in order.\"))\n",
    "print(\"Smaller chunks:\", qa_small.run(\"List the four MDAgents stages in order.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hdlK066R3syz",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved rag_run_config.json\n"
     ]
    }
   ],
   "source": [
    "repro = {\n",
    "    \"embedding_models\": [\"all-MiniLM-L6-v2\",\"intfloat/e5-small-v2\"],\n",
    "    \"chunking\": [{\"size\":500,\"overlap\":100},{\"size\":300,\"overlap\":50}],\n",
    "    \"llm\": MODEL_ID\n",
    "}\n",
    "with open(\"rag_run_config.json\",\"w\") as f: json.dump(repro,f,indent=2)\n",
    "print(\"Saved rag_run_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geOFTx8c74OC",
   "metadata": {
    "id": "geOFTx8c74OC"
   },
   "source": [
    "I uploaded the three neurips PDFs and ran the track A notebook. First run was miniLM + a small local HF model. Then I swapped to e5-small, which gave a cleaner one line VRR definition. Also tried chunk 500/100 vs 300/50 smaller chunks sometimes pulled appendix/template bits, while the default chunks worked better for listing the four MDAgents stages. Saved env_rag.json and rag_run_config.json. main takeaway: embeddings + chunking decide what gets retrieved, so the answers change with those settings always peek at the source snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GBaRBWON5KSj",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
