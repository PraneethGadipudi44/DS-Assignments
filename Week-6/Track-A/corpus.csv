doc_id,text
NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference,"MDAgents: An Adaptive Collaboration of LLMs for
Medical Decision-Making
Yubin Kim1 Chanwoo Park1 Hyewon Jeong1♮ Yik Siu Chan1
Xuhai Xu1 Daniel McDuff2 Hyeonhoon Lee3
Marzyeh Ghassemi1 Cynthia Breazeal1 Hae Won Park1
1Massachusetts Institute of Technology
2Google Research
3Seoul National University Hospital
{ybkim95,cpark97,hyewonj,yiksiuc,xoxu,mghassem,cynthiab,haewon}@mit.edu
dmcduff@google.com
hhoon@snu.ac.kr
Abstract
Foundation models are becoming valuable tools in medicine. Yet despite their
promise, the best way to leverage Large Language Models (LLMs) in complex
medical tasks remains an open question. We introduce a novel multi-agent frame-
work, named Medical Decision-making Agents (MDAgents) that helps to address
this gap by automatically assigning a collaboration structure to a team of LLMs.
The assigned solo or group collaboration structure is tailored to the medical task at
hand, a simple emulation inspired by the way real-world medical decision-making
processes are adapted to tasks of different complexities. We evaluate our frame-
work and baseline methods using state-of-the-art LLMs across a suite of real-world
medical knowledge and medical diagnosis benchmarks, including a comparison of
LLMs’ medical complexity classification against human physicians2. MDAgents
achieved the best performance in seven out of ten benchmarks on tasks requiring
an understanding of medical knowledge and multi-modal reasoning, showing a sig-
nificant improvement of up to 4.2%(p < 0.05) compared to previous methods’ best
performances. Ablation studies reveal that MDAgents effectively determines medi-
cal complexity to optimize for efficiency and accuracy across diverse medical tasks.
Notably, the combination of moderator review and external medical knowledge in
group collaboration resulted in an average accuracy improvement of 11.8%. Our
code can be found at https://github.com/mitmedialab/MDAgents.
1 Introduction
Medical Decision-Making (MDM) is a multifaceted and intricate process in which clinicians collabo-
ratively navigate diverse sources of information to reach a precise and specific conclusion [97]. For
instance, a primary care physician (PCP) may refer a patient to a specialist when faced with a complex
case, or a patient visiting the emergency department or urgent care might be triaged and then directed
to a specialist for further evaluation [5, 54]. MDM involves interpreting complex and multi-modal
data, such as imaging, electronic health records (EHR), physiological signals, and genetic informa-
tion, while rapidly integrating new medical research into clinical practice [68, 78]. Recently, Large
Language Models (LLMs) have shown potential for AI support in MDM [22, 37, 48, 64, 72, 90]. It
is known that they are able to process and synthesize large volumes of medical literature [74] and
clinical information [1], as well as support probabilistic [94] and causal [39] reasoning, makes LLMs
promising tools. However, there is no silver bullet in medical applications that require careful design.
♮Hyewon Jeong received her MD degree from Yonsei University College of Medicine, South Korea.
2Appendix F contains a detailed comparison results between human physicians and LLMs.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).
Medical KnowledgeA 19-year-old Caucasian male presents to your office with hypopigmented skin. He undergoes a skin biopsy and is found to have an absence of melanocytes in the epidermis. Which of the following is the most likely diagnosis? A. Tinea versicolor B. Albinism C. Vitiligo D. Solar lentigo E. Melanoma
Differential Diagnosis (DDx)
Multi-modal Reasoning
Sex: M, Age: 47 Geographical region: North America Pathology: PSVT Symptoms:-I feel pain. The pain is: tugging, Burning …Differential diagnosis:PSVT: 0.22, Anemia: 0.16,Panic attack: 0.14, Atrial fibrillation: 0.11, Anaphylaxis: 0.11, Cluster headache: 0.09, Chagas: 0.07, Scombroid food poisoning: 0.07, HIV (initial infection): 0.01
What does the circle in image D surround? A: Abnormal mitotic figures B: Central keratinization C: Frank atypia D: Areas of necrosis 
Multidisciplinary Team (MDT)
Log Ans
Integrated Care Team (ICT)
Report
Primary Care Clinician (PCC)
PromptingAns
Collaborative Discussion
LowModerateHigh
M-turnsN-rounds
Medical Query2. Recruitment1. Complexity Check
 Team 1
Team 2Report
Team NAnsReport Generation3. Analysis and Synthesis4. Final Decision
LLM Checker
Figure 1: Medical Decision-Making Agents (MDAgents) framework. Given a medical query from
different medical datasets, the framework performs 1) medical complexity check, 2) recruitment, 3)
analysis and synthesis, and 4) decision-making steps.
While decision-making tools including multi-agent LLMs [ 11, 86] have shown promise in non-
medical domains [31, 32, 44, 46, 62, 65], their evaluation in health applications has been limited.
To date, their “generalist” design has not effectively integrated the real-world systematic MDM
process [57] which requires an adaptive, collaborative, and tiered approach. Clinicians consider the
current and past history of the patient, available evidence from medical literature, and their domain
expertise and experience [20] for MDM. One example of MDM is to triage patients in emergency
room based on the severity and complexity of their medical conditions [12, 26, 87]. Patients with
pathognomonic, single uncomplicated acute conditions, or stable chronic conditions that their PCP
could manage [85] could be low complexity cases. On the other hand, patients with injuries that
involve multiple organs, chronic conditions with side effects, or superimposed diseases who often
require multiple collaborative discussions (MDT) or sequential consultations (ICT) among specialty
physicians [27, 61] are considered high complexity cases 3.
Inspired by the way that clinicians make decisions in practice, we propose Medical Decision-making
Agents (MDAgents), an adaptive medical decision-making framework that leverages LLMs to
emulate the hierarchical diagnosis procedures ranging from individual clinicians to collaborative
clinician teams (Figure 1). MDAgents work in three steps: 1) Medical complexity check; 2)
Recruitment based on medical complexity; 3) Analysis and synthesis and 4) Final decision-making to
return the answer. Our contributions are threefold:
1. We introduce MDAgents, the first adaptive decision-making framework for LLMs that mirrors
real-world MDM processes via dynamic collaboration among AI agents based on task complexity.
2. MDAgents demonstrate superior performance in accuracy over previous solo and group methods
on 7 out of 10 medical benchmarks, and we show an effective trade-off between performance and
efficiency (i.e. the number of API calls) by varying the number of agents.
3. We provide rigorous testing under various hyperparameters (e.g. temperatures), demonstrating
better robustness of MDAgents compared to solo and group methods. Furthermore, our ablations
evidence MDAgents’ ability to find the appropriate complexity level for each MDM instance.
2 Related Works
Language Models in Medical Decision-Making LLMs have shown promise in a range of ap-
plications within the medical field [14, 37, 40, 48, 63, 75, 76, 90, 96]. They can answer questions
from medical exams [43, 52], perform biomedical research [36], clinical risk prediction [37], and
clinical diagnosis [55, 67]. Medical LLMs are also evaluated on generative tasks, including creating
medical reports [79], describing medical images [ 81], constructing differentials [53], performing
diagnostic dialogue with patients [77], and generating psychiatric evaluations of interviews [24]. To
advance the capabilities of medical LLMs, two main approaches have been explored: (1) training with
3Detailed examples of low-, moderate- and high-complexity cases are provided in Appendix E.1.
2
Table 1: Comparison between our framework and previous methods (Solo and Group). Among these
works, MDAgents is the only one to perform all key dimensions of LLM decision-making.
Method MDAgents
(Ours) Single Voting [82] Debate [17] MedAgents [72] ReConcile [10]
Interaction Type
Multiple Roles ✓ ✗ ✓ ✓ ✓ ✓
Early Stopping ✓ ✗ ✓ ✓ ✓ ✗
Refinement ✓ ✗ ✗ ✓ ✓ ✗
Complexity Check ✓ ✗ ✗ ✗ ✗ ✗
Multi-party Chat ✓ ✗ ✗ ✓ ✗ ✗
Conversation Pattern Flexible Static Static Static Static Static
domain-specific data [28], and (2) applying inference-time techniques such as prompt engineering
[67] and Retrieval Augmented Generation (RAG) [92]. While initial research has been concentrated
on pre-training and fine-tuning with medical knowledge, the rise of large general-purpose LLMs has
enabled training-free methods where models leverage their latent medical knowledge. For example,
GPT-4 [60], with richer prompt crafting, surpasses the passing score on USMLE by over 20 points and
with prompt tuning can outperform fine-tuned models including Med-PaLM [58, 59]. The promise
of general-purpose models has thus inspired various techniques such as Medprompt and ensemble
refinement to improve LLM reasoning [ 67], as well as RAG tools that use external resources to
improve the factuality and completeness of LLM responses [ 38, 92]. Frameworks like MEDIQ
[49] and UoT [33] advance LLM reliability in clinical settings by enhancing information-seeking
through adaptive question-asking and uncertainty reduction, supporting more realistic diagnostic
processes. Our approach leverages these techniques and the capabilities of general-purpose models
while acknowledging that a solitary LLM [37, 48, 90] may not fully encapsulate the collaborative
and multidisciplinary nature of real-world MDM. We thus emphasize joinging multiple expert LLMs
for effective collaboration in order to solve complicated medical tasks with greater accuracy.
Multi-Agent Collaboration An array of studies have explored effective collaboration frameworks
between multiple LLM agents [47, 86] to enhance capability above and beyond an individual LLM
[80]. A common framework is role-playing, where each agent adopts a specific role (e.g. an Assistant
Agent or a Manager Agent), a task is then broken down into sub-steps and solved collaboratively
[47, 86]. While role-playing focuses on collaboration and multi-step problem-solving [88], another
framework, “multi-agent debate”, prompts each agent to solve the task independently [17]. Then, they
reason through other agents’ answers to converge on a shared response, this approach can improve
the factuality, mathematical ability and reasoning capabilities of the multi-agent solution [17, 50].
Similar frameworks include voting [82], multi-disciplinary collaboration [ 72], group discussions
(ReConcile [10]), and negotiating [ 23]. Table 1 compares existing setups across key dimensions
in multi-agent interaction. Although these frameworks have shown improvement in the respective
tasks, they rely on a pre-determined number of agents and interaction settings. When applied on a
wider variety of tasks , this static architecture may lead to suboptimal multi-agent configurations,
negatively impacting performance [51]. Furthermore, multi-agent approaches run the risk of being
computationally inefficient or expensive to employ and need to justify these costs with noticable
performance gains [ 17]. Given that different models and frameworks could generalize better to
different tasks [93], we propose a framework that dynamically assigns the optimal collaboration
strategy at inference time based on the complexity of the query. We apply our strategy to MDM, a
task that requires teamwork and should benefit from multi-agent collaboration [72].
3 MDAgents: Medical Decision-making Agents
The design of MDAgents (Figures 1 and 2) incorporates four stages: 1) Medical Complexity Check
- The system evaluates the medical query, categorizing it as low, moderate, or high complexity
based on clinical decision-making techniques [6, 7, 21, 71, 84]. 2) Expert Recruitment - Based on
complexity, the framework activates a single Primary Care Clinician (PCC) for low complexity issues,
or a Multi-disciplinary Team (MDT) or Integrated Care Team (ICT) for moderate or high complexities
[7, 18, 21, 34, 45, 71]. 3) Analysis and Synthesis - Solo queries use prompting techniques like
Chain-of-Thought (CoT) and Self-Consistency (SC). MDTs involve multiple LLM agents forming a
consensus, while ICTs synthesize information for the most complex cases. 4) Decision-making -
The final stage synthesizes all inputs to provide a well-informed answer to the medical query.
3
Question: What does the image of Gross Sectioned Myelolipoma demonstrate?Step 2. Expert Recruitment
(A) fibrosis(B) inflammation(C) hemorrhage(D) calcification
Step 3. Initial Assessment
(1) Answer: (A) fibrosis
(2) Answer: (A) fibrosis
(3) Answer:(C) hemorrhage
(4) Answer:(C) hemorrhage
(5) Answer: (A) fibrosis
Step 4. Collaborative Discussion
Step 1. Complexity CheckLowModerate     HighReason: While the identification of gross pathology might seem straightforward to a specialist in pathology, the distinction between fibrosis, hemorrhage, and other conditions can sometimes be subtle and subject to interpretation …
Turn 1Turn 2Turn 𝒕
 Turn 𝑻Round 𝒓	∈{𝟏,𝟐,…,𝑹}: Communication channel
if          :            Terminate
(…) while the expertise of Agent 1 (Pathologist     ) in tissue examination is crucial and their argument for (A) fibrosis is noted, the gross pathological features described by Agent 3 (Surgeon      ) and Agent 4 (Oncologist      ) seem to align more closely with the typical presentation of a hemorrhage. Furthermore, hemorrhage is a well-documented complication in myelolipomas, particularly larger ones or those subjected to trauma.Therefore, based on the analysis of the conversation history, the final decision is aligned with interpretation (C) hemorrhage (red arrow)as argued by Agent 3 (Surgeon      ) in Round 1, Turns 1 and 3, and Agent 4 (Oncologist      ) in Round 1, Turn 2. This conclusion is supported by the gross pathological features consistent with hemorrhage rather than fibrosis, despite the reasoned opinions for fibrosis by Agents 1 (Pathologist  ), 2 (Radiologist      ), and 5 (Endocrinologist      ).
Step 5. Review and Final Decision
GP Moderator
Pathologist(1)
Oncologist(4)
Endocrinologist(5)
Surgeon(3)
Radiologist(2)GP Moderator
🫱
🫲
𝒓
Figure 2: Illustrative example of MDAgents in a moderate complexity case from the PMC-VQA
dataset. More detailed case studies can be found in Figure 11 and 12 in the Appendix.
3.1 Agent Roles
Moderator. The moderator agent functions as a general practitioner (GP) or emergency department
doctor who first triages the medical query. This agent assesses the complexity of the problem and
determines whether it should be handled by a single agent, a MDT, or an ICT. The moderator
ensures the appropriate pathway be selected based on the query’s complexity and oversees the entire
decision-making process.
Recruiter. The recruiter agent is responsible for assembling the appropriate team of specialist
agents based on the complexity assessment of the moderator. The recruiter may assign a single
PCP agent for low-complexity cases, while MDT or ICT with relevant expertise will be formed for
moderate and high-complexity cases.
General Doctor/Specialist. These agents are domain-specific or general physicians recruited by
the recruiter agent. Depending on the complexity of the case, they may work independently or as part
of a team. General physicians handle less complex, routine cases, whereas specialists are recruited
for their specific expertise in more complex scenarios. These agents engage in the collaborative
decision-making process, contributing their specialized knowledge to reach a consensus or provide
detailed reports for high-complexity cases.
3.2 Medical Complexity Classification (Line 1 of Algorithm 1, Appendix C.2)
The first step in the MDAgents framework is to determine the complexity of a given medical query q
by the moderator LLM which functions as a generalist practitioner (GP). The moderator aims to
act as a classifier to return the complexity level of the given medical query, it is provided with the
information on how medical complexity should be defined and is instructed to classify the query into
one of three different complexity levels:
1. Low - Simple, well-defined medical issues that can be resolved by a single PCP agent. These
typically include common, acute illnesses or stable chronic conditions where the medical needs
are predictable and require minimal interdisciplinary coordination.
2. Moderate - The medical issues involve multiple interacting factors, necessitating a collaborative
approach among an MDT. These scenarios require integration of diverse medical knowledge areas
and coordination between specialists through consultation to develop effective care strategies.
3. High - Complex medical scenarios that demand extensive coordination and combined expertise
from an ICT. These cases often involve multiple chronic conditions, complicated surgical or trauma
cases, and decision-makings that integrates specialists from different healthcare departments.
3.3 Expert Recruitment (Line 3, 7, 17 of Algorithm 1)
Given a medical query, the goal of therecruiter LLM is to enlist domain experts as individuals, in
groups, or as multiple teams, based on the complexity levels determined by the moderator LLM.
Specifically, we assign medical expertise and roles to multiple LLMs, instructing them to either act
independently as solo medical agents or collaborate with other medical experts in a team. In Figure 9
in the Appendix, we also provide frequently recruited agents for each benchmark as a reference.
4
3.4 Medical Collaboration and Refinement
The initial assessment protocol of our decision-making framework categorizes query complexity into
low, moderate, and high. This categorization is grounded in established medical constructs such as
acuity [25] for straightforward cases, comorbidity [69] and case management complexity [13] for
intermediate and multi-disciplinary care requirements, and severity of illness [16] for high complexity
cases requiring comprehensive management. We outlines the specific refinement approach:
Low - Straightforward cases (Line 2-4 of Algorithm 1). For queries classified under Low com-
plexity, characterized by straightforward clinical decision pathways, a single PCP agent (Figure 10
(a)) is deployed by the definition in Section 3.2. The domain expert who is recruited by recuriter
LLM, applies few-shot prompting to the problem. The output answer, denoted as ans, is directly
obtained from the agent’s response to Q without the need for iterative refinement, formalized as
ans = Agent(Q), with Agent representing the engaged PCP agent.
Moderate - Intermediate complexity cases (Line 6-14 of Algorithm 1). In addressing more
complex queries, the utilization of an MDT (Figure 10 (b) and (c)) approach has been increasingly
recognized for its effectiveness in producing comprehensive and nuanced solutions [45]. The MDT
framework leverages the collective expertise of professionals from diverse disciplines, facilitating
a holistic examination of the query at hand. This collaborative method is particularly advanta-
geous in scenarios where the complexity of a problem transcends the scope of a single domain,
necessitating a fusion of insights from various specialties [ 7, 71]. The MDT approach not only
enhances decision-making quality through the integration of multidimensional perspectives but also
significantly improves the adaptability and efficiency of the problem-solving process [21].
Building upon this foundation, our framework specifically addresses queries of moderate complexity
through a structured, multi-tiered collaborative approach. An MDT recruited by recruiter LLM (see
Figure 10 in Appendix) starts an iterative discussion process aimed at reaching a consensus with at
most R rounds (Line 10-12). For each round r ∈ R, agents Ai, i∈ 1, . . . , Nindicate participation
and preferred interlocutors. The system facilitates message exchanges for T turns. If consensus is not
reached and agents agree to continue, a new round begins with access to previous conversations. For
every round, consensus within the MDT is determined by parsing and comparing their opinions. In
the event of a disagreement, the moderator agent, consistent with the one described in Section 3.2
reviews the MDT’s discourse and formulates feedback for each agent.
High - Complex care cases (Line 17-24 of Algorithm 1). In contrast to the MDT approach, the ICT
(Figure 10 (d)) paradigm is essential for addressing the highest tier of query complexity in healthcare.
This structured progression through the ICT ensures a depth of analysis that is specialized and focused
at each stage of the decision-making process. Beginning with the Initial Assessment Team, moving
through various diagnostic teams, and culminating with the Final Review & Decision Team, our ICT
model aligns specialist insights into a cohesive narrative that informs the ultimate decision (Appendix
Algorithms 1 Lines 19-21). A key component of this process is the report generation process described
in Appendix with the prompt, where each team, led by a lead clinician, collaboratively produces
a comprehensive report synthesizing their findings. This phased approach, supported by evidence
from recent healthcare studies, has been shown to enhance the precision of clinical decision-making,
as each team builds upon the foundation laid by the previous, ensuring a meticulous and refined
examination of complex medical cases [34]. The resultant reports, accumulating throughout the ICT
process, are not only reflective of comprehensive medical evaluations but also of a systematic and
layered analysis that is critical in the management of intricate health scenarios [18].
3.5 Decision-making
In the final stage of our framework, the decision-maker agent synthesizes the diverse inputs generated
throughout the decision-making process to arrive at a well-informed final answer to the medical query
q. This synthesis involves several components depending on the complexity level of the query:
1. Low: Directly utilizes the initial response from the primary decision-making agent.
2. Moderate: Incorporates the conversation history ( Interaction) between the recruited agents to
understand the nuances and disagreements in their responses.
3. High: Considers detailed reports (Reports) generated by the agents, which include comprehensive
analyses and justifications for their diagnostic suggestions.
The decision-making process is formulated as ans = Agent(·) where the final answer, ans is deter-
mined by integrating the outputs from analysis and synthesis step based on its medical complexities.
This integration employs ensemble techniques such as temperature ensembles to ensure the decision
is robust and reflects a consensus among the models when applicable (see Appendix C.2 for details).
5
Table 2: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive setting. Bold represents
the best and Underlined represents the second best performance for each benchmark and model. All
benchmarks except for MedVidQA were evaluated with GPT-4(V) and MedVidQA was evaluated
with Gemini-Pro(Vision). Full experimental results with other models are listed in Table 9-12 in
Appendix.
Category Method
Medical Knowledge Retrieval Datasets
MedQA
T
PubMedQA
T
Path-VQA
I T
PMC-VQA
I T
MedVidQA
V T
Single-agent
Zero-shot 75.0 ±1.3 61.5 ±2.2 57.9 ±1.6 49.0 ±3.7 37.9 ±8.4
Few-shot 72.9 ±11.4 63.1 ±11.7 57.5 ±4.5 52.2 ±2.0 47.1 ±8.6
+ CoT [83] 82.5 ±4.9 57.6 ±9.2 58.6 ±3.1 51.3 ±1.5 48.6 ±5.5
+ CoT-SC [82] 83.9 ±2.7 58.7 ±5.0 61.2 ±2.1 50.5 ±5.2 49.2 ±8.2
ER [67] 81.9 ±2.1 56.0 ±7.0 61.4 ±4.1 52.7 ±2.9 48.5 ±4.1
Medprompt [59] 82.4 ±5.1 51.8 ±4.6 59.2 ±5.7 53.4 ±7.9 44.5 ±2.0
Multi-agent
(Single-model)
Majority V oting 80.6 ±2.9 72.2 ±6.9 56.9 ±19.7 36.8 ±6.7 50.8 ±7.4
Weighted V oting 78.8 ±1.1 72.2 ±6.9 62.1 ±13.9 25.4 ±9.0 57.8 ±2.1
Borda Count 70.3 ±8.5 66.9 ±3.0 61.9 ±8.1 27.9 ±5.3 54.5 ±4.7
MedAgents [72] 79.1 ±7.4 69.7 ±4.7 45.4 ±8.1 39.6 ±3.0 51.6 ±4.8
Meta-Prompting [70] 80.6 ±1.2 73.3 ±2.3 55.3 ±2.3 42.6 ±4.2 -
Multi-agent
(Multi-model)
Reconcile [10] 81.3 ±3.0 79.7 ±3.2 57.5 ±3.3 31.4 ±1.2 -
AutoGen [86] 60.6 ±5.0 77.3 ±2.3 43.0 ±8.9 37.3 ±6.1 -
DyLAN [51] 64.2 ±2.3 73.6 ±4.2 41.3 ±1.2 34.0 ±3.5 -
Adaptive MDAgents (Ours) 88.7±4.0 75.0 ±1.0 65.3 ±3.9 56.4 ±4.5 56.2 ±6.7
Category Method
Clinical Reasoning and Diagnostic Datasets
DDXPlus
T
SymCat
T
JAMA
T
MedBullets
T
MIMIC-CXR
I T
Single-agent
Zero-shot 70.3 ±2.0 88.7 ±2.3 62.0 ±2.0 67.0 ±1.4 40.0 ±5.3
Few-shot 69.4 ±1.0 86.7 ±3.1 69.0 ±4.2 72.0 ±2.8 35.3 ±5.0
+ CoT [83] 72.7 ±7.7 78.0 ±2.0 66.0 ±5.7 70.0 ±0.0 36.2 ±5.2
+ CoT-SC [82] 52.1 ±6.4 83.3 ±3.1 68.0 ±2.8 76.0 ±2.8 51.7 ±4.0
ER [67] 61.3 ±2.4 82.7 ±2.3 71.0 ±1.4 76.0 ±5.7 50.0 ±0.0
Medprompt [59] 59.5 ±17.7 87.3 ±1.2 70.7 ±4.3 71.0 ±1.4 53.4 ±4.3
Multi-agent
(Single-model)
Majority V oting 67.8 ±4.9 91.9 ±2.2 70.0 ±5.7 70.0 ±0.0 49.5 ±10.7
Weighted V oting 65.9 ±3.3 90.5 ±2.9 66.1 ±4.1 66.0 ±5.7 53.5 ±2.2
Borda Count 67.1 ±6.7 78.0 ±11.8 61.0 ±5.6 66.0 ±2.8 45.3 ±6.8
MedAgents [72] 62.8 ±5.6 90.0 ±0.0 66.0 ±5.7 77.0 ±1.4 43.3 ±7.0
Meta-Prompting [70] 52.6 ±6.1 77.3 ±2.3 64.7 ±3.1 49.3 ±1.2 42.0 ±4.0
Multi-agent
(Multi-model)
Reconcile [10] 68.4 ±7.4 90.6 ±2.5 60.7 ±5.7 59.5 ±8.7 33.3 ±3.4
AutoGen [86] 67.3 ±11.8 73.3 ±3.1 64.6 ±1.2 55.3 ±3.1 43.3 ±4.2
DyLAN [51] 56.4 ±2.9 75.3 ±4.6 60.1 ±3.1 57.3 ±6.1 38.7 ±1.2
Adaptive MDAgents (Ours) 77.9 ±2.1 93.1 ±1.0 70.9 ±0.3 80.8 ±1.7 55.9 ±9.1
* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement
* T : text-only, I : image+text, V : video+text
* All experiments were tested with 3 random seeds
4 Experiments and Results
In this section, we evaluate our framework and baseline methods across different medical benchmarks
in Solo, Group, and Aaptive settings. Our experiments and ablation studies highlight the framework’s
performance, demonstrating robustness and efficiency by modulating agent numbers and temperatures.
Results also show a beneficial convergence of agent opinions in collaborative settings.
4.1 Setup
To verify the effectiveness of our framework, we conduct comprehensive experiments with baseline
methods on ten datasets including MedQA, PubMedQA, DDXPlus, SymCat, JAMA, MedBullets,
Path-VQA, PMC-VQA, MIMIC-CXR and MedVidQA. A detailed explanation and statistics for each
dataset are deferred to Appendix A and Figure 8. We use 50 samples per dataset for testing, and the
inference time for each complexity was: low - 14.7s, moderate - 95.5s, and high - 226s in average.
For all the quantitative experiments in this section, we compare three settings: (1)Solo: Using a single
LLM agent in the decision-making state. (2) Group: Implementing multi-agents to collaborate during
the decision-making process. (3) Adaptive: Our proposed method MDAgents, adaptively constructs
the inference structure from PCP to MDT and ICT. We use 3-shot prompting for low-complexity
cases and zero-shot prompting for moderate and high-complexity cases across all settings.
Medical Question Answering With MedQA [35], PubMedQA [36], MedBullets [9], and JAMA
[9], we focus on question answering through text, involving both literature-based and conceptual
medical knowledge questions. Specifically, PubMedQA tasks models to answer questions using
abstracts from PubMed, requiring synthesis of biomedical information. MedQA tests the model’s
ability to understand and respond to multiple-choice questions derived from medical educational
materials and examinations. MedBullets provides USMLE Step 2/3 type questions that demand
6
the application of medical knowledge and clinical reasoning. JAMA Clinical Challenge presents
challenging real-world clinical cases with diagnosis or treatment decision-making questions, testing
the model’s clinical reasoning (Figure 8 in Appendix shows complexity distribution for each dataset)
Diagnostic Reasoning DDXPlus [ 73] and SymCat [ 2] involve clinical vignettes that require
differential diagnosis, closely mimicking the diagnostic process of physicians. These tasks test the
model’s ability to reason through symptoms and clinical data to suggest possible medical conditions,
evaluating the AI’s diagnostic reasoning abilities similar to a clinical setting. SymCat [ 2] uses
synthetic patient records constructed from a public disease-symptom data source and is enhanced
with additional contextual information through the NLICE method.
Medical Visual Interpretation Path-VQA [30], PMC-VQA [95], MedVidQA [29], and MIMIC-
CXR [3] challenge models to interpret medical images and videos, requiring integration of visual
data with clinical knowledge. PathVQA focuses on answering questions based on pathology images,
testing AI’s capability to interpret complex visual information from medical images. PMC-VQA
evaluates AI’s proficiency in deriving answers from both text and images found in scientific publi-
cations. MedVidQA extends to video-based content, where AI models need to process information
from medical procedure videos. MIMIC-CXR-VQA specifically targets chest radiographs, utilizing a
diverse and large-scale dataset designed for visual question-answering tasks in the medical domain.
Baseline Methods
• Solo: The baseline methods considered for the Solo setting include the following: Zero-shot [41]
directly incorporates a prompt to facilitate inference, while Few-shot [8] involves a small number of
examples. Few-shot CoT [83] integrates rationales before deducing the answer. Few-shot CoT-SC
[82] builds upon Few-shot CoT by sampling multiple chains to yield the majority answer. Ensemble
Refinement (ER) [67] is a prompting strategy that conditions model responses on multiple reasoning
paths to bolster the reasoning capabilities of LLMs. Medprompt [59] is a composition of several
prompting strategies that enhances the performance of LLMs and achieves state-of-the-art results
on multiple benchmark datasets, including medical and non-medical domains.
• Group: We tested five group decision-making methods: V oting [82], MedAgents [72], Reconcile
[10], AutoGen [86], and DyLAN [ 51]. Autogen was based on four agents, with one User, one
Clinician, one Medical Expert, and one Moderator, with one response per agent [86]. DyLAN setup
followed the base implementations of four agents with no specific roles and four maximum rounds
of interaction [51]. While the methods support multiple models, GPT-4 was used for all agents.
4.2 Results
In Table 2, we report the classification accuracy on MedQA, PubMedQA, DDXPlus, SymCat, JAMA,
MedBullets, Path-VQA, PMC-VQA and MedVidQA dataset. We compare our method (Adaptive)
with several baselines in both Solo and Group settings.
Adaptive method outperforms Solo and Group settings. As depicted in Figure 4 and Table 2,
MDAgents significantly outperforms (p < 0.05) both Solo and Group setting methods, showing best
performance in 7 out of 10 medical benchmarks tested. This reveals the effectiveness of adaptive
strategies integrated within our system, particularly when navigating through the text-only (e.g.,
DDXPlus where it outperformed the best performance of single-agent by 5.2% and multi-agent by
9.5%) and text-image datasets (e.g., Path-VQA, PMC-VQA and MIMIC-CXR). Our approach not
only comprehends textual information with high precision but also adeptly synthesizes visual data, a
pivotal capability in medical diagnostic evaluations.
(a) (b) (c) (d)
Low Moderate High
Figure 3: Experiment with the MedQA dataset ( N=25 randomly sampled questions). (a) LLM’s
capability to classify complexity. (b-d) Evaluating 25 medical problems by solving each one 10 times
at various complexity levels. The x-axis represents the accuracy achieved for each problem, while the
y-axis shows the number of problems that reached that level of accuracy.
7
Why Do Adaptive Decision-making Framework Work Well? It is important to accurately assign
difficulty levels to medical questions. For instance, if a medical question is obviously easy, utilizing a
team of specialists (such as an IDT) might be excessive and potentially lead to overly pessimistic
approaches. Conversely, if a difficult medical question is only tackled by a PCP, the problem might
not be adequately addressed. The core issue here is the LLM’s capability to classify the difficulty of
medical questions appropriately. If an LLM inaccurately classifies the difficulty level, the chosen
medical solution may not be suitable, potentially leading to the wrong decision making. Therefore,
understanding what constitutes an appropriate difficulty level is essential.
We hypothesize that an LLM, functioning as a classifier, will select the optimal complexity level
for each MDM problem. This hypothesis is supported by Figure 3, which illustrates that the model
appropriately matches the complexity levels; low, moderate, and high of the given problem. To
determine this, we assessed the accuracy of solutions across various difficulty levels. Specifically, we
evaluated 25 medical problems by repeating each problem for 10 times at each difficulty level. By
measuring the success rate, we aimed to identify the difficulty level that yielded the highest accuracy.
This approach ensures that the LLM’s complexity classification aligns with the most effective and
accurate medical solutions, thereby optimizing the application of medical expertise to each question.
Formally, for any given problemP, we denote the probability that the correct answer can be solved
at a specific complexity level as pcomplexity-level(P), where complexity-level ∈ {low, moderate, high}.
arg max(P) ∈ {low, moderate, high} refers to the complexity level that has the highest proba-
bility among plow(P), pmoderate(P), and phigh(P). Similarly, arg min(P) is the complexity level
with the lowest probability, and arg mid(P) is the one with the middle probability. We denote
a, b, and c as the probabilities that the LLM selects the complexity levels corresponding to
arg max, arg mid, and arg min, respectively. Thus, the accuracy of our system for problem P
can be described by a · parg max(P) +b · parg mid(P) +c · parg min(P), and the overall accuracy is
given by EP [a · parg max(P) +b · parg mid(P) +c · parg min(P)]. The estimated values of a, b, care
a = 0.81 ± 0.29, b = 0.11 ± 0.28, and c = 0.08 ± 0.16, which indicates that LLM can provide an
optimal complexity level with probability at least 80%.
These findings suggest that a classifier LLM can implicitly simulate various complexity levels and
optimally adapt to the complexity required for each medical problem, as shown in Figure 3. This
ability to adjust complexity dynamically proves to be crucial for applying LLMs effectively in MDM
contexts as shown by the competitiveness of our Adaptive approach.
SoloGroupOurs
Accuracy
**
OursSoloGroup
ns
Figure 4: Our method outperforms Solo and Group
settings across different medical benchmarks.
Solo vs. Group Setting in MDM. The ex-
perimental results reveal distinct performance
patterns between Solo and Group settings across
various medical benchmarks. In simpler datasets
like MedQA, solo methods, leveraging Few-shot
CoT and CoT-SC, achieved up to 83.9% accu-
racy compared to the group’s best of 81.3%.
Conversely, for more complex datasets like Sym-
Cat, group settings perform better, with SymCat
showing 91.9% accuracy in the group settings
versus 88.7% in solo settings. Notably, group
settings (e.g. Weighted V oting, Reconcile) per-
formed better in multi-modal datasets such as
Path-VQA (image + text), MedVidQA ( video
+ text), and MIMIC-CXR (image + text), high-
lighting the advantage of collaborative process in complex tasks. This result aligns with findings from
[4], which showed that pooled diagnoses from groups of physicians significantly outperformed those
of individual physicians, with accuracy increasing as group size increased. Overall, solo settings
outperformed group settings in four benchmarks, while group settings outperformed solo in six
benchmarks. These results reveals that while solo methods excel in straightforward tasks, group
settings provide better performance in complex, multi-faceted tasks requiring diverse expertise.
4.3 Ablation Studies
Impact of Complexity Selection. We evaluate the importance of the complexity assessment and
adaptive process through ablation studies (Figure 5). Our adaptive method significantly outperforms
static complexity assignments across different modality benchmarks. For text-only queries, the
Adaptive method achieves an accuracy of 81.2%, significantly higher thanlow (64.2%), moderate
(71.6%), and high (65.8%) settings. Interestingly, 64% of the text-only queries were classified as
8
high complexity, indicating that many text-based queries required in-depth analysis with different
expertise. In the image + text modality, the Adaptive method classified 55% of the queries as low
complexity, suggesting that the visual information often provides clear and straightforward cues that
simplify the decision-making process. Finally, for video + text queries, 87% of these queries were
classified as low complexity, reflecting that the dynamic visual data in conjunction with text can often
be straightforwardly interpreted. However, further evaluation on more challenging video medical
datasets is needed, as MedVidQA contains relatively less complex medical knowledge.
Method Avg. Accuracy (%)
MDAgents (Ours) 71.8
+ MedRAG 75.2 ( ↑ 4.7 %)
+ Moderator’s Review 77.6 ( ↑ 8.1 %)
+ Moderator’s Review & MedRAG 80.3 ( ↑ 11.8 %)
Table 3: Ablations for the impact of moderator’s review and
MedRAG. The Accuracy were averaged accuracy across all datasets.
Impact of Moderator’s Re-
view and RAG Table 3 ex-
amines the impact of incorpo-
rating external medical knowl-
edge and moderator reviews
into the MDAgents framework
on accuracy. MedRAG [ 89]
is a systematic toolkit for
Retrieval-Augmented Genera-
tion (RAG) that leverages various corpora; biomedical, clinical and general medicine, to provide
comprehensive knowledge. The baseline accuracy of MDAgents is 71.8%. Integrating MedRAG
increases accuracy to 75.2% (up 4.7%), while the moderator’s review alone raises it to 77.6% (up
8.1%). The combined use of both methods achieves the highest accuracy at 80.3% (up 11.8%).
The results indicate that MedRAG and moderator review both enhance performance, with their
combined effect being synergistic. This highlights that leveraging recent external knowledge and
structured feedback mechanisms is crucial for refining and converging on accurate medical decisions.
This improvement underscores the importance of a hybrid strategy, aligning with real-world practices
of continuous learning and expert consultation to optimize performance in medical applications.
4.4 Impact of Number of Agents in Group Setting.
Our experiment with varying the number of agents in a collaborative Group setting (Appendix Figure
6 (a-b)) shows that a higher number of agents does not lead to better performance. Rather, our
Adaptive method achieves optimal performance with fewer agents (N=3, peak accuracy of 83.5%)
by intelligently calibrating the number of collaborating agents. This not only indicates efficiency in
decision-making but also computational and economic benefits, considering the reduced number of
API calls needed, especially when contrasted with the Solo and Group settings.
With regards to computational efficiency, the Solo setting (5-shot CoT-SC) resulted in a 6.0 and
Group setting (MedAgents with N=5) resulted in a 20.3 API calls, suggesting a high computational
cost without a corresponding increase in accuracy. On the other hand, our Adaptive method exhibits a
more economical use of resources, demonstrated by fewer API calls (9.3 withN=3) while maintaining
high accuracy, a critical factor in deploying scalable and cost-effective medical AI solutions.
4.5 Robustness of MDAgents with different parameters.
Our Adaptive approach shows resilience to changes in temperature (Appendix Figure 6 (c), low
(T=0.3) and high (T=1.2)) with performance improving under higher temperatures. This suggests
that our model can utilize the creative and diverse outputs generated at higher temperatures to
enhance decision-making, a property that is not as pronounced in the Solo and Group conditions.
This robustness is particularly valuable in real-world medical domains with high uncertainty and
ambiguity in datasets [ 15]. Additionally, studies have shown that creative diagnostic approaches
Aa
text-only video + text
image + textAa Aa
Figure 5: Impact of complexity selection of the query. Accuracy of each ablation on text-only (left),
text+image (center) and text+video (right) benchmarks are reported.
9
(a) (b) (c)
Figure 6: Impact of the number of agents on (a) Accuracy, (b) Number of API Calls on medical
benchmarks with GPT-4 (V) and (c) Performance of three different settings under low (T=0.3) and
high (T=1.2) temperatures on medical benchmarks. Our Adaptive setting shows better robustness to
different temperatures and even takes advantage of higher temperatures.
: text-only: image +text: video +text
Figure 7: An illustration of consensus entropy in group collaboration process of MDAgents (w/
Gemini-Pro (Vision), N=30 for each dataset) on medical benchmarks with different modality inputs.
can mitigate cognitive biases and improve diagnostic accuracy [66], while fostering flexibility and
adaptability in decision-making [19]. These insights support the enhanced performance observed
under higher temperatures in our framework. However, the future work should explore a wider range
of temperatures to fully understand the robustness and adaptability of our approach.
4.6 Convergence Trends in Consensus Dynamics
There is clear trend towards consensus among MDAgents cross various data modalities (Figure 7).
The text+video modality demonstrates a rapid convergence, reflecting the agents’ efficient processing
of combined textual and visual cues. On the other hand, the text+image and text-only modalities
display a more gradual decline in entropy, indicating a progressive narrowing of interpretative
diversity among the agents. Despite the differing rates and initial conditions, all modalities exhibit
convergence of agent opinions over time. This uniformity in reaching consensus highlights the
MDAgents’ capability to integrate and reconcile information. Please refer to Appendix B for a
detailed explanation of the entropy calculation.
5 Conclusion
This paper introduces MDAgents, a framework designed to enhance the utility of LLMs in complex
medical decision-making by dynamically structuring effective collaboration models. To reflect the
nuanced consultation aspects in clinical settings, MDAgents adaptively assigns LLMs either to roles
independently or within groups, depending on the task’s complexity. This emulation of real-world
medical decision processes has been comprehensively evaluated, with MDAgents outperforming
previous solo and group methods in 7 out of 10 medical benchmarks. The case study illustrates the
practical efficacy and collaborative dynamics of our proposed framework, providing insights into
how differing expert opinions are synthesized to reach a more accurate diagnosis. This is evidenced
by our agents’ ability to converge on the correct diagnosis despite initially divergent perspectives.
Ablation studies further elucidate the individual contributions of agents and strategies within the
system, revealing the critical components and interactions that drive the framework’s success. By
harnessing the strength of multi-modal reasoning and fostering a collaborative process among LLM
agents, our framework opens up new possibilities for enhancing LLM-assisted medical diagnosis
systems, pushing the boundaries of automated clinical reasoning.
10
Acknowledgments and Disclosure of Funding
We thank Yoon Kim at MIT, Vivek Natarajan at Google, WonJin Yoon and Tim Miller at Harvard
Medical School, Seonghwan Bae at Sacheon Public Health Center, Hui Dong Lim at Seoul National
University Hospital for their revisions, feedback, and support. C.P. acknowledges support from the
Takeda Fellowship, the Korea Foundation for Advanced Studies, and the Siebel Scholarship.
References
[1] Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David Sontag. Large
language models are few-shot clinical information extractors. arXiv preprint arXiv:2205.12689,
2022.
[2] Zaid Al-Ars, Obinna Agba, Zhuoran Guo, Christiaan Boerkamp, Ziyaad Jaber, and Tareq
Jaber. Nlice: Synthetic medical record generation for effective primary healthcare differential
diagnosis, 2024.
[3] Seongsu Bae, Daeun Kyung, Jaehee Ryu, Eunbyeol Cho, Gyubok Lee, Sunjun Kweon, Jungwoo
Oh, Lei Ji, Eric Chang, Tackeun Kim, et al. Ehrxqa: A multi-modal question answering
dataset for electronic health records with chest x-ray images. Advances in Neural Information
Processing Systems, 36, 2024.
[4] Michael L Barnett, Dhruv Boddupalli, Shantanu Nundy, and David W Bates. Comparative
accuracy of diagnosis by collective intelligence of multiple physicians vs individual physicians.
JAMA network open, 2(3):e190096–e190096, 2019.
[5] Michael L Barnett, Nancy L Keating, Nicholas A Christakis, A James O’malley, and Bruce E
Landon. Reasons for choice of referral physician among primary care and specialist physicians.
Journal of general internal medicine, 27:506–512, 2012.
[6] Ofir Ben-Assuli, Nanda Kumar, Ofer Arazy, and Itamar Shabtai. The use of analytic hierarchy
process for measuring the complexity of medical diagnosis. Health Informatics Journal ,
26(1):218–232, 2020.
[7] Justin Bitter, Elizabeth van Veen-Berkx, Hein G Gooszen, and Pierre van Amelsvoort. Mul-
tidisciplinary teamwork is an important issue to healthcare professionals. Team Performance
Management: An International Journal, 19(5/6):263–278, 2013.
[8] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.
[9] Hanjie Chen, Zhouxiang Fang, Yash Singla, and Mark Dredze. Benchmarking large language
models on answering and explaining challenging medical questions, 2024.
[10] Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal. Reconcile: Round-table conference
improves reasoning via consensus among diverse llms, 2024.
[11] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu,
Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong
Sun, and Jie Zhou. Agentverse: Facilitating multi-agent collaboration and exploring emergent
behaviors, 2023.
[12] Michael Christ, Florian Grossmann, Daniela Winter, Roland Bingisser, and Elke Platz. Modern
triage in the emergency department. Deutsches Ärzteblatt International, 107(50):892, 2010.
[13] Jane Cioffi and Roslyn Markham. Clinical decision-making by midwives: managing case
complexity. Journal of advanced nursing, 25(2):265–272, 1997.
11
[14] Jan Clusmann, Fiona R Kolbinger, Hannah Sophie Muti, Zunamys I Carrero, Jan-Niklas Eckardt,
Narmin Ghaffari Laleh, Chiara Maria Lavinia Löffler, Sophie-Caroline Schwarzkopf, Michaela
Unger, Gregory P Veldhuizen, et al. The future landscape of large language models in medicine.
Communications medicine, 3(1):141, 2023.
[15] Nick Craddock and Laurence Mynors-Wallis. Psychiatric diagnosis: impersonal, imperfect and
important. The British Journal of Psychiatry, 204(2):93–95, 2014.
[16] Lesley F Degner and Jeffrey A Sloan. Decision making during serious illness: what role do
patients really want to play? Journal of clinical epidemiology, 45(9):941–950, 1992.
[17] Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. Improving
factuality and reasoning in language models through multiagent debate, 2023.
[18] Carolyn Ee, James Lake, Joseph Firth, Fiona Hargraves, M De Manincor, Tanya Meade,
Wolfgang Marx, and Jerome Sarris. An integrative collaborative care model for people with
mental illness and physical comorbidities. International Journal of Mental Health Systems ,
14:1–16, 2020.
[19] Arthur S Elstein and Alan Schwarz. Clinical problem solving and diagnostic decision making:
selective review of the cognitive literature. BMJ, 324(7339):729–732, 2002.
[20] Glyn Elwyn, Dominick Frosch, Richard Thomson, Natalie Joseph-Williams, Amy Lloyd, Paul
Kinnersley, Emma Cording, Dave Tomson, Carole Dodd, Stephen Rollnick, et al. Shared
decision making: a model for clinical practice. Journal of general internal medicine, 27:1361–
1367, 2012.
[21] Nancy E Epstein. Multidisciplinary in-hospital teams improve patient outcomes: A review.
Surgical neurology international, 5(Suppl 7):S295, 2014.
[22] Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi, Fei Huang, and
Jingren Zhou. Ai hospital: Interactive evaluation and collaboration of llms as intern doctors for
clinical diagnosis, 2024.
[23] Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. Improving language model negotiation
with self-play and in-context learning from ai feedback, 2023.
[24] Isaac R Galatzer-Levy, Daniel McDuff, Vivek Natarajan, Alan Karthikesalingam, and Matteo
Malgaroli. The capability of large language models to measure psychiatric functioning. arXiv
preprint arXiv:2308.01834, 2023.
[25] Amy L Garcia. Variability in acuity in acute care.JONA: The Journal of Nursing Administration,
47(10):476–483, 2017.
[26] Nicki Gilboy, Paula Tanabe, Debbie Travers, Alexander M Rosenau, et al. Emergency severity
index (esi): A triage tool for emergency department care, version 4. implementation handbook
2012 edition. AHRQ publication, 12, 2011.
[27] David Grembowski, Judith Schaefer, Karin E Johnson, Henry Fischer, Susan L Moore, Ming
Tai-Seale, Richard Ricciardi, James R Fraser, Donald Miller, Lisa LeRoy, et al. A conceptual
model of the role of complexity in the care of patients with multiple chronic conditions.Medical
care, 52:S7–S14, 2014.
[28] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan
Naumann, Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining for
biomedical natural language processing. ACM Transactions on Computing for Healthcare ,
3(1):1–23, October 2021.
[29] Deepak Gupta, Kush Attal, and Dina Demner-Fushman. A dataset for medical instructional
video classification and question answering, 2022.
[30] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+
questions for medical visual question answering, 2020.
12
[31] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn
Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.
arXiv preprint arXiv:2103.03874, 2021.
[32] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang,
Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng
Xiao, Chenglin Wu, and Jürgen Schmidhuber. Metagpt: Meta programming for a multi-agent
collaborative framework, 2023.
[33] Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh Tuan Luu, Junxian
He, Pang Wei Koh, and Bryan Hooi. Uncertainty of thoughts: Uncertainty-aware planning
enhances information seeking in large language models. arXiv preprint arXiv:2402.03271,
2024.
[34] M. Jimenez-Lara. Reaping the benefits of integrated health care. stanford social innovation
review. Stanford Social Innovation Review., 2016.
[35] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What
disease does this patient have? a large-scale open domain question answering dataset from
medical exams, 2020.
[36] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: A
dataset for biomedical research question answering. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP), pages 2567–2577, 2019.
[37] Qiao Jin, Zhizheng Wang, Yifan Yang, Qingqing Zhu, Donald Wright, Thomas Huang, W John
Wilbur, Zhe He, Andrew Taylor, Qingyu Chen, et al. Agentmd: Empowering language agents
for risk prediction with large-scale clinical tool learning. arXiv preprint arXiv:2402.13225,
2024.
[38] Qiao Jin, Yifan Yang, Qingyu Chen, and Zhiyong Lu. Genegpt: augmenting large language
models with domain tools for improved access to biomedical information. Bioinformatics,
40(2), February 2024.
[39] Emre Kıcıman, Robert Ness, Amit Sharma, and Chenhao Tan. Causal reasoning and large
language models: Opening a new frontier for causality. arXiv preprint arXiv:2305.00050, 2023.
[40] Yubin Kim, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. Health-llm:
Large language models for health prediction via wearable sensor data, 2024.
[41] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners. Advances in neural information processing systems,
35:22199–22213, 2022.
[42] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate
Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to
self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024.
[43] Tiffany H. Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille
Elepaño, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, and Victor
Tseng. Performance of chatgpt on usmle: Potential for ai-assisted medical education using large
language models. PLOS Digital Health, 2(2):1–12, 02 2023.
[44] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris
Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a
benchmark for question answering research. Transactions of the Association for Computational
Linguistics, 7:453–466, 2019.
[45] Danielle L LaFrance, Mary Jane Weiss, Ellie Kazemi, Joanne Gerenser, and Jacqueline Dobres.
Multidisciplinary teaming: Enhancing collaboration through increased understanding. Behavior
analysis in practice, 12(3):709–726, 2019.
13
[46] Beibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, and Ishai Menache. Large language
models for supply chain optimization. arXiv preprint arXiv:2307.03875, 2023.
[47] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard
Ghanem. Camel: Communicative agents for ""mind"" exploration of large language model society,
2023.
[48] Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang, Weizhi Ma, and
Yang Liu. Agent hospital: A simulacrum of hospital with evolvable medical agents. arXiv
preprint arXiv:2405.02957, 2024.
[49] Shuyue Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan Ilgen, Emma Pierson,
Pang Wei Koh, and Yulia Tsvetkov. Mediq: Question-asking llms for adaptive and reliable
medical reasoning. arXiv preprint arXiv:2406.00922, 2024.
[50] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang,
Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models
through multi-agent debate, 2023.
[51] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. Dynamic llm-agent network: An
llm-agent collaboration framework with agent team optimization, 2023.
[52] Valentin Liévin, Christoffer Egeberg Hother, Andreas Geert Motzfeldt, and Ole Winther. Can
large language models reason about medical questions?, 2023.
[53] Daniel McDuff, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan
Singhal, Yash Sharma, Shekoofeh Azizi, Kavita Kulkarni, et al. Towards accurate differential
diagnosis with large language models. arXiv preprint arXiv:2312.00164, 2023.
[54] Ateev Mehrotra, Christopher B Forrest, and Caroline Y Lin. Dropping the baton: specialty
referrals in the united states. The Milbank Quarterly, 89(1):39–68, 2011.
[55] Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M. Krumholz, Jure
Leskovec, Eric J. Topol, and Pranav Rajpurkar. Foundation models for generalist medical
artificial intelligence. Nature, 616(7956):259–265, 2023.
[56] Tong Mu, Alec Helyar, Johannes Heidecke, Joshua Achiam, Andrea Vallone, Ian Kivlichan,
Molly Lin, Alex Beutel, John Schulman, and Lilian Weng. Rule based rewards for language
model safety. Advances in Neural Information Processing Systems, 2024.
[57] Mark A Musen, Blackford Middleton, and Robert A Greenes. Clinical decision-support systems.
In Biomedical informatics: computer applications in health care and biomedicine , pages
795–840. Springer, 2021.
[58] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capa-
bilities of gpt-4 on medical challenge problems, 2023.
[59] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas
King, Jonathan Larson, Yuanzhi Li, Weishung Liu, Renqian Luo, Scott Mayer McKinney,
Robert Osazuwa Ness, Hoifung Poon, Tao Qin, Naoto Usuyama, Chris White, and Eric Horvitz.
Can generalist foundation models outcompete special-purpose tuning? case study in medicine.
November 2023.
[60] OpenAI. Gpt-4 technical report, 2024.
[61] Anand K Parekh, Richard A Goodman, Catherine Gordon, Howard K Koh, and HHS Intera-
gency Workgroup on Multiple Chronic Conditions. Managing multiple chronic conditions: a
strategic framework for improving health outcomes and quality of life. Public health reports,
126(4):460–471, 2011.
[62] Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar, and Kaiqing Zhang. Do llm agents have regret?
a case study in online learning and games. arXiv preprint arXiv:2403.16843, 2024.
14
[63] Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan
Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, et al. Capabilities of gemini models in
medicine. arXiv preprint arXiv:2404.18416, 2024.
[64] Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda Zhu, Joyce Ho,
Carl Yang, and May D. Wang. Ehragent: Code empowers large language models for few-shot
complex tabular reasoning on electronic health records, 2024.
[65] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and
Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive
learning. arXiv preprint arXiv:2010.03768, 2020.
[66] Hardeep Singh, Denise M Connor, and Gurpreet Dhaliwal. Five strategies for clinicians to
advance diagnostic excellence. BMJ, 376, 2022.
[67] Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung,
Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Senevi-
ratne, Paul Gamble, Chris Kelly, Abubakr Babiker, Nathanael Schärli, Aakanksha Chowdhery,
Philip Mansfield, Dina Demner-Fushman, Blaise Agüera y Arcas, Dale Webster, Greg S.
Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajko-
mar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large
language models encode clinical knowledge. Nature, 620(7972):172–180, 2023.
[68] Harold C Sox, Michael C Higgins, Douglas K Owens, and Gillian Sanders Schmidler. Medical
decision making. John Wiley & Sons, 2024.
[69] J Stairmand, Louise Signal, D Sarfati, C Jackson, L Batten, M Holdaway, and C Cunningham.
Consideration of comorbidity in treatment decision making in multidisciplinary cancer team
meetings: a systematic review. Annals of Oncology, 26(7):1325–1332, 2015.
[70] Mirac Suzgun and Adam Tauman Kalai. Meta-prompting: Enhancing language models with
task-agnostic scaffolding, 2024.
[71] Miren Taberna, Francisco Gil Moncayo, Enric Jané-Salas, Maite Antonio, Lorena Arribas,
Esther Vilajosana, Elisabet Peralvez Torres, and Ricard Mesía. The multidisciplinary team
(mdt) approach and quality of care. Frontiers in oncology, 10:85, 2020.
[72] Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman
Cohan, and Mark Gerstein. Medagents: Large language models as collaborators for zero-shot
medical reasoning, 2024.
[73] Arsene Fansi Tchango, Rishab Goel, Zhi Wen, Julien Martel, and Joumana Ghosn. Ddxplus: A
new dataset for automatic medical diagnosis, 2022.
[74] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez,
Ting Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine.Nature medicine,
29(8):1930–1940, 2023.
[75] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez,
Ting Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine.Nature Medicine,
29(8):1930–1940, 2023.
[76] Daniel Truhn, Jan-Niklas Eckardt, Dyke Ferber, and Jakob Nikolas Kather. Large language
models and multimodal foundation models for precision oncology. NPJ Precision Oncology,
8(1):72, 2024.
[77] Tao Tu, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy
Wang, Brenna Li, Mohamed Amin, Nenad Tomasev, Shekoofeh Azizi, Karan Singhal, Yong
Cheng, Le Hou, Albert Webson, Kavita Kulkarni, S Sara Mahdavi, Christopher Semturs, Juraj
Gottweis, Joelle Barral, Katherine Chou, Greg S Corrado, Yossi Matias, Alan Karthikesalingam,
and Vivek Natarajan. Towards conversational diagnostic ai, 2024.
15
[78] Sean R Tunis, Daniel B Stryer, and Carolyn M Clancy. Practical clinical trials: increasing the
value of clinical research for decision making in clinical and health policy.Jama, 290(12):1624–
1632, 2003.
[79] Dave Van Veen, Cara Van Uden, Louis Blankemeier, Jean-Benoit Delbrouck, Asad Aali, Chris-
tian Bluethgen, Anuj Pareek, Malgorzata Polacin, Eduardo Pontes Reis, Anna Seehofnerová,
Nidhi Rohatgi, Poonam Hosamani, William Collins, Neera Ahuja, Curtis P. Langlotz, Jason
Hom, Sergios Gatidis, John Pauly, and Akshay S. Chaudhari. Adapted large language models can
outperform medical experts in clinical text summarization. Nature Medicine, 30(4):1134–1142,
February 2024.
[80] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen,
Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey
on large language model based autonomous agents, 2024.
[81] Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang Shen. Chatcad: Interactive
computer-aided diagnosis on medical image using large language models, 2023.
[82] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha
Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language
models. arXiv preprint arXiv:2203.11171, 2022.
[83] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,
Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.
Advances in neural information processing systems, 35:24824–24837, 2022.
[84] Thomas Weida and Jane Weida. Outpatient e/m coding simplified.Family Practice Management,
29(1):26–31, 2022.
[85] Robin M Weinick, Rachel M Burns, and Ateev Mehrotra. Many emergency department visits
could be managed at urgent care centers and retail clinics. Health affairs, 29(9):1630–1636,
2010.
[86] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun
Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, and
Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation, 2023.
[87] Richard C Wuerz, Leslie W Milne, David R Eitel, Debbie Travers, and Nicki Gilboy. Reliability
and validity of a new five-level triage instrument. Academic emergency medicine, 7(3):236–242,
2000.
[88] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang,
Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong,
Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin,
Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng,
Xipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language model
based agents: A survey, 2023.
[89] Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. Benchmarking retrieval-augmented
generation for medicine. arXiv preprint arXiv:2402.13178, 2024.
[90] Weixiang Yan, Haitian Liu, Tengxiao Wu, Qian Chen, Wen Wang, Haoyuan Chai, Jiayi Wang,
Weishan Zhao, Yixin Zhang, Renjun Zhang, et al. Clinicallab: Aligning agents for multi-
departmental clinical diagnostics in the real world. arXiv preprint arXiv:2406.13890, 2024.
[91] Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.2,
how to learn from mistakes on grade-school math problems, 2024.
[92] Cyril Zakka, Akash Chaurasia, Rohan Shad, Alex R. Dalal, Jennifer L. Kim, Michael Moor,
Kevin Alexander, Euan Ashley, Jack Boyd, Kathleen Boyd, Karen Hirsch, Curt Langlotz, Joanna
Nelson, and William Hiesinger. Almanac: Retrieval-augmented language models for clinical
medicine, 2023.
16
[93] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker,
Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent
Vanhoucke, and Pete Florence. Socratic models: Composing zero-shot multimodal reasoning
with language, 2022.
[94] Haodi Zhang, Jiahong Li, Yichi Wang, and Yuanfeng Songi. Integrating automated knowledge
extraction with large language models for explainable medical decision-making. In 2023 IEEE
International Conference on Bioinformatics and Biomedicine (BIBM), pages 1710–1717. IEEE,
2023.
[95] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi
Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering, 2023.
[96] Hongjian Zhou, Fenglin Liu, Boyang Gu, Xinyu Zou, Jinfa Huang, Jinge Wu, Yiru Li, Sam S.
Chen, Peilin Zhou, Junling Liu, Yining Hua, Chengfeng Mao, Chenyu You, Xian Wu, Yefeng
Zheng, Lei Clifton, Zheng Li, Jiebo Luo, and David A. Clifton. A survey of large language
models in medicine: Progress, application, and challenge, 2024.
[97] Junbin Zhou and Xiao Xu. The difficulty of medical decision-making: should patients be
involved? Hepatobiliary Surgery and Nutrition, 12(3):407, 2023.
17
Limitations and Future Works
Despite the successes of our framework in showing promising performance in medical decision-
making tasks, we recognize several limitations that open pathways for future research.
Medical Focused Foundation Models. An essential enhancement would be to incorporate the
foundation models and systems specifically trained on medical data, like Med-Gemini [63], AMIE
[77], and Med-PaLM 2 [67]. These models excel in generating professional medical terminologies,
which can facilitate more effective and accurate communication between multiple agents involved in
the decision-making process. By leveraging these specialized models, the agents can interact using
a shared, precise medical vocabulary, enhancing the system’s overall performance and reliability.
This approach not only ensures more medically accurate content generation but also supports better
collaboration and understanding among the agents, which is essential for complex medical decision-
making tasks.
Patient-Centered Diagnosis. A primary limitation lies in the fact that our current framework
operates within the confines of multiple-choice question answering and does not account for the
interactive, patient-centered nature of real-world diagnostics. Effective diagnosis often relies on
continuous exchanges that include the patient’s narrative, the physician’s expertise, and input from
caregivers. To bridge this gap, future iterations of our framework will aim to incorporate a more
interactive system that not only assists physicians but also directly engages with both patients and
caregivers in a multi-stakeholder approach. Moreover, by incorporating regret-aware [91] decision-
making, the system can learn to minimize diagnostic regret over time, refining its responses based
on the outcomes of prior interactions. This regret-aware framework will help guide the LLM to
seek additional information when uncertainties arise, thereby supporting more informed decisions
across complex, multi-stakeholder scenarios. Embedding these real-world interactions within the
feedback loop will enable the system to provide more nuanced and patient-centric support, enhancing
the quality and personalization of medical decision-making across all involved parties.
Potential Risks and Mitigations. While our framework shows promise, potential risks include
medical hallucinations and the generation of inaccurate or misleading information. To address these
risks, integrating self-correction mechanisms, such as those proposed by [42], could enable the model
to autonomously identify and rectify its own errors via reinforcement learning-based self-correction.
Additionally, implementing rule-based reward structures, as suggested in [56], would allow the model
to adhere to specific safety and accuracy guidelines during training. These methods can support a
safer, more reliable diagnostic support tool by introducing corrective feedback loops and standardized
behavior guidelines. Furthermore, integrating confidence scores and uncertainty estimates with the
model’s recommendations could enhance the decision-making process by enabling end-users to
weigh various diagnostic options, ultimately increasing the system’s trustworthiness and safety.
A Dataset Information
We evaluate multi-agent collaboration frameworks across seven common medical question-answering
datasets, which vary in question complexity. Generally, questions are deemed more complex if they
involve multiple modalities or entail a lengthy, detailed diagnostic task. Below, we detail each dataset
and provide a sample entry:
1. MedQA. The MedQA dataset consists of professional medical board exams from the US,
Mainland China, and Taiwan [35]. Our study focuses on the English test set, comprising
1,273 questions sourced from the United States Medical Licensing Examination (USMLE).
These questions are formatted as multiple-choice text queries with five options. Due to their
textual nature and brevity, we categorize these questions as low.
Sample Question: “A 47-year-old female undergoes a thyroidectomy for treatment of Graves’
disease. Post-operatively, she reports a hoarse voice and difficulty speaking. You suspect
that this is likely a complication of her recent surgery. What is the embryologic origin of the
damaged nerve that is most likely causing this patient’s hoarseness?”
Options: A: 1st pharyngeal arch, B: 2nd pharyngeal arch, C: 3rd pharyngeal arch, D: 4th
pharyngeal arch, E: 6th pharyngeal arch
18
Table 4: Summary of the Datasets. T : Text, I : Image, V : Video. In Appendix A, we provide
detailed sample information for each benchmark.
Dataset Modality Format Choice Testing Size Domain
MedQA T Question + Answer A/B/C/D 1273 US Medical Licensing Examination
PubMedQA T Question + Context + Answer Yes/No/Maybe 500 PubMed paper abstracts
DDxPlus T Question + Answer A/B/C/D/ · · · 134 K Pathologies, Symptoms and Antecedents from Patients
SymCat T Question + Answer A/B/C/D 369 K Disease-symptom records from public datasets
JAMA T Case + Question + Answer A/B/C/D 1524 Challenging real-world Clinical Cases from diverse Medical
Domains
MedBullets T Case + Question + Answer A/B/C/D 308 Online Platform Resources for Medical Study
MIMIC-CXR T I Question + Answer Closed Answer 1531 Chest X-ray images and Free-text Reports.
PMC-VQA T I Question + Answer A/B/C/D 50 K VQA pairs across Images, spanning diverse Modalities and
Diseases
Path-VQA T I Question + Answer Yes/No 3391 Open-ended Questions from Pathology Images
MedVidQA T V Question + Answer A/B/C/D 155 First Aids, Medical Emergency, and Medical Education
Questions
2. PubMedQA. PubMedQA is a QA dataset based on biomedical research [36]. It requires
yes/no/maybe answers to questions grounded in PubMed abstract. The dataset comprises
entries each containing a question, a relevant abstract minus the conclusion, and a ground
truth label. We used 50 samples for testing. Given its binary choice format, we consider the
complexity of this dataset to be low.
Sample Question: “Can predilatation in transcatheter aortic valve implantation be omit-
ted?”
Context: “The use of a balloon expandable stent valve includes balloon predilatation of the
aortic stenosis before valve deployment. The aim of the study was to see whether or not
balloon predilatation is necessary in transcatheter aortic valve replacement (TAVI). Sixty
consecutive TAVI patients were randomized to the standard procedure or to a protocol where
balloon predilatation was omitted. There were no significant differences between the groups
regarding early hemodynamic results or complication rates.""
3. DDXPlus. DDXPlus is a medical diagnosis dataset using synthetic patient information and
symptoms [73]. Each instance represents a patient, with attributes including age, sex, initial
evidences, evidence, multiple options of possible pathologies, and a ground truth diagnosis.
Due to its text-only and multiple-choice nature, we consider the complexity to be low.
Sample Patient Information: Age: 96, Sex: F
Evidences: [‘e66’, ‘insp_siffla’, ‘j45’, ‘posttus_emesis’, ‘trav1_@_N’, ‘vaccination’]
Initial evidence: ‘posttus_emesis’
Options: (A) Bronchite (B) Coqueluche
4. PMC-VQA. PMC-VQA is a large-scale medical visual question-answering dataset that
contains 227K Visual Question Answering (VQA) pairs of 149K images [95]. It is structured
as a multiple-choice QA task with one image input accompanying each question. Since the
query requires a model to consider both text and image inputs, while maintaining medical
expertise, we consider the complexity to be moderate.
Sample Question: What is the appearance of the hyperintense foci in the basal ganglia on
T1-weighted MRI image?
Image: PMC8415802_FIG1.jpg
Options: A: Hypodense, B: Hyperdense, C: Isointense, D: Hypointense
5. Path-VQA. PathVQA is a VQA dataset specifically on pathology images [ 30]. Differ-
ent from PMC-VQA which consists of multiple choice questions, Path-VQA includes
open-ended questions and binary ""yes/no"" questions. For the purpose of maintaining a
standardized accuracy evaluation, we use only the yes/no questions. Similar to PMC-VQA,
we consider the complexity to be moderate.
Sample Question: Was a gravid uterus removed for postpartum bleeding?
Image: test_0273.jpg
6. MedVidQA. MedVidQA dataset consists of 3,010 health-related questions with visual
answers from validated video sources (e.g. medical school, health institutions, etc). We
19
enhanced the dataset by using GPT-4 to generate multiple-choice answers, including one
correct ‘golden answer’ and several false options, expanding its use for training and evaluat-
ing automated medical question-answering systems.
Sample Question: How to perform corner stretches to treat neck pain?
Video: h5MvX50zTLM.mp4
Options: A: By bending your knees and touching your toes, B: By performing jumping jacks,
C: By leaning into a corner with your elbows up at shoulder level, D: By doing push-ups
7. SymCat. SymCat is a synthetic dataset which includes 5 million symptom-condition
samples, covering 801 distinct conditions each with 376 potential symptoms dataset [2].
Sample Patient Information: Age: 42, Gender: F , Race: White, Ethnicity: Nonhispanic
Sample Patient Symptoms: Pain:cramping:Abdomen:Lower abdomen::::Worsening after
meals::32; Altered stool:Lumpy:::::::Often:44; Flatulence:::::::::30; Bloating:::::::::41
Options: ""A"": ""Asthma"", ""B"": ""IBS (Constipation type)"", ""C"": ""Viral meningitis (Varicella
zoster virus)"", ""D"": ""Bacterial (Gastro)enteritis (Yersinia infection most likely)""
8. JAMA. JAMA includes 1524 clinical cases collected from the JAMA Network Clinical
Challenge archive, which are summaries of actual challenging clinical cases. Each sample
is framed as a question, with a long case description and four options [9].
Sample Case: A 62-year-old woman undergoing peritoneal dialysis (PD) for kidney failure
due to IgA nephropathy presented to the PD clinic with a 1-day history of severe abdominal
pain and cloudy PD fluid. Seven days prior, she inadvertently broke aseptic technique when
tightening a leaking connection of her PD catheter tubing. On presentation, she was afebrile
and had normal vital signs. Physical examination revealed diffuse abdominal tenderness.
Cloudy fluid that was drained from her PD catheter was sent for laboratory analysis (Table
1).Await peritoneal dialysis fluid culture results before starting intraperitoneal antibiotics.
Sample Question: What Would You Do Next?
""A"": ""Administer empirical broad-spectrum intraperitoneal antibiotics"", ""B"": ""Administer
empirical broad-spectrum intravenous antibiotics"", ""C"": ""Await peritoneal dialysis fluid
culture results before starting intraperitoneal antibiotics"", ""D"": ""Send blood cultures""
9. Medbullets. Medbullets comprises 308 USMLE Step 2/3 style questions collected from
open-access tweets on X (formerly Twitter) since April 2022. The difficulty is comparable
to that of Step 2/3 exams, which emulate common clinical scenarios [9].
Sample Question: A 2-week-old boy is evaluated by his pediatrician for abnormal feet. The
patient was born at 39 weeks via vaginal delivery to a G1P1 29-year-old woman. The
patient has been breastfeeding and producing 5 stools/day. He is otherwise healthy. His
temperature is 99.5˘00b0F (37.5 ˘00b0C), blood pressure is 60/38 mmHg, pulse is 150/min,
respirations are 24/min, and oxygen saturation is 98% on room air. A cardiopulmonary
exam is notable for a benign flow murmur. A musculoskeletal exam reveals the findings
shown in Figure A. Which of the following is the most appropriate next step in management?
Options: ""A"": ""Botulinum toxin injections"", ""B"": ""Reassurance and reassessment in 1
month"", ""C"": ""Serial casting"", ""D"": ""Surgical pinning"",
10. MIMIC-CXR-VQA. MIMIC-CXR is a large-scale visual question-answering dataset of
377,110 chest radiographs. It was obtained from 227,827 imaging studies sourced from
the BIDMC between 2011-2016. It includes patient identifiers which can be linked to
MIMIC-IV [3].
Sample Question: Are there any abnormalities in the upper mediastinum?
Image: p11/p11218589/s59138139/7a1e4762-c176bd78-6281fe5c-6b0c9734-e9a4c8f1.jpg
B Entropy Calculation for Consensus Dynamics
The entropy H serves as an indicator of consensus progression among the agents. It is quantified as:
H = −
MX
i=1
p(xi) log2 p(xi) (B.1)
where M is the total number of unique answers, xi represents a unique answer, and p(xi) is the
probability of occurring among all answers. This calculation helps to measure the degree of agreement
among the agents over time, with lower entropy indicating higher consensus. The trends in entropy
20
across different data modalities provide insights into how quickly and effectively MDAgents can
reach a unified decision.
C Prompt Templates
C.1 A single agent setting
{{instruction}}
The following are multiple choice questions (with answers) about medical knowledge.
{{few_shot_examples}}
{{context}} **Question:** {{question}} {{answer_choices}} **Answer:**(
Few-shot multiple choice questions
{{instruction}}
The following are multiple choice questions (with answers) about medical knowledge.
{{few_shot_examples w/ CoT Solutions}}
{{context}} **Question:** {{question}} {{answer_choices}} **Answer:**(
Chain-of-Thought multiple choice questions
{{instruction}}
The following are multiple choice questions (with answers) about medical knowledge.
{{few_shot_examples w/ CoT Solutions}}
{{context}} **Question:** {{question}} {{answer_choices}}
{{reasoning_paths}} **Answer:**(
Ensemble Refinement multiple choice questions
{{instruction}}
The following are multiple choice questions (with answers) about medical knowledge.
{{few_shot_examples w/ CoT Solutions from similarity calculation}}
for N times do
{{context}} **Question:** {{question}} {{shuffled_answer_choices}}
**Answer:**(
Medprompt multiple choice questions
C.2 Multi-agent setting
21
You are a medical expert who conducts initial assessment and your job is to decide the
difficulty/complexity of the medical query.
Now, given the medical query as below, you need to decide the difficulty/complexity of it:
{{question}}
Please indicate the difficulty/complexity of the medical query among below options:
1) low: a PCP or general physician can answer this simple medical knowledge checking question without
relying heavily on consulting other specialists.
2) moderate: a PCP or general physician can answer this question in consultation with other specialist in
a team.
3) high: Team of multi-departmental specialists can answer to the question which requires specialists
consulting to another department (requires a lot of team effort to treat the case).
**Answer:**(
Complexity check prompt
You are an experienced medical expert who recruits a group of experts with diverse identity and ask them
to discuss and solve the given medical query.
Now, given the medical query as below, you need to decide the difficulty/complexity of it:
{{question}}
You can recruit up to N experts in different medical expertise. Considering the medical question and the
options for the answer, what kind of experts will you recruit to better make an accurate answer?
Also, you need to specify the communication structure between experts (e.g., Pulmonologist ==
Neonatologist == Medical Geneticist == Pediatrician > Cardiologist)
For example, if you want to recruit five experts, you answer can be like:
{{examplers}}
Please answer in above format, and do not include your reason.
**Answer:**(
Recruiter prompt
D Additional Results
This section presents additional experimental results and analyses that provide further insights into
the performance and characteristics of our MDAgents framework.
You are a {{role}} who {{description}}. Your job is to collaborate with other medical experts in a
team.
Agent initialization prompt
Given the opinions from other medical agents in your team, please indicate whether you want to talk to
any expert (yes/no). If not, provide your opinion. {{opinions}}
Next, indicate the agent you want to talk to: {{agent_list}}
Remind your medical expertise and leave your opinion to an expert you chose. Deliver your opinion once
you are confident enough and in a way to convince other expert with a short reason.
Agent interaction prompt
22
You are a final medical decision maker who reviews all opinions from different medical experts and
makes the final decision.
Given the {{inputs}}, please review the {{inputs}} and make the final answer to the question by
{{decision_methods}} (e.g., majority voting, ensemble refinement).
**Answer:**(
Final decision prompt
Given the {{agent answers}}, please complete the following steps:
1. Take careful and comprehensive consideration of the provided reports.
2. Extract key knowledge from the reports.
3. Derive a comprehensive and summarized analysis based on the extracted knowledge.
4. Generate a refined and synthesized report based on your analysis.
Report Generation
D.1 Accuracy on entire MedQA 5-options Dataset
To provide a comprehensive evaluation of our approach, we conducted experiments on the entire
MedQA 5-options dataset using GPT-4o mini. This expands upon the subsampled experiments
presented in the main experiments in Table 2. Below Table shows the accuracy results for various
methods.
Table 5: Accuracy (%) on entire MedQA 5-options dataset with GPT-4o mini
.
Category Method Accuracy (%)
Single-agent
Zero-shot 71.5
3-shot 72.3
+ CoT 76.6
+ CoT-SC 77.2
Multi-agent (Multi-model) Majority V oting 76.3
Weighted V oting 79.1
Borda Count 76.1
Multi-agent (Single-model) Reconcile 80.2
Adaptive MDAgents (Ours) 83.6
These results demonstrate that our MDAgents approach outperforms both single-agent and other
multi-agent methods across the full dataset, achieving an accuracy of 83.6%. This underscores the
effectiveness of our framework in handling diverse medical questions at scale.
D.2 Estimated Costs for Full Test Set Experiments
To provide transparency and aid in reproducibility, we estimated the costs associated with running
experiments on the entire test sets using GPT-4 (Vision). Below Table presents these cost estimates in
USD for various datasets and methods.
Table 6: Estimated costs for experimenting with entire test sets with GPT-4 (Vision) (in USD)
Method MedQA PubMedQA Path-VQA PMC-VQA DDXPlus SymCat JAMA MedBullets MIMIC-CXR Total Cost
CoT 55.24 13.16 3,028.54 27,134.00 16,461.90 10,593.99 134.55 61.23 1,388.70 58,871.29
Ours 172.43 41.36 9,369.45 82,194.34 44,814.97 31,176.05 367.13 161.70 4,406.90 172,704.33
While our approach incurs higher costs due to its multi-agent nature, the significant performance
improvements justify this increased computational expense for critical medical decision-making
tasks.
23
D.3 Impact of Knowledge Enhancement with RAG
Method Accuracy (%)
MDAgents (baseline) 71.8
+ MedRAG 75.2
+ Medical Knowledge Initialization 76.0
+ Moderator’s Review 77.6
+ Moderator’s Review & MedRAG 80.3
Table 7: Impact of knowledge enhancement on MDAgents performance
We investigated whether simply assigning roles to agents is sufficient for expert-like performance,
and explored the impact of equipping agents with different knowledge using Retrieval-Augmented
Generation (RAG). Table 7 presents the results of these experiments.
These results indicate that while role assignment provides a foundation, augmenting agents with
specific knowledge (using MedRAG) and structured reviews (Moderator’s Review) significantly
enhances their ability to simulate domain expertise. The combination of Moderator’s Review and
MedRAG yielded the best performance, highlighting the synergy between structured collaboration
and domain-specific knowledge retrieval.
D.4 Complexity Assignment and Collaborative Settings
To address the impact of complexity assignment on accuracy and API costs, we conducted additional
experiments focusing on high-complexity cases, particularly in image+text scenarios. Table 8 shows
the results for various collaborative settings.
Collaboration Setting Accuracy (%)
Sequential & No Discussion 39.0
Sequential & Discussion 45.0
Parallel & No Discussion 56.0
Parallel & Discussion 59.0
Table 8: Impact of collaboration settings on high-complexity image+text tasks
These results underscore the importance of multi-turn discussions, particularly in complex cases. The
parallel collaboration with discussion yielded the highest accuracy (59.0%), suggesting that enabling
agents to work simultaneously and engage in dialogue is crucial for handling intricate medical queries.
The significant performance gap between discussion and no-discussion scenarios (45.0% vs. 39.0%
for sequential, and 59.0% vs. 56.0% for parallel) highlights the value of interactive deliberation in
medical decision-making processes.
24
Table 9: Accuracy (%) on Medical benchmarks with Solo (
 ) setting. Bold represents the best and
Underlined represents the second best performance for each benchmark and model.
Category Method
Medical Knowledge Retrieval Datasets
MedQA
T
PubMedQA
T
Path-VQA
I T
PMC-VQA
I T
MedVidQA
V T
GPT-3.5
Zero-shot 48.5 ±3.3 56.8 ±12.0 - - -
Few-shot 47.8 ±16.4 59.0 ±1.0 - - -
+ CoT 54.2 ±8.9 49.7 ±11.9 - - -
+ CoT-SC 60.5 ±3.2 49.4 ±13.5 - - -
ER 60.2 ±4.0 52.9 ±15.9 - - -
Medprompt 60.1 ±10.8 59.8 ±8.5 - - -
GPT-4(V)
Zero-shot 75.0 ±1.3 61.5 ±2.2 57.9 ±1.6 49.0 ±3.7 -
Few-shot 72.9 ±11.4 63.1 ±11.7 57.5 ±4.5 52.2 ±2.0 -
+ CoT [83] 82.5 ±4.9 57.6 ±9.2 58.6 ±3.1 51.3 ±1.5 -
+ CoT-SC [82] 83.9 ±2.7 58.7 ±5.0 61.2 ±2.1 50.5 ±5.2 -
ER [67] 81.9 ±2.1 56.0 ±7.0 61.4 ±4.1 52.7 ±2.9 -
Medprompt [59] 82.4 ±5.1 51.8 ±4.6 59.2 ±5.7 53.4 ±7.9 -
Gemini-Pro(Vision)
Zero-shot 42.0 ±10.4 65.2 ±14.5 45.9 ±2.8 44.8 ±2.0 37.9 ±8.4
Few-shot 34.0 ±7.2 55.0 ±0.0 64.5 ±2.3 48.2 ±1.0 47.1 ±8.6
+ CoT 50.0 ±6.0 60.2 ±9.0 66.4 ±11.7 47.1 ±4.2 48.6 ±5.5
+ CoT-SC 52.7 ±4.6 55.8 ±8.9 63.6 ±6.0 46.3 ±2.8 49.2 ±8.2
ER 52.0 ±7.2 58.4 ±14.2 57.6 ±8.4 38.4 ±2.0 48.5 ±4.1
Medprompt 45.3 ±3.1 50.6 ±5.4 55.0 ±2.0 41.8 ±3.0 44.5 ±2.0
Category Method
Clinical Reasoning and Diagnostic Datasets
DDXPlus
T
SymCat
T
JAMA
T
MedBullets
T
MIMIC-CXR
I T
GPT-3.5
Zero-shot 56.2 ±4.1 84.0 ±0.0 36.0 ±3.3 56.0 ±2.8 -
Few-shot 48.9 ±8.5 86.0 ±2.8 38.0 ±4.2 59.0 ±1.4 -
+ CoT 52.8 ±5.4 82.0 ±0.0 34.0 ±2.4 56.0 ±5.7 -
+ CoT-SC 37.8 ±6.1 80.0 ±2.8 43.0 ±4.2 63.0 ±4.2 -
ER 42.3 ±6.9 84.0 ±1.8 44.0 ±5.7 58.0 ±0.0 -
Medprompt 41.2 ±6.2 86.0 ±2.6 43.0 ±1.4 54.0 ±5.7 -
GPT-4(V)
Zero-shot 70.3 ±2.0 88.7 ±2.3 62.0 ±2.0 67.0 ±1.4 40.0 ±5.3
Few-shot 69.4 ±1.0 86.7 ±3.1 69.0 ±4.2 72.0 ±2.8 35.3 ±5.0
+ CoT [83] 72.7 ±7.7 78.0 ±2.0 66.0 ±5.7 70.0 ±0.0 36.2 ±5.2
+ CoT-SC [82] 52.1 ±6.4 83.3 ±3.1 68.0 ±2.8 76.0 ±2.8 51.7 ±4.0
ER [67] 61.3 ±2.4 82.7 ±2.3 71.0 ±1.4 76.0 ±5.7 50.0 ±0.0
Medprompt [59] 59.5 ±17.7 87.3 ±1.2 70.7 ±4.3 71.0 ±1.4 53.4 ±4.3
Gemini-Pro(Vision)
Zero-shot 49.9 ±6.5 88.9 ±6.4 42.7 ±3.9 40.0 ±1.5 40.0 ±2.8
Few-shot 47.1 ±5.6 89.2 ±3.4 41.0 ±1.4 44.0 ±3.9 39.2 ±1.2
+ CoT [83] 65.5 ±4.9 91.9 ±3.4 38.0 ±1.6 52.7 ±7.1 45.2 ±6.8
+ CoT-SC [82] 60.3 ±2.4 92.0 ±1.5 46.0 ±0.0 51.0 ±4.2 54.9 ±3.4
ER [67] 46.7 ±6.9 58.5 ±7.5 50.8 ±5.8 53.2 ±7.8 53.2 ±3.5
Medprompt [59] 58.2 ±5.5 92.5 ±4.5 44.4 ±3.2 54.0 ±5.7 51.2 ±1.9
* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement
* T : text-only, I : image+text, V : video+text
* All experiments were tested with 3 random seeds
25
Table 10: Accuracy (%) on Medical benchmarks with Group (
 ) setting. Bold represents the best
performance for each benchmark and model.
Category Method
Medical Knowledge Retrieval Datasets
MedQA
T
PubMedQA
T
Path-VQA
I T
PMC-VQA
I T
MedVidQA
V T
GPT-3.5
Majority V oting 60.4 ±2.1 68.5 ±9.6 - - -
Weighted V oting 57.3 ±3.0 65.8 ±11.4 - - -
Borda Count 55.3 ±7.1 70.2 ±8.8 - - -
MedAgents [72] 56.0 ±5.3 55.0 ±1.4 - - -
GPT-4(V)
Majority V oting 80.6 ±2.9 72.2 ±6.9 56.9 ±19.7 36.8 ±6.7 -
Weighted V oting 78.8 ±1.1 72.2 ±6.9 62.1 ±13.9 25.4 ±9.0 -
Borda Count 70.3 ±8.5 66.9 ±3.0 61.9 ±8.1 27.9 ±5.3 -
MedAgents [72] 79.1 ±7.4 69.7 ±4.7 45.4 ±8.1 39.6 ±3.0 -
Gemini-Pro(Vision)
Majority V oting 51.6 ±2.2 65.3 ±12.9 58.2 ±7.3 27.1 ±5.4 50.8 ±7.4
Weighted V oting 52.3 ±3.3 63.7 ±10.0 66.4 ±11.1 20.9 ±3.8 57.8 ±2.1
Borda Count 49.4 ±9.7 57.7 ±15.0 68.2 ±1.8 25.3 ±8.7 54.5 ±4.7
MedAgents [72] 48.4 ±5.5 63.6 ±6.0 64.9 ±12.5 35.1 ±3.1 61.6 ±4.8
Multi-agent
Reconcile [10] 81.3 ±3.0 79.7 ±3.2 57.5 ±3.3 31.4 ±1.2 -
AutoGen [86] 60.6 ±5.0 77.3 ±2.3 43.0 ±8.9 37.3 ±6.1 -
DyLAN [51] 64.2 ±2.3 73.6 ±4.2 41.3 ±1.2 34.0 ±3.5 -
Meta-Prompting [70] 80.6 ±1.2 73.3 ±2.3 55.3 ±2.3 42.6 ±4.2 -
Category Method
Clinical Reasoning and Diagnostic Datasets
DDXPlus
T
SymCat
T
JAMA
T
MedBullets
T
MIMIC-CXR
I T
GPT-3.5
Majority V oting 53.6 ±2.2 83.7 ±3.3 47.0 ±1.4 46.0 ±4.1 -
Weighted V oting 55.2 ±2.0 85.9 ±3.0 49.0 ±1.4 43.0 ±8.4 -
Borda Count 63.9 ±12.1 84.9 ±1.6 50.0 ±1.0 45.0 ±5.6 -
MedAgents [72] 47.3 ±11.0 87.0 ±4.2 41.0 ±3.3 56.0 ±6.2 -
GPT-4(V)
Majority V oting 67.8 ±4.9 91.9 ±2.2 70.0 ±5.7 70.0 ±0.0 49.5 ±10.7
Weighted V oting 65.9 ±3.3 90.5 ±2.9 66.1 ±4.1 66.0 ±5.7 53.5 ±2.2
Borda Count 67.1 ±6.7 78.0 ±11.8 61.0 ±5.6 66.0 ±2.8 45.3 ±6.8
MedAgents [72] 62.8 ±5.6 90.0 ±0.0 66.0 ±5.7 77.0 ±1.4 43.3 ±7.0
Gemini-Pro(Vision)
Majority V oting 52.3 ±15.3 73.5 ±4.9 47.0 ±1.4 44.0 ±4.2 47.9 ±6.6
Weighted V oting 54.3 ±16.9 64.6 ±6.5 42.0 ±3.2 43.6 ±3.9 43.2 ±2.0
Borda Count 67.0 ±27.7 77.3 ±9.3 37.0 ±1.8 46.1 ±3.2 44.6 ±4.5
MedAgents [72] 43.0 ±2.7 80.5 ±1.9 40.8 ±2.9 50.5 ±3.6 38.7 ±1.5
Multi-agent
Reconcile [10] 68.4 ±7.4 90.6 ±2.5 60.7 ±5.7 59.5 ±8.7 33.3 ±3.4
AutoGen [86] 67.3 ±11.8 73.3 ±3.1 64.6 ±1.2 55.3 ±3.1 43.3 ±4.2
DyLAN [51] 56.4 ±2.9 75.3 ±4.6 60.1 ±3.1 57.3 ±6.1 38.7 ±1.2
Meta-Prompting [70] 52.6 ±6.1 77.3 ±2.3 64.7 ±3.1 49.3 ±1.2 42.0 ±4.0
* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement
* T : text-only, I : image+text, V : video+text
* All experiments were tested with 3 random seeds
Table 11: Accuracy (%) on Medical benchmarks with Our (
 ) method. Bold represents the
best performance for each benchmark and model.
Method
Medical Knowledge Retrieval Datasets
MedQA
T
PubMedQA
T
Path-VQA
I T
PMC-VQA
I T
MedVidQA
V T
GPT-3.5 64.0 ±1.6 66.0 ±5.7 - - -
GPT-4(V) 88.7 ±4.0 75.0 ±1.0 65.3 ±3.9 56.4 ±4.5 -
Gemini-Pro(Vision) 57.4 ±1.8 71.0 ±1.6 72.0 ±2.3 62.2 ±7.6 56.2 ±6.7
Method
Clinical Reasoning and Diagnostic Datasets
DDXPlus
T
SymCat
T
JAMA
T
MedBullets
T
MIMIC-CXR
I T
GPT-3.5 62.5 ±6.7 85.7 ±3.0 48.6 ±6.5 55.3 ±4.3 -
GPT-4(V) 77.9 ±2.1 93.1 ±1.0 70.9 ±0.3 80.8 ±1.7 55.9 ±9.1
Gemini-Pro(Vision) 59.2 ±1.2 65.0 ±6.1 47.0 ±2.2 42.9 ±3.4 48.1 ±5.8
* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement
* T : text-only, I : image+text, V : video+text
* All experiments were tested with 3 random seeds
26
Table 12: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive settings with increased
number of samples (N=100). All benchmarks except for MedVidQA ( Gemini 1.5 Flash) were
evaluated with GPT-4o mini.
Category Method MedQA
T
PubMedQA
T
Path-VQA
I T
PMC-VQA
I T
MedVidQA
V T
Single-agent
Zero-shot 75.0 54.0 58.0 48.0 50.0
Few-shot 77.0 55.0 58.0 50.0 51.0
+ CoT 78.0 50.0 59.0 52.0 53.0
+ CoT-SC 79.0 51.0 60.0 53.0 53.0
ER 76.0 51.0 61.0 51.0 52.0
Medprompt 79.0 58.0 60.0 54.0 53.0
Multi-agent
(Single-model)
Majority V oting 79.0 68.0 63.0 52.0 54.0
Weighted V oting 80.0 68.0 64.0 51.0 55.0
Borda Count 81.0 69.0 62.0 50.0 52.0
MedAgents 80.0 69.0 55.0 52.0 50.0
Meta-Prompting 82.0 69.0 56.0 49.0 -
Multi-agent
(Multi-model)
Reconcile 83.0 70.0 58.0 45.0 -
AutoGen 65.0 63.0 45.0 40.0 -
DyLAN 68.0 67.0 42.0 48.0 -
Adaptive MDAgents (Ours) 87.0 71.0 60.0 55.0 56.0
Category Method DDXPlus
T
SymCat
T
JAMA
T
MedBullets
T
MIMIC-CXR
I T
Single-agent
Zero-shot 53.0 84.0 57.0 49.0 38.0
Few-shot 60.0 87.0 58.0 52.0 33.0
+ CoT 66.0 84.0 55.0 64.0 33.0
+ CoT-SC 68.0 84.0 57.0 60.0 40.0
ER 76.0 80.0 56.0 59.0 43.0
Medprompt 70.0 84.0 62.0 60.0 43.0
Multi-agent
(Single-model)
Majority V oting 53.0 82.0 56.0 59.0 54.0
Weighted V oting 52.0 86.0 56.0 56.0 52.0
Borda Count 53.0 86.0 56.0 59.0 51.0
MedAgents 56.0 80.9 51.0 58.0 40.9
Meta-Prompting 53.0 79.0 56.0 51.0 48.0
Multi-agent
(Multi-model)
Reconcile 60.0 87.0 59.0 60.0 43.3
AutoGen 47.0 87.0 53.0 55.0 47.0
DyLAN 54.0 84.0 55.0 57.0 42.0
Adaptive MDAgents (Ours) 75.0 89.0 59.0 67.0 56.0
* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement
* T : text-only, I : image+text, V : video+text
27
Figure 8: Complexity Distribution for each dataset classified by GPT-4(V) and Gemini-Pro (Vision)
(for MedVidQA). The plot illustrates the varying levels of medical complexity across datasets,
reflecting the diverse nature of medical question answering, diagnostic reasoning, and medical visual
interpretation tasks. For instance, MedQA is categorized under Medical Knowledge Retrieval due to
their focus on text-based questions and literature synthesis, while MIMIC-CXR, categorized under
Clinical Reasoning and Diagnostic tasks, shows a high complexity distribution due to the need for
interpreting detailed radiographic images (See Section in Section 4.1 for the task categorization)
28
: text-only: image+ text: video+ text
Figure 9: Top-3 most recruited medical experts in each benchmark. The alignment between the
dataset characteristics and the recruited experts is evident in several cases. For instance, MIMIC-CXR,
which features chest x-ray images, predominantly recruits Radiologists, Pulmonologists, and experts
in Biomedical Informatics due to their expertise in interpreting medical imaging.
Figure 10: Simplified agent structure examples assigned during the expert recruitment process ranging
from (a) A Primary Care Clinician (PCC), (b) Multi-disciplinary Team (MDT), (C) MDT w/ hierarchy
to (d) Integrated Care Team (ICT).
29
Algorithm 1 Adaptive Medical Decision-making Framework
Require: Problem Q
1: Complexity ← COMPLEXITY CHECK (Q) ▷ Determine the complexity of the medical query
2: if Complexity = low then
3: Agent ← RECRUIT (Q, Complexity) ▷ Recruit a Primary Care Clinician agent
4: ans ← Agent(Q)
5: else if Complexity = moderate then
6: MDT ← RECRUIT (Q, Complexity) ▷ Recruit a Multi-disciplinary Team
7: Agent ← RECRUIT (Q, Complexity, MDT)
8: r ← 0
9: Consensus ← False
10: Interaction ← []
11: while r ≤ R and not Consensus do
12: Consensus, Log← COLLABORATIVE DISCUSSION (Q, MDT) ▷ Iterative discussions
13: if not Consensus then
14: for all Agent ∈ MDT do
15: F eedback← Moderator (Interaction, Agent) ▷ Review and provide feedback
16: Agent.UPDATE (F eedback) ▷ Update the feedback
17: end for
18: Interaction ← Interaction + [Log] + [F eedback]
19: end if
20: r ← r + 1
21: end while
22: ans ← Agent(Q, Interaction) ▷ Moderator agent makes the final decision
23: else
24: ICT ← RECRUIT (Q, Complexity) ▷ Recruit an Integrated Care Team
25: Reports ← []
26: for T eam∈ ICT do
27: Report ← GENERATE REPORT (Q, T eam) ▷ Each Team curates a report
28: Reports ← Reports + [Report]
29: end for
30: ans ← Agent(Q, Reports) ▷ Final decision made
31: end if
32: return ans
30
E Case Study
E.1 Medical Decision Making Case Studies
MDM requires efforts of both individual expertise and collaboration to navigate the complexities of
patient care. Clinicians often face challenging scenarios that necessitate a comprehensive approach,
integrating insights from various specialties to arrive at the best possible outcomes.
E.1.1 Real-World Medical Cases
Below are the real-world example cases that could be classified as low, moderate, to high complexity
cases.
Case 1: Adjusting Medication Dosage for Chronic Disease (Low Complexity) A 55-year-old
female patient with type 2 diabetes visits her PCP for a routine check-up. The patient has been
taking 500 mg of metformin orally twice a day and has been adhering to a low-carbohydrate diet.
Upon testing with fasting glucose level, the glucose level is above normal. PCP reviewed the current
medical dosage and increased the dosage to manage the blood glucose level of the patient.
Case 2: Differential Diagnosis in the Emergency Department (Moderate Complexity) A 40-
year-old male patient arrives at the emergency department (ED) with a high fever, severe headache,
and muscle pain, raising concerns about a potential infectious disease. The ED physician conducts an
initial examination but recognizes the need for a more detailed evaluation to identify the underlying
cause. The patient is referred to the infectious disease department for further assessment. An
infectious disease specialist, along with the ED physician, reviews the patient’s symptoms, travel
history, and recent exposures. They collaborate on ordering specific diagnostic tests, including blood
cultures and imaging studies. Through this teamed decision-making process, they diagnose the patient
with dengue fever and promptly initiate appropriate antiviral treatment.
Case 3: Managing Adverse Responses to Medication in Chronic Disease (High Complexity)
A 60-year-old female patient with chronic heart failure has been experiencing new symptoms of
shortness of breath and mild fever, suggesting either a complication due to her chronic heart failure
or a new infection. The urgent care doctor identifies the severity of the situation and promptly refers
the patient to the emergency department of a large hospital, where the patient has triaged to see a
cardiologist and an infectious disease doctor for specialized care. The team conducts a detailed review
of the patient’s medication history and current symptoms, does a physical exam to listen to lung
sounds, and orders a few exams including labs, a chest x-ray, echocardiogram, and electrocardiogram.
The team identifies that the patient has pulmonary effusion and upper respiratory viral infection.
E.1.2 Medical Cases from MedQA Dataset
Now, let us look at the cases from the MedQA [ 35] dataset that illustrate either individual PCP
or teamed decision-making is crucial in managing medical conditions, ranging from low to high
complexity levels of potential cases. These examples highlight the importance of checking the
complexity of the case for proper management.
Case 4: Diagnosis by PCP The case below with the “Low Complexity” header is classified as low
complexity by a medical doctor. In this case, a PCP can answer this question without consulting a
gastroenterologist. The diagnosis of gastric cancer and management based on the manifestation of
the disease, that has been described in this question and beyond should be from a gastroenterologist.
However, PCPs are expected to have the basic scientific and pathophysiological knowledge that is
related to gastric cancer and use that knowledge to solve this problem.
Case 5: Diagnosis and management by single Pediatric Endocrinologist The case below with
the “Moderate Complexity” header is classified as moderate complexity by the medical doctor. In
this case, a pediatric endocrinologist (specialist) alone can diagnose a patient and have a treatment
plan. Note that this patient could have been referred to this pediatric endocrinologist by a PCP who is
regularly seeing this patient.
31
Question: A 70-year-old man comes to the physician because of a 4-month history of epigastric pain,
nausea, and weakness. He has smoked one pack of cigarettes daily for 50 years and drinks one alcoholic
beverage daily. He appears emaciated. He is 175 cm (5 ft 9 in) tall and weighs 47 kg (103 lb); BMI is 15
kg/m². He is diagnosed with gastric cancer. Which of the following cytokines is the most likely direct
cause of this patient’s examination findings?
Answer:
A) TGF-β
B) IL-6
C) IL-2
D) TNF-β
Low Complexity
Question: A 5-year-old girl is brought to the clinic by her mother for excessive hair growth. Her mother
reports that for the past 2 months she has noticed hair at the axillary and pubic areas. She denies any
family history of precocious puberty and reports that her daughter has been relatively healthy with an
uncomplicated birth history. She denies any recent illnesses, weight change, fever, vaginal bleeding, pain,
or medication use. Physical examination demonstrates Tanner stage 4 development. A pelvic ultrasound
shows an ovarian mass. Laboratory studies demonstrate an elevated level of estrogen. What is the most
likely diagnosis?
Answer:
A) Granulosa cell tumor
B) Idiopathic precocious puberty
C) McCune-Albright syndrome
D) Sertoli-Leydig tumor
Moderate Complexity
Case 6: Diagnosis and management by multidisciplinary team The case below is of a “High
Complexity” patient primarily having neurological symptoms but with problems with vision, which
requires a neurologist to consult to ophthalmology department for further evaluation.
Question: A 63-year-old woman presents to her primary-care doctor for a 2-month history of vision
changes, specifically citing the gradual onset of double vision. Her double vision is present all the time
and does not get better or worse throughout the day. She has also noticed that she has a hard time
keeping her right eye open, and her right eyelid looks ’droopy’ in the mirror. Physical exam findings
during primary gaze are shown in the photo. Her right pupil is 6 mm and poorly reactive to light. The
rest of her neurologic exam is unremarkable. Laboratory studies show an Hb A1c of 5.0%. Which of the
following is the next best test for this patient?
Answer:
A) Direct fundoscopy
B) Intraocular pressures
C) MR angiography of the head
D) Temporal artery biopsy
High Complexity
E.2 Cases Studies with MDAgents
In this section, we provide two examples of our framework with moderate (Figure 11) and high
(Figure 12) complexity in PMC-VQA (image+text) and DDXPlus (text-only) respectively. These
32
Question: What does the image of Gross Sectioned Myelolipoma demonstrate?
Surgeon(3)
Moderator
Oncologist(4)
Endocrinologist(5)
Pathologist(1)
Radiologist(2)
Step 2. Expert Recruitment
(A) fibrosis(B) inflammation(C) hemorrhage(D) calcification
Step 3. Initial Assessment
(1) Answer: (A) fibrosis
(2) Answer: (A) fibrosis
(3) Answer:(C) hemorrhage
(4) Answer:(C) hemorrhage
(5) Answer: (A) fibrosis
Step 4. Collaborative Discussion
(3)→ (1): I am a surgeon who specializes in surgical procedures for various medical conditions. I have extensive experience in diagnosing and treating a wide range of surgical conditions, including cancer, trauma, cardiovascular disease, and gastrointestinal disorders.I believe that the image shows (C) hemorrhage (red arrow). Hemorrhageis the escape of blood from a blood vessel. It can occur due to a variety of factors, including trauma, bleeding disorders, and certain medical conditions. In the image, the red arrow points to an area of the tissue that is dark red and appears to be surrounded by a thin rim of white tissue. This is consistent with the appearance of hemorrhage.
(4)→ (3): I am an oncologist who specializes in the diagnosis and treatment of cancer. I believe that the image of Gross Sectioned Myelolipoma demonstrates (C) hemorrhage (red arrow). Hemorrhage is the escape of blood from a blood vessel. It can occur due to a variety of factors, including trauma, bleeding disorders, and certain medical conditions.In the image, the red arrow points to an area of the tissue that is dark red and appears to be surrounded by a thin rim of white tissue. This is consistent with the appearance of hemorrhage.I am confident in my opinion because I have extensive experience in diagnosing and treating cancer. I have seen many cases of hemorrhage, and I am familiar with the appearance of this condition.I would like to discuss my interpretation of the image with you. I believe that the image shows hemorrhage. I am confident that we can come to a consensus on the diagnosis.
Round 1
Step 1. Complexity CheckIntermediateWhile the identification of gross pathology might seem straightforward to a specialist in pathology, the distinction between fibrosis, hemorrhage, and other conditions can sometimes be subtle and subject to interpretation, as evidenced by the detailed discussion and disagreement among the agents with different medical backgrounds in the provided scenario.
Turn 1
(2)→ (3): I am a radiologist who focuses on the interpretation of medical imaging, such as X-rays, CT scans, MRIs, and ultrasounds. I have extensive experience in diagnosing and treating a wide range of medical conditions using medical imaging.I believe that the image demonstrates (A) fibrosis (red arrow). Fibrosis is a common finding in myelolipomas, which are benign tumors composed of mature fat cells and hematopoietic tissue. (C) Hemorrhage, on the other hand, is less common and typically appears as a dark red or black area on imaging. I am confident in my interpretation of the image and believe that the surgeon may have misinterpreted the findings.
Turn 2 Turn 2I do not want to talk to any other experts.I am confident in my opinion that the image shows (C) hemorrhage. I have carefully considered the opinions of the other medical experts, but I believe that the area pointed to by the red arrow is more consistent with the appearance of hemorrhage than fibrosis.I am open to discussing my opinion further with the other medical experts, but I do not believe that it is necessary to talk to any other experts at this time.
(3)→ (2): I am a surgeon who specializes in surgical procedures for various medical conditions.I believe that the image shows (C) hemorrhage (red arrow). Hemorrhage is the escape of blood from a blood vessel. It can occur due to a variety of factors, including trauma, bleeding disorders, and certain medical conditions. (…)
Turn 1
 Turn 2
Step 5. Review and Final Decision
The decision is challenging due to the conflicting interpretations of the image. However, the arguments for hemorrhage by Agent 3 (Surgeon) and Agent 4 (Oncologist) are based on the visual characteristics that are grossly consistent with hemorrhage –specifically, the dark red area suggested by the red arrow. In clinical practice, the appearance of hemorrhage in a gross section is often a dark red coloration, which is indeed what the image seems to display.While the expertise of Agent 1 (Pathologist) in tissue examination is crucial and their argument for fibrosisis noted, the gross pathological features described by Agent 3 (Surgeon) and Agent 4 (Oncologist) seem to align more closely with the typical presentation of a hemorrhage. Furthermore, hemorrhage is a well-documented complication in myelolipomas, particularly larger ones or those subjected to trauma.Therefore, based on the analysis of the conversation history, the final decision is aligned with interpretation (C) hemorrhage (red arrow)as argued by Agent 3 (Surgeon) in Round 1, Turns 1 and 3, and Agent 4 (Oncologist) in Round 1, Turn 2. This conclusion is supported by the gross pathological features consistent with hemorrhage rather than fibrosis, despite the reasoned opinions for fibrosis by Agents 1 (Pathologist), 2 (Radiologist), and 5 (Endocrinologist).
Interaction Log
Turn 1Turn 2Turn 3
Turn 4
Round 1
(4)→ (1): I am an oncologist who specializes in the diagnosis and treatment of cancer. I believe that the image of Gross Sectioned Myelolipoma demonstrates (C) hemorrhage (red arrow).Hemorrhage is the escape of blood from a blood vessel. I understand that you believe the image shows (A) fibrosis, but I disagree. I believe that the image shows hemorrhage because the area pointed to by the red arrow is dark red and appears to be surrounded by a thin rim of white tissue. This is consistent with the appearance of hemorrhage, not fibrosis.I am also aware that you are a specialist in the study and diagnosis of disease through examination of tissue samples, but I believe that my expertise in oncology gives me a unique perspective on this image.I believe that my argument is strong and that I can convince the pathologist of my opinion. I am confident in my diagnosis of hemorrhage, and I am willing to discuss the image further to support my opinion.
Turn 3
GT: (C) hemorrhage
: correct opinion: incorrect opinion
Terminate
Figure 11: Illustration of our proposed framework in moderate complexity setting. Given a medical
query (image + text) the framework performs reasoning in five steps: (i) complexity check, (ii)
expert recruitment, (iii) initial assessment, (iv) collaborative discussion, and (v) review and final
decision-making. Green text represents the correct answer and the Red text represents the incorrect
answer.
case studies reveals how our framework provides an environment for agents to collaborate, gather
information, moderate and make final decisions in complex medical scenarios.
33
Question: Based on the following information, please indicate the most likely diagnosis:Step 2. Expert Recruitment(A)Myasthénie grave (B)Syndrome de Guillain-Barré (C)Embolie pulmonaire (D) Myocardite (E) Réaction dystonique aïgue (F) OAP/Surcharge pulmonaire (G) Fibrillation auriculaire/Flutter auriculaire (H) Anémie (I) Anaphylaxie (J) Chagas (K) Scombroïde
Step 3. Information Gathering and Report Generation 
Neurologist (Lead)→ Ophthalmologist, Speech Therapist: Based on the symptoms presented, which include jaw claudication (claud_mâchoire), double vision (diplopie), speech disorder (dysarthrie), difficulty swallowing (dysphagie), muscle weakness (faiblesse_msmi), eyelid dropping (ptose), and unusual fatigue (fatigabilité_msk), the most likely diagnosis appears to be (A) Myasthenia Gravis (Myasthéniegrave).For further investigations:1. Ophthalmologist: Since double vision and eyelid drooping are some of the common symptoms of Myasthenia Gravis, the ophthalmologist may conduct a comprehensive eye examination. This might include testing for alterations in eye movement and eyelid strength.2. Speech therapist: Given the occurrence of dysarthria (speech disorder) and dysphagia (difficulty in swallowing), a Speech therapist may be involved to evaluate the patient's speech and swallowing abilities. They may conduct a detailed evaluation which includes observations of strength and coordination of speech muscles, assessment of speech clarity, and swallowing test to confirm the diagnosis.
Step 1. Complexity CheckHighThe query is classified as highcomplexity due to the diverse and severe neurological symptoms like double vision, slurred speech, and muscle weakness, which suggest multiple underlying conditions. This complexity necessitates collaboration among multiple specialists, including neurologists and pulmonologists, to accurately diagnose and treat the patient, involving extensive differential diagnosis and integrated clinical decision-making.
Step 4. Final Decision
Moderator: Based on the provided patient information and clinical evidence, the most likely diagnosis is (A) Myasthenia Gravis (Myasthéniegrave). The symptoms such as fatigue, weakness in the muscles, difficulty in swallowing (dysphagia), difficulty in speaking (dysarthrie), double vision (diplopie), and drooping of one or both eyelids (ptose) are all indicative of Myasthenia Gravis.
GT: (A) Myasthénie grave
Patient Information§Age: 20§Sex:M§Initial Evidence: fatigabilité_msk§ClinicalEvidence: ['atcdfam_mg', 'claud_mâchoire', 'diplopie', 'dysarthrie', 'dysphagie', 'dyspn', 'faiblesse_msmi', 'fatigabilité_msk', 'ptose', 'trav1_@_N']
 Speech
Ophthalmologist
Neurologist
Rheumatologist
Moderator
IAT
ElectrophysiologistPulmonologist
DietitianPhysical Therapist
Clinical Psychologist
DET
PHT
Patient AdvocateCare Coordinator
Senior Consultants
PRDT
Ophthalmologist: As an ophthalmologist on the Initial Assessment Team, I focused my investigation on the ocular symptoms presented by the patient, particularly diplopia and ptosis, both indicative of Myasthenia Gravis (MG).(…)The key findings were as follows:1. Ptosis: Both eyes exhibited a noticeable droop, more pronounced in the upward gaze, indicating lid lag typical of MG.2. Diplopia: The patient reported double vision, particularly in lateral and upward gaze. Change in diplopia with different gaze directions is consistent with MG.Therefore, based on my assessment, the collective ocular symptoms of ptosis and diplopia are highly suggestive of Myasthenia Gravis.
Speech Therapist: As a speech-language pathologist in an Initial Assessment Team (IAT) tackling the case of a potential Myasthenia Gravis patient, my role incorporates evaluating the degree and severities of the dysarthria and dysphagia symptoms.In this context, the dysarthria will present as disrupted speech fluency, characterized by difficulty articulating words. As for dysphagia, this entails troubles in swallowing. These symptoms surface as Myasthenia Gravis cause weakness in the patient's skeletal muscles, which are the muscles necessary for speech and swallowing mechanics.(…)The findings from these investigations will not only support the Myasthenia Gravis diagnosis, they will also provide foundational data for the creation of a targeted rehabilitation plan. The plan will aim to maintain oral intake while ensuring safety, preventing any potential complications like aspiration pneumonia.
Team 1. Initial Assessment Team (IAT)
Senior Consultant (Lead)→ Clinical Care Coordinator, Patient Advocate: Given the patient's symptoms, the most likely diagnosis is (A) Myasthenia Gravis, a neuromuscular disease causingmuscle weakness and fatigue. However, it's important to conduct an in-depth investigation with our team of clinicians.1. Clinical Care Coordinator: This individual should help coordinate necessary medical tests to confirm this diagnosis. Tests may include blood tests to check for antibodies contributing to muscle weakness, nerve conduction studies, or an edrophonium test. Additionally, they will also organize appropriate care resources and ensure effective communications across the team.2. Patient Advocate: As an advocate, this team member needs to communicate between the patient and the medical team. They need to ensure that the patient understands the potential diagnosis, the implications, and the subsequent steps in the investigative process and treatment. Their goal is to offer emotional support and make sure the patient's concerns or questions are accurately conveyed to and addressed by the medical team.
Clinical Care Coordinator: My expertise as a clinical care coordinator involves bringing together different healthcare teams, managing resources and information to ensure the best care for our patients. It involves organizing and streamlining the treatment process, making sure that the care provided is efficient and comprehensive.(…)Hypothesized Diagnosis: Myasthenia Gravis Recommended Investigations:1. Blood tests to look for specific antibodies present in Myasthenia Gravis.2. Nerve conduction studies to test the nerves' ability to send signals.3. Edrophonium test, where the drug edrophonium, which briefly relieves weakness in people with myasthenia gravis, is used.
Patient Advocate: As a Patient Advocate, I understand that while the diagnosis may be medically accurate, it's key to clarify what this means for the patient's day-to-day life:1. Communication: I spoke to the patient and their family about the potential diagnosis of Myasthenia Gravis, explaining the nature of the condition, common symptoms, and potential treatment options. I also emphasized that further tests are needed before confirming the diagnosis.2. Understand patient's fears and concerns: The patient expressed worries about the impacts of the disease on their daily life and potential side effects of medication. They are particularly concerned about the potential for extreme muscle weakness and how this might disrupt their current job and responsibilities.3. Support: I assured the patient that our multidisciplinary team is dedicated to providingthe best care for them and that we are open to discussing any concerns they have regarding their condition.
Team 4. Final Review and Decision Team (FRDT)
Summarized Report
Combined Report
Figure 12: Illustration of our proposed framework in high complexity setting. Given a medical
query (text-only) the framework performs reasoning in four steps: (i) complexity check, (ii) expert
recruitment, (iii) information gather and report generation, (iv) final decision. Green text represents
the correct answer.
F Medical Complexity Comparison with Human Physicians
The core premise of our MDAgent framework is its ability to adapt to the complexity of medical tasks.
To validate this approach and gain insights into how LLMs perceive medical complexity compared to
human experts, we conducted an annotation study. This study aimed to explore the alignment between
LLMs and physicians in assessing medical question complexity, a critical factor in the effectiveness
of our MDAgent framework.
Study Design We selected 50 representative questions from the MedQA dataset, ensuring a balanced
representation across USMLE steps 1, 2, and 3. This selection process aimed to cover a wide range of
medical topics and complexity levels, mirroring the diverse challenges that our MDAgent framework
is designed to address.
Three physicians participated in our study: two with two years of Internal Medicine training (Post
Graduate Year 2, PGY-2) and one general physician. This composition allowed us to capture a range
34
of clinical perspectives. The physicians rated each question on a scale of -1 (low complexity), 0
(moderate complexity), and 1 (high complexity).
Inter-rater Reliability To quantify the agreement among our physician raters, we employed
Intraclass Correlation Coefficients (ICC). ICC is a widely used statistical measure in medical research
for assessing the consistency of ratings among multiple raters. We specifically chose two ICC
variants:
• ICC2k (Two-way random effects, average measures): 0.269 [-0.14, 0.55]
• ICC3k (Two-way mixed effects, average measures): 0.280 [-0.15, 0.57]
ICC2k was selected because it assumes our raters are randomly selected from a larger population of
similar raters, allowing for generalization of our findings. ICC3k, on the other hand, treats the raters
as fixed, focusing on the consistency among our specific set of physicians.
Both ICC values indicate moderate agreement among the raters. This level of agreement reflects
the inherent complexity and subjectivity in evaluating medical questions, even among trained pro-
fessionals. It also highlights the challenging nature of the task our MDAgent framework aims to
address.
Annotation Interface To facilitate the annotation process for both physicians and LLMs, we
developed a specialized interface. This interface was designed to present medical questions in a clear
and consistent manner, allowing for efficient and standardized complexity ratings. Figure 13 shows a
screenshot of the annotation interface used in our study.
Figure 13: Annotation interface used for medical complexity assessment. The latest version can be
found at https://dxagents.github.io/2024/05/01/medqa.html.
LLM Annotations and Comparison To compare LLM performance with human expert judgments,
we employed several state-of-the-art models to annotate the same set of questions. We then compared
these assessments with the majority opinion of the physicians, determined by the mode of their ratings
(or the mean in cases of complete disagreement).
Table 13 presents the Pearson correlation between each LLM’s complexity ratings and the physicians’
majority opinions:
35
Model Correlation with Physician Majority
gpt-4o-mini -0.090
gpt-4o 0.022
gpt-4 0.070
gemini-1.5-flash 0.110
Table 13: Correlation between LLM complexity ratings and physician majority opinions
The results of our study provide valuable insights into the current state of LLM capabilities in medical
complexity assessment and underscore the importance of our MDAgent framework:
1. Subjectivity in medical complexity: The moderate ICC values among physicians highlight
the inherent subjectivity in assessing medical question complexity. This finding validates
our approach in MDAgent, which doesn’t rely on a single, fixed assessment of complexity
but rather adapts its collaboration structure dynamically.
2. Current LLM limitations: The low correlations between LLM and human assessments
indicate that current LLMs may not fully capture the nuances that human experts consider
when evaluating medical complexity. This observation reinforces the need for our MDAgent
framework, which can compensate for individual LLM limitations through collaborative
decision-making.
3. Potential for improvement: The variation in correlation across different LLM models
(from -0.090 to 0.110) suggests there is room for improvement in LLM performance. This
aligns with our MDAgent approach, which can leverage the strengths of multiple models
and adapt to future improvements in LLM capabilities.
4. Value of human expertise: The discrepancy between LLM and physician assessments un-
derscores the continued importance of human medical expertise. Our MDAgent framework
acknowledges this by incorporating human-like collaboration strategies and the potential for
human oversight in critical decisions.
5. Adaptability of MDAgent: The challenges revealed in this study highlight the wisdom of
our MDAgent’s adaptive approach. By dynamically adjusting its collaboration structure
based on perceived task complexity, MDAgent can mitigate the limitations of individual
LLMs and approach the nuanced understanding demonstrated by human experts.
36
NeurIPS Paper Checklist
1. Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The abstract and introduction state the primary contributions of the paper,
including the introduction of the MDAgents framework, its adaptive decision-making
structure, and the significant performance improvements demonstrated through experimental
results.
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The paper includes a section on limitations, discussing the scope of the
framework, and areas for future improvement.
Guidelines:
• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs
37
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [NA]
Justification: The paper does not include theoretical results that require assumptions or
proofs.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The paper provides comprehensive details on datasets, experimental setups,
and methodologies used, ensuring that the results can be reproduced accurately.
Guidelines:
• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
38
5. Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The paper includes a link to the code repository and provides sufficient
instructions in the supplemental material to reproduce the experiments.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The paper specifies all relevant experimental details, including data splits,
number of samples, and number of seeds, ensuring transparency and reproducibility of the
results.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The paper reports error bars and includes information on the statistical sig-
nificance of the experimental results, ensuring a clear understanding of the variability and
reliability of the findings.
Guidelines:
• The answer NA means that the paper does not include experiments.
39
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: The experiments primarily involved inference using API calls to GPT-3.5,
GPT-4 (V), and Gemini-Pro (Vision). The type of compute workers, memory, and time
of execution are managed by the API providers (OpenAI and Gemini). Details about the
number of API calls and the specific configurations used for each experiment are provided,
ensuring reproducibility.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: The research adheres to the NeurIPS Code of Ethics, ensuring responsible
conduct throughout the study.
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
40
Answer: [Yes]
Justification: The paper discusses potential societal impacts, both positive and negative,
including the improvement of medical decision-making and the risks associated with misuse
or biases in the AI system.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [Yes]
Justification: The paper discusses potential limitations, including the risks of medical
hallucinations and false knowledge. It includes guidelines within the main text to ensure
responsible use of the MDAgents framework and publicly available datasets, emphasizing
ethical usage and adherence to best practices.
Guidelines:
• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: The paper properly credits the creators of existing assets used and clearly states
the licenses and terms of use.
41
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not introduce any new assets.
Guidelines:
• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing or research with human subjects.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
42
Justification: The paper does not involve crowdsourcing or research with human subjects.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
43"
NeurIPS-2024-richelieu-self-evolving-llm-based-agents-for-ai-diplomacy-Paper-Conference,"Richelieu: Self-Evolving LLM-Based Agents for AI
Diplomacy
Zhenyu Guan ♢, Xiangyu Kong♣†B, Fangwei Zhong♠†B, Yizhou Wang♡♢
♢ Institute for Artificial Intelligence, Peking University
♣ College of Computer Science, Beijing Information Science and Technology University
♠ School of Artificial Intelligence, Beijing Normal University
♡ Center on Frontiers of Computing Studies, School of Computer Science,
Nat’l Eng. Research Center of Visual Technology, Peking University
† State Key Laboratory of General Artificial Intelligence, BIGAI
BCorresponding authors: xykong@bistu.edu.cn, fangweizhong@bnu.edu.cn
Abstract
Diplomacy is one of the most sophisticated activities in human society, involv-
ing complex interactions among multiple parties that require skills in social rea-
soning, negotiation, and long-term strategic planning. Previous AI agents have
demonstrated their ability to handle multi-step games and large action spaces
in multi-agent tasks. However, diplomacy involves a staggering magnitude of
decision spaces, especially considering the negotiation stage required. While
recent agents based on large language models (LLMs) have shown potential
in various applications, they still struggle with extended planning periods in
complex multi-agent settings. Leveraging recent technologies for LLM-based
agents, we aim to explore AI’s potential to create a human-like agent capable
of executing comprehensive multi-agent missions by integrating three funda-
mental capabilities: 1) strategic planning with memory and reflection; 2) goal-
oriented negotiation with social reasoning; and 3) augmenting memory through
self-play games for self-evolution without human in the loop. Project page:
https://sites.google.com/view/richelieu-diplomacy.
1 Introduction
Diplomacy, a central element of international relations, is an intricate and multifaceted activity that
lies at the heart of human society’s most complex interactions. It requires various skills such as social
reasoning, negotiation, and long-term planning to manage relationships and alliances among multiple
parties. Mirroring this complexity, the Diplomacy game involves seven players to control European
powers, presenting a challenging strategic landscape that demands advanced negotiation and strategic
planning to succeed.
The AI community has shown an increasing interest in the deployment of AI agents to master
such games [Shoker et al., 2023, Konya et al., 2023, Kramár et al., 2022, Duéñez-Guzmán et al.,
2023, Mukobi et al., 2023, Kovaˇc et al., 2023]. The recent breakthrough [Bakhtin et al., 2022] has
turned into press diplomacy, which allows communication between players. However, the previous
methods [Bakhtin et al., 2022] heavily rely on domain-specific human data, leading to its poor
generalization to other scenarios/ applications. The question then arises: Can we build an AI agent
that excels in the art of diplomacy without relying on domain-specific human data?
Recently, agents based on the Large Language Model(LLM) have emerged as a promising develop-
ment for AI agents. The previous applications on personal assistants [Li et al., 2024b], robotics [Cheng
38th Conference on Neural Information Processing Systems (NeurIPS 2024).
2019 DipNet 2022 Cicero 2024 Richelieu
RL
No Press Diplomacy
Human Data
RL
Reasoning with Negotiation
Human Data
LLMs-based-Agent 
Evolution in Self-Play
No Human Data
Self-Play
Evolution Memory 
Update
Experience
Acquisition
Figure 1: A new paradigm for building AI Diplomacy agent.
et al., 2024, Yang et al., 2023c], and video games [Wan et al., 2024] have shown the surprising ability
of LLM-based agents in communication and planning, benefiting from the emergent ability of com-
mon sense reasoning, in-context/ few-shot learning, and sophisticated natural language processing
on LLMs. However, diplomacy presents a unique set of challenges. It not only requires planning
long-horizon strategic [Qi et al., 2024] and communicating with natural language, but also reasoning
and adopting the complex social dynamics with partial observations, including gaining trust and
reputation, building rapport, detecting deception, and assessing the reliability of other players.
In this work, we aim to make the first attempt to explore LLMs’ potential to develop a human-like AI
diplomacy agent. We name the agent Richelieu in memorizing a pivotal figure in European history
who had enduring impacts on French politics, foreign affairs, and state building. To achieve this goal,
we have identified four core and essential capabilities that are crucial for building an LLM-based
societal agent.
1. Social reasoning. This is the basic function for a social agent to interact with others,
particularly for adapting to the dynamic changes in the nation’s intentions and relationships.
2. Balance long- and short-term planning. Diplomacy necessitates a careful balance between
short-term tactics and long-term strategies. An effective AI agent must assess the immediate
consequences of its actions alongside their potential long-term impacts.
3. Memory management. A robust memory system is a critical component of learning and
improvement. The AI agent must be able to recall and integrate information from past
negotiations and actions to inform its current and future decision-making processes. This
endows the agent with the ability to evolve.
4. Self-reflection. An AI agent capable of profound reflection can analyze its own decisions,
learn from its memory experience, and adapt its strategies accordingly.
By integrating these four capabilities, the agent can operate at the highest level of diplomatic
sophistication, outperforming the state-of-the-art AI diplomats [Bakhtin et al., 2022].
Our contributions can be summarized in three-fold: 1) We introduced a new paradigm for building
AI diplomacy agents, compared to previous work (Figure 1). The agent can self-evolve by generating
experience via self-play games, without any task-specific human data. 2) We demonstrate the superior
performance of our agent playing against the SOTA method, e.g., Cicero [Bakhtin et al., 2022], that
relies on a large-scale human demonstration for training. 3) We further analyze the effectiveness of
each module in our agent and the generalization of our agent in adopting different LLMs, such as
GPT-4 and Llama 3.
2 Related work
AI Diplomacy. The diplomacy game involves seven players controlling different powers in Europe.
In each turn, players can negotiate for cooperation before making moves to take as many supply
centers as they can. Apparently, this challenging strategy task requires both complex negotiation
skills and superior planning capability for player agents to achieve final victory. So far, most previous
works on this task remain focused on the planning strategies (a.k.a. No-Press Diplomacy where no
communication channels are allowed). The setting remains challenging considering its enormous
2
action space of 1021 to 1064 per turn (compared with Chess, which has much fewer than 100 actions
per turn). No wonder existing efforts rely on human data to play the game. Among the methods, one
typical research is DipNet [Paquette et al., 2019] which uses supervised and reinforcement learning.
Based on DipNet, BRPI [Anthony et al., 2020], SearchBot [Gray et al., 2020], DORA [Bakhtin
et al., 2021], and KL-Regularized search (Diplodocus) [Jacob et al., 2022] were conducted. Recently,
research has also emerged for the full-setting of Diplomacy, i.e., Press Diplomacy, where players are
allowed to negotiate with each other before making their moves in each turn. Such studies [De Jonge
and Sierra, 2017][Bakhtin et al., 2022][Jaidka et al., 2024][Kramár et al., 2022] mainly benefit from
the recent thriving language models. Specifically, notable advancements include policy iteration
methods from DeepMind and Meta AI Research’s equilibrium search agent [Jaidka et al., 2024].
However, Deepmind proposes to learn negotiation agents based on predefined contracts/protocols
[Kramár et al., 2022]. And Meta AI’s work, instead of one unified architecture, Cicero [Bakhtin
et al., 2022] integrates a language model for negotiation and an RL model for planning respectively.
Such separately trained models make it inconvenient for agents’ continual evolution. Moreover, like
no-press methods, these approaches heavily rely on human player data for agent training. Unlike these
approaches, this paper delves into solving the negotiation and planning in one single self-evolving
LLM-based agent model, without any pre-collected human expert training data.
LLM-Based Agents. With the emergence and growth of large language models (LLM), there
is a growing trend in utilizing LLMs as fundamental controllers for autonomous agents[Wang
et al., 2024c]. One wide application genre is LLM-based answering engines, which merely cover
the negotiation aspects of Diplomacy. Such systems include HuggingGPT [Shen et al., 2023],
GPT4Tools [Yang et al., 2023b] and ToT [Yao et al., 2023], etc. They leverage LLMs to manage
Al models, use tools, implement policy iteration, and enhance problem-solving across various tasks.
Related work including AutoGPT, AgentGPT, BabyAGl [Talebirad and Nadiri, 2023], Toolformer
[Schick et al., 2023], and Visual ChatGPT aim to improve LLM’s capabilities in task automation
and tool usage. Reflexion, a framework that improves LLMs through linguistic feedback and
episodic memory [Zhang et al., 2024a], facilitating better decision-making across diverse tasks is
proposed. Besides [Wang et al., 2024d][Wang et al., 2023a][Wang et al., 2023b][Zhu et al., 2023][Yan
et al., 2023] apply LLM agents to the complex planning tasks in the well-known open-world game
Minecraft[Fan et al., 2022]. Unlike these LLM-based agents which only focus on the negotiation or
planning ability respectively, the proposed approach involves a self-evolving scheme in a self-play
game to handle both of them simultaneously.
3 Problem Statement
The Diplomacy game [Wikipedia, 2024, Calhamer, 1974] is set in pre-World War I Europe and
involves each player (agent) representing one of the seven Great Powers of Europe, such as Germany,
France, England, Italy, Austria-Hungary, Russia, and Turkey. Each player has a set of military units,
including armies and fleets, which they can move and use to capture other supply centers. The
ultimate goal for the agent is to control a majority of the total supply centers on the board by the end
of the game’s Fall phase. It’s important to note that it is not won by eliminating other players or their
units; it is won by controlling the requisite number of supply centers. This often involves forming and
breaking alliances, negotiating, and sometimes betraying other players to achieve one’s own goals.
In each turn, agent i gets the current state st ∈ S, the actions of other players from the previous
turn ⃗ a−i
t−1, and the messages ⃗ m−i,i
t from other players during this turn’s negotiations. The state st
for the environment includes the ownership of each territory on the map by a particular country and
where the armies of each country are located. Based on this information, the agent needs to engage in
negotiations with other players, sending messages ⃗ mi,−i
t to chat with other players, and then take the
actions ai
t in this turn. The possible actions an agent can take ai
t ∈ A are commands to the armies,
such as moving into an adjacent territory, supporting another unit, or holding a position. Actions can
also include diplomatic moves, such as proposing or withdrawing from an alliance, although these
are less formalized in the game mechanics.[Paquette et al., 2019, Hill, 2014]
4 Self-Evolving LLM-based Diplomat
We have constructed a comprehensive framework with modules for memory management, social
reasoning, strategic planning, negotiation, decision-making, memory update, and self-evolving to
3
v.s.
ENVIRONMENT
Social 
Reasoning
Intention
Relationship
Planner with Reflection
Negotiator
Actor
Memory
MAP
RULE
Domian
Knowledge
SELF-PLAY
Sub-goals
INIT
Summarized
u ___u ___
Updated 
Social Belief
Social 
Belief
States
Actions
Sub-goals
Messages
Experience 
Abstract
Figure 2: The framework of the proposed LLM-based-agent, Richelieu. It can explicitly reason social
beliefs, propose sub-goals with reflection, negotiate with others, and take actions to master diplomacy.
It augments memories by self-play games for self-evolving without any human annotation.
fully leverage the capabilities of LLMs. Richelieu starts by setting up with map details, game rules,
domain knowledge, and the long-term goal.[Zhang et al., 2022, Wei et al., 2022, Wang et al., 2022a]
At each turn, the agent will run in the following steps: 1) Social Reasoning: First of all, the agent
undergoes a comprehensive analysis of the game state st to build the social belief, including the
intention of other players and their relationship ⃗ϕt ∈ Φn.[Zhang et al., 2024c, Gürcan, 2024] 2)
Planner with Reflection: Then, the agent proposes sub-goals χi
t ∈ X that is strategically aligned
with the long-term goals Υ, with the social belief and refining the proposed goal with experience
⃗ ηt ∈ Hm abstract from the memory M via self-reflection.[Wang et al., 2024b,e] 3) Negotiator: To
achieve the sub-goals, the negotiator will start a dialogue session with some players, and evaluate
their trueness ⃗ψ−i
t by referring to their messages ⃗ m−i,i
t , the current state st, their sincerity ⃗ γ−i
t and
the experience ⃗ξt.[Abdelnabi et al., 2023, Bianchi et al., 2024] 4) Actor: After negotiation, the actor
decides its course of action ai
t, based on the sub-goal χi
t and updated social state st+1, marking
the end of that turn. 5) Memory Management: The state of the current turn st, the content of
negotiations ⃗ mt, the actions taken by all players ⃗ at ∈ An, and the sub-goals set forthχi
t are all logged
within the memory as µ ∈ M. This logged data serves as a historical experience, guiding Richelieu’s
subsequent actions in future turns [Hatalis et al., 2023, Zhang et al., 2024e]. 6) Self-evolution:
The agent’s evolution is highly dependent on the diversity of experiences stored in its memory. As
this diversity grows, so does the agent’s capability. Without human demonstrations, we employ
multi-agent self-play games, i.e., our agents respectively control all the countries to simulate and
acquire diverse experiences for self-evolving. Notably, the agent can further evolve during testing to
adapt to different players.
4.1 Social Reasoning
There are no permanent enemies or allies. The relationship among countries is dynamically changing
upon the evolving global state. However, it is difficult to determine the appropriate allies and enemies
with partial observation. For example, there is uncertainty about the intentions of potential allies,
which could lead to betrayal at pivotal moments. Consequently, we need to identify the intention and
relationship of the current state by social reasoning to shape the social belief [Zhang et al., 2024c,
Gürcan, 2024].
1) Modeling Relationship: Before setting sub-goals, Richelieu evaluates its relations with others,
identifying enemies such as aggressive nations, vulnerable neighbors for expansion, and those with
long-term potential threats. It also seeks out potential allies to counter these threats.[Sun et al., 2024,
Zhang et al., 2024d] Simultaneously, Richelieu also tries to identify potential allies that could be
instrumental in countering these adversaries. By isolating the analysis of inter-player relationships as
a discrete element, Richelieu strategically exploits the actions of other players in subsequent stages of
the game to reach its goals. 2) Inferring Intention: The social belief is used by the planner, ensuring
that its sub-goals are formulated with a comprehensive consideration of the behaviors and intentions
4
of other intelligent agents within the game. Richelieu’s sub-goals will particularly emphasize those
who are identified as potential adversaries or allies, fostering more effective collaboration with
potential allies and participation in strategic opposition against adversaries. Furthermore, the insights
gleaned from this analysis are instrumental in the subsequent negotiation phases. They are employed
to assess the authenticity of the statements made by other players, as well as to aid Richelieu in
reaching cooperative agreements [de Zarzà et al., 2023, He et al., 2024].
4.2 Strategic Planner with Reflection
The strategic planner specifies the sub-goals, which serve as an intermediary between immediate
actions and the overarching goal of securing victory in the game. That is because we observe that
LLMs are often characterized by their propensity to prioritize short-term gains in decision-making
processes, with a notable deficiency in incorporating the future into their strategic calculations. [Renze
and Guven, 2024, Zhang et al., 2024b]For example, it is common for a non-neighboring country to
become too powerful. Formally, ⃗ χt ← SR(st, ⃗ϕt, Υ) where ⃗ χt = (χi
t, χ1
t , . . . , χn
t ) represents the
proposed sub-goals and other players’ intention that we inferred, ⃗ϕt ∈ Φn represents the inferred
relationship on the social belief. These goals may encompass a range of tactical considerations, such
as the containment of a formidable rival’s advancement or the strategic expansion in a particular
direction to consolidate power.
Reflection with Memory. We further develop a reflection mechanism to enhance the rationality
and effectiveness of our agent’s sub-goals in achieving long-term goals.[Liu et al., 2024] This
reflection mechanism relies on past experiences to critique and enhance proposed sub-goals. We
employ a similarity-based function to find relevant historical experiences that match the current game
state from its memory. This function considers two factors: goal similarity and state similarity, to
select the most comparable experiences. The process can be written as: ⃗ ηt ← h(st, χi
t, M), where
⃗ ηt ∈ Hm. In practice, considering the limited context windows of LLM, we retrieve the most
analogous experiences from the memory based on these metrics. Experiences with high evaluative
scores reinforce successful strategies and support the continuity of existing sub-goals. On the other
hand, lower scores indicate areas that need improvement and prompt the necessary adjustments. As
our agent, Richelieu undergoes more training sessions, and hence its reflection ability gets improved.
The growing pool of historical experiences consistently enhances its performance.
4.3 Negotiator and Actor
By chatting with other players, the goal of the negotiation is to update the social belief according
to the received words and reach the sub-goal by manipulating others’ intentions, such as securing
cooperative agreements with other nations, terminating ongoing conflicts with a specific country,
or deterring the formation of alliances directed against its interests.[Noh and Chang, 2024, Zhan
et al., 2024] However, it is difficult to reach a consensus, as the interests and strategies of the various
nations often conflict, and trust between players can be scarce, making it challenging to establish and
maintain cooperative agreements. In this case, we argue that the negotiator should identify the true
intentions and relationship of the opponent before generating words for the negotiation.
To fully utilize the power of LLMs, we construct a social reasoning flow for negotiation, as shown in
Figure 3. During the negotiation process, we guide Richelieu to consider the veracity of what other
players said and their true intentions, in conjunction with our established sub-goals and analysis of
our relationships with other players, to negotiate and form alliances with potential allies and attempt
to deceive enemies [Xia et al., 2024, Moghimifar et al., 2024].
To counteract the challenge of non-binding agreements and potential deception, we incorporate a
discrete module dedicated to the assessment of the veracity of statements made by other players
during negotiations. To determine the truthiness of other players’ statements ψj
t , three main factors
are considered. The most important is the consistency between the player’s sub-goals χj
t that our
agent inferred before and the intentions conveyed through his statements mj,i
t . To aid the judgment,
our agent also goes through the memory to retrieve the consistent experiences ⃗ξt. Additionally, the
player’s overall honesty scoreγi is taken into account. Hence, we get the truthiness of the opponent j:
ψj
t ← g(st, χj
t, mj,i
t , ⃗ϕt, γj, ⃗ξt), where ⃗ξt = w(st, mj,i
t , M). With such a reasoning flow, our agent
5
words for cooperation
words for fake 
cooperation
no words
  lies?
words to change 
other’s intention
Enemy?
F
T
T
F
true  intention
Enemy?
Enemy?
confrontation
cooperation
T
F
T
Receive
Words
F
other's 
words
T
Deception?
F
change 
relationship
F
T
MEMORY
Figure 3: The social reasoning flow for negotiation. With the received words and memory, the
agent will reason by answering the following questions: “Is the opponent lying?"", “What is the true
intention of the opponent?"", “Is the opponent enemy?"", “Is it necessary to deceive the opponent?"",
and “Is it necessary to change the relationship with the opponent?"", and then generate the words
accordingly for negotiation.
can adeptly navigate diplomatic discourse. After the negotiation, the actor will get the updated social
beliefs and choose a specific action for the army.
4.4 Memory Management and Evolution in Self-Play Games
Memory is the foundation of the framework that accumulates the historical experience of the agent
and summarizes them for other modules [Gao and Zhang, 2024, Li et al., 2024a, Yu et al., 2024,
Hou et al., 2024]. It supports other modules, such as planner and negotiator, to provide long-tail
experiences.
Experience Management. Specifically, the memory module is tasked with the acquisition and
archival of historical data, encompassing the observed game state st at each turn, its sub-goals
χi
t, the messages during the negotiation ⃗ mt, and the actions of all the players ⃗ at. Subsequently,
the raw experience is summarized in a shorter content with an evaluation λt ∈ Λ of the proposed
sub-goals and an assessment of the credibility of other players γj ∈ Γ. λt serves to reflect upon
the agent’s sub-goals. It evaluates whether sub-goals are reasonable based on the subsequent state
and long-term goals Υ. As the game progresses, it is continuously updated in response to changes
in the state λt ← f(χi
t, Υ,⃗ s), where ⃗ s= ( st, st+1, . . . sT ). The formula represents the update
of the evaluation λt for the sub-goal in turn t by the memory in turn T. The updates will cease
when there is a fundamental change in the sub-goal compared to the goal at turn t. This prevents
subsequent decisions from impacting the assessment of the current decision-making. We employ
γj ∈ Γ to evaluate the credibility of player j and utilize τj
t ∈ {0, 1} to denote the truthfulness, i.e.,
whether the statements made by the player j during the negotiation process at time t are truthful.
The truthiness of player j’s statements is updated according to the memory from the previous turns,
τj
t ← T(st, st+1, aj
t, mj,i
t ). The credibility of player j, γj will be updated based on player j’s
statements τj
t , written as γj ← p(γj, τj
t−1). Players’ credibility ⃗ γis a short-term memory that is
applicable only to the current turn. Other data collected or generated constitutes long-term memory.
These data will be combined to form a history µ ∈ M, and then is incorporated into memory.
Acquisition Experience via Self-Play Games. self-play training mechanism has been widely applied
on training agents via reinforcement learning [Zhong et al., 2019, 2021, Wu et al., 2022]. However,
it is hardly explored in LLM-based agents. For LLM-based agents, a self-play game allows the
agent to accumulate more experiences for self-evolution [Liu et al., 2024, Zhang et al., 2024a].
As self-play continues, the acquisition of new and better historical experiences by the agent will
diminish. This means that the agent’s capabilities will not improve indefinitely. At the same time,
as the memory grows, selecting appropriate historical experiences becomes a new challenge. The
chosen m experiences ⃗ ηt may be almost identical, which could actually reduce the amount of useful
information available to Richelieu. After self-play training, when Richelieu is faced with a certain
6
Table 1: The results of our method playing against Cicero.
Model Win ↑ Most SC↑ Survived↑ Defeated↓
Richelieu_1 6.20% 9.40% 38.90% 45.50%
Richelieu_2 6.60% 7.80% 40.80% 44.80%
Richelieu_3 7.10% 9.30% 39.90% 43.70%
Richelieu_4 7.40% 8.00% 40.20% 44.40%
Cicero_1 5.90% 6.50% 41.50% 46.10%
Cicero_2 6.30% 7.20% 42.50% 44.00%
Cicero_3 5.90% 7.00% 41.60% 45.50%
Richelieu 6.83% 8.63% 39.95% 44.60%
Cicero 6.03% 6.90% 41.87% 45.20%
Model Win ↑ Most SC↑ Survived↑ Defeated↓
Richelieu_1 6.30% 7.90% 39.40% 46.40%
Richelieu_2 6.60% 8.30% 41.20% 43.90%
Richelieu_3 7.20% 8.70% 41.70% 42.40%
Cicero_1 5.80% 6.70% 41.20% 46.30%
Cicero_2 6.50% 7.20% 42.50% 43.80%
Cicero_3 6.00% 7.00% 41.60% 45.40%
Cicero_4 6.10% 7.20% 42.30% 44.40%
Richelieu 6.70% 8.30% 40.77% 44.23%
Cicero 6.10% 7.03% 41.90% 44.98%
state, it can draw on a larger pool of similar historical experiences. Diverse experiences enable the
agent to reflect more comprehensively on the strategies it currently devises, leading to a stronger
optimization of decision-making. As shown in Figure 5, Richelieu’s performance against Cicero
[Bakhtin et al., 2022] becomes better with increasing training iterations. With the accumulation of
experiences, Richelieu’s win rate exhibits a steady increase with accumulated training iterations,
ultimately plateauing at a stable performance level. In contrast, the defeated rate shows a consistent
decrease, approaching an asymptotic value. These observations confirm the effectiveness of self-play
in Richelieu’s evolution.
5 Experiment
In the experiments, our goal is to answer the following questions: 1) Mastery of Non-Press
Diplomacy: Can our agent master the non-press diplomacy against baselines? 2) Competing with
State-of-the-Art: Can our agent surpass the performance of the current state-of-the-art agents in
press diplomacy? 3) Compatibility with LLMs: Can our self-evolving framework be compatible
with different LLMs? 4) Contribution of Each Module: Do the individual modules within our
framework contribute to the overall improvement of our agent’s performance?
5.1 Experimental Setup
Environment. The widely-used open source Diplomacy game platform introduced by [Paquette et al.,
2019] is adopted for evaluating Richelieu against other models. It is easy to switch between no-press
(with negotiation between players) and press (no negotiation between players) games based on this
platform, facilitating a comparison of both settings. The platform also contains over 10,000 human
game data on which previous approaches are used. Note that our method does not need them. In each
episode, a model will host one randomly selected country to compete against countries controlled by
other methods. It wins if it occupies all the supply centers and loses and vice versa.
Evaluation Metrics. We evaluate the models based on the results of multiple rounds of games. In
each round, the model is randomly assigned a country to control. Typically, 1000 rounds are played
to obtain the average results. We evaluate the models in two metrics. One is based on the win rate,
Most SC rate, survived rate, and defeated rate. There are four possible outcomes for each country in
the game. If a country loses all its supply centers (SC), it is eliminated and recorded as “defeated"". If
a country occupies 18 or more out of 34 supply centers, the game ends, and that country is recorded
as “win"", while other countries are recorded as “defeated"". In other cases, the game ends in a draw.
The country with the most supply centers is recorded as “Most SC"", the countries that have been
eliminated are recorded as “defeated"", and the other countries are recorded as “Survived"". The other
is based on the scores obtained by the models after multiple rounds of competition. To compare the
capabilities of multiple models, we use C-Diplo Argir[Archer, 2024], a scoring system. This system
is used in many international diplomacy competitions. The scoring method is as follows: If a player
wins by occupying 18 or more supply centers, the player scores 93 points, and each of the other six
players scores 1 point. If the game ends in a draw, the player with the most centers scores 37 points.
The second player with the most centers scores 14 points. The third player with the most centers
scores 7 points. Each player scores 1 point per center owned. Each player also scores 1 point for
participating. In this way, regardless of the game outcome, a total of 99 points will be distributed
among the players in each game.
7
Baselines. We select six previous models as baselines for comparison. Among them, Cicero[Bakhtin
et al., 2022] by Meta is a diplomacy model with a negotiation module. The SL-DipNet and RL-DipNet
[Paquette et al., 2019], the BRPI [Anthony et al., 2020], the SearchBot [Gray et al., 2020], and the
DORA[Bakhtin et al., 2021] are no-press diplomacy models. We also build an LLM-based agent,
AutoGPT [Yang et al., 2023a]. In experiments, we set a temperature of 0.3 to ensure a relatively
stable generation of LLM policies. The overall reasoning framework also ensures the stability and
consistency of the AI agent’s performance.
5.2 Results
SL-DipNetRL-DipNet
BRPI
SearchBot
DORACicero
Richelieu
SL-DipNet
RL-DipNet
BRPI
SearchBot
DORA
Cicero
Richelieu
 0.5
1.0
1.5
2.0
2.5
3.0
3.5
Figure 4: The relative scores among 7 different
agents when massively playing on the no-press
setting. Each point shows the ratio of the model’s
score on the vertical axis to the score gained by the
model on the horizontal axis.
Massively Play with Baselines on no-press
setting. We let Richelieu compete with the
other six models including Cicero[Bakhtin
et al., 2022], SL-DipNet and RL-DipNet [Pa-
quette et al., 2019], BRPI [Anthony et al.,
2020], SearchBot [Gray et al., 2020], and
DORA[Bakhtin et al., 2021] on No-Press Diplo-
macy, in which players make moves without
communication. Figure 4 indicates that Riche-
lieu outperforms other previous models relying
on human game data. In contrast, Richelieu does
not need such data but outperforms these meth-
ods by a clear margin, which demonstrates the
outstanding planning capability of Richelieu.
Play against Cicero on press setting. We also
evaluate Richelieu through competition against
Cicero in the challenging scenario where nego-
tiation is enabled. Specifically, we randomly
assign three countries to one model and the re-
maining four to another. After playing several
rounds of the game, the win rate, most SC rate, survived rate, and the defeated rate is calculated using
a weighted average for evaluation. Table 1 demonstrates the competitive performance of Richelieu
in comparison to Cicero. Richelieu’s win rate is approximately 0.7% higher than Cicero’s. If the
Most SC rate is also taken into account, Richelieu is about 2% higher than Cicero. At the same time,
Richelieu’s loss rate is also 0.6% lower. According to our scoring system, Richelieu’s score is about
10% higher than Cicero’s. This is nontrivial especially when Richelieu is trained in a self-play game
without humans and the opponents are trained with the data from human players.
Although Richelieu’s win rate improvement compared to Cicero is not significant, the relative value
of the improvement is quite large. Moreover, the main reason for the modest improvement is that in
the seven countries, there are three or four controlled by Richelieu with similar abilities, which often
results in the game ending in a draw. Moreover, we observed a large gap by comparing the scores the
agents gained in the massively play with baselines on the no-press setting shown in Figure 4. Our
agent’s score is about 10% higher than Cicero’s.
Table 2: The results of Richelieu playing against AutoGPT.
Model Win ↑ Most SC↑ Survived↑ Defeated↓
Richelieu_1 9.30% 18.20% 37.90% 34.60%
Richelieu_2 9.90% 19.40% 37.70% 33.00%
Richelieu_3 8.10% 17.40% 39.20% 35.30%
AutoGPT_1 1.20% 4.60% 32.40% 61.80%
AutoGPT_2 1.20% 4.20% 34.40% 60.20%
AutoGPT_3 1.50% 4.00% 32.50% 62.00%
AutoGPT_4 2.60% 3.60% 32.30% 61.50%
Richelieu 9.10% 18.33% 38.27% 34.30%
AutoGPT 1.63% 4.10% 32.90% 61.37%
Play against AutoGPT on press set-
ting. We further built an LLM-based
agent using AutoGPT and compared
it with our agent. In the testing, we
randomly select three countries to be
controlled by Richelieu, and the other
four countries to be controlled by Au-
toGPT. Note that the agent controls
each country independently. The re-
sults are shown in Table 2. We can
see that our model significantly out-
performs the off-the-shelf reasoning
framework for LLM-based agents.
8
9 10 11 12
4.50%
5.00%
5.50%
6.00%
6.50%
7.00%
7.50%
8.00%
8.50%
Win rate
9 10 11 12
11.0%
12.0%
13.0%
14.0%
15.0%
16.0%
17.0%
18.0%
Win & Most SC rate
9 10 11 12
44.0%
45.0%
46.0%
47.0%
48.0%
49.0%
50.0%
51.0%
Defeated rate
GPT-4-R
GPT-4-C
ERNIE Bot-R
ERNIE Bot-C
Spark Desk-R
Spark Desk-C
Llama 3-R
Llama 3-C
Figure 5: Richelieu modules benefit different LLMs. The solid line represents the experimental
results for Richelieu, while the dashed line corresponds to Cicero. Different colors are used for
different LLMs. The horizontal axis represents the logarithm of the number of training sessions, and
the vertical axis denotes the rate.
Table 3: Ablation study: average results of 3 Richelieu vs. 4 Cicero.
Modeling
others sub-goals Negotiation
pipeline
Reflection
with Memory Self-play Win↑ Most SC↑ Survived↑ Defeated↓
0.4% 0.7% 4.3% 94.6%
✓ 0.7% 1.2% 10.6% 87.5%
✓ ✓ 3.3% 4.7% 26.7% 65.3%
✓ ✓ ✓ 3.8% 5.8% 33.1% 57.3%
✓ ✓ ✓ ✓ 5.2% 6.6% 39.5% 48.7%
✓ ✓ ✓ ✓ ✓ 6.7% 8.5% 40.4% 44.4%
Generalization of self-evolving framework to different LLMs. To demonstrate the effectiveness
of our framework in a variety of LLM, we conducted experiments using four models: GPT-4, ERNIE
Bot, Spark Desk, and Llama 3. As the number of training iterations increases, Richelieu’s win rate
steadily improves while the defeated rate declines, ultimately reaching a relatively stable outcome.
This suggests that our self-play method is effective. After training, the win rate using GPT-4 increased
from 1.5% lower than Cicero’s to about 0.7% higher than Cicero’s. The win rate using Llama 3
increased from 2.3% lower than Cicero’s to almost equal to Cicero’s. The win rates using Models
Spark Desk and ERNIE Bot increased from 3% and 4% lower than Cicero’s to 0.7% and 1.6% lower
than Cicero’s, respectively. The experimental results show that, despite variations in Richelieu’s
performance due to the inherent differences in the capabilities of these LLMs, as illustrated in
Figure 5, our framework and training approach significantly enhance the capabilities of all LLMs.This
indicates the generalization of a self-evolving framework to various LLMs. To demonstrate the effect
of the memory from the self-play game on our agent, we found two turns with similar states in
different rounds, one before self-play and the other after. The cases are shown in Appendix B.1.
Ablation Study. We conduct comprehensive ablation studies on Richelieu by analyzing the benefit of
incorporating Richelieu’s various modules, like planners or memory, into basic LLMs. The results are
shown in Table 3. As illustrated in Figure 5, while the enhanced alignment in LLMs indeed boosts
performance (GPT-4 is better than others), we observed that a vanilla GPT-4 still falls short in AI
diplomacy without our framework, as can be seen in Table 3. Richelieu’s performance has markedly
improved with the integration of each module, demonstrating its ability to leverage other players’
actions in decision-making while balancing short-term and long-term benefits. Its negotiation skills
have also enhanced significantly, enabling it to clearly communicate intentions to cooperate and avoid
deception. Moreover, the self-play experience further boosts Richelieu’s performance. These findings
suggest that while alignment in LLMs is essential, our approach is crucial for unlocking models’
potential in social simulation.
9
6 Conclusion
In this paper, we introduce Richelieu, a self-evolving LLM-based agent for AI diplomacy. Our
model enables hierarchical planning for multi-agent tasks and utilizes a memory module for reflective
optimization. Our model does not require human data and can evolve through self-play. It ultimately
outperforms existing models like Cicero in the Diplomacy. Our ablation study demonstrates the
effectiveness of the modules we have established. By conducting experiments using different LLMs,
we validate the generalization of our framework to various LLMs. We believe that the use of
LLM-based agents will become an effective approach in social science in the future.
7 Limitations and Future Work
Our study is subject to certain limitations. We utilize diplomacy as the platform for constructing our
model. However, the space of actions within diplomacy is constrained, whereas the decision-making
space in real-world diplomacy is virtually boundless. In diplomacy, apart from the information
exchanged between players during the negotiation, all other information is public and certain.
Conversely, real-world diplomacy operates within a framework of incomplete information.
Our framework is capable of applying to most social interaction tasks. Most components in our
framework can be easily generalized to a new task by modifying the content. Social reasoning enables
the agent to handle complex and dynamic social relationships. The negotiation pipeline opens the
potential of communicating with others to prob the other’s mind or reach a consensus. The hierar-
chical strategy with reflection enhances the ability to handle long-term planning. The self-evolving
mechanism (reflection with self-play memory) further improves the overall performance without
manual supervision. These modules cover most of the challenges in multi-agent interactions. The
potential applications of such an AI agent are vast, ranging from simulated diplomatic environments
to real-world assistance and analysis. In future research, we intend to develop a more realistic game
space, characterized by incomplete information and multi-player games, to enhance and refine our
model further. We will also extend the framework to other multi-agent scenarios, including embodied
interactions [Zhong et al., 2023, Ci et al., 2023, Chen et al., 2023], sensor networks [Wang et al.,
2022b, Xu et al., 2020, Pan et al., 2022, Li et al., 2020], and video games [Wang et al., 2024a,
Ma et al., 2024]. This framework can also be employed to develop various applications, such as
recommendation [Huang et al., 2023, Chen et al., 2024, Huang et al., 2024, Hong et al., 2024],
business negotiation [Hua et al., 2024], and education [Shea et al., 2024].
8 Ethical Consideration
The method proposed in this work has the potential for positive uses like enabling AI agents to emerge
in cooperation via negotiation or avoiding being fooled by fake promises (or helping humans do so).
However, negative cases can also arise if the technique is used for possible fraud activities. Fortunately,
there is research [Bakhtin et al., 2019][Zellers et al., 2019] dealing with such scenarios. And we also
urge for more research efforts in this field to foster safe applications of similar technologies.
Acknowledgements
This work was supported by the National Science and Technology Major Project (2022ZD0114904),
NSFC-6247070125, NSFC-62406034, NSFC-62406010, the State Key Lab of General Artificial
Intelligence at Peking University, Qualcomm University Research Grant, and Wuhan East Lake High-
Tech Development Zone, National Comprehensive Experimental Base for Governance of Intelligent
Society.
10
References
Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Schönherr, and Mario Fritz. Llm-deliberation:
Evaluating llms with interactive multi-agent negotiation games. arXiv preprint arXiv:2309.17234,
2023.
Calhamer Allan. The Games & puzzles book of modern board games . W. Luscombe, 1st edition,
1975. ISBN 978-0860020592.
Thomas Anthony, Tom Eccles, Andrea Tacchetti, János Kramár, Ian Gemp, Thomas Hudson, Nicolas
Porcel, Marc Lanctot, Julien Pérolat, Richard Everett, et al. Learning to play no-press diplomacy
with best response policy iteration. In Advances in Neural Information Processing Systems ,
volume 33, pages 17987–18003, 2020.
Bruno-AndrÃ© Giraudon & Vincent Archer. C-diplo argir, 2024. URL https:
//world-diplomacy-database.com/php/scoring/scoring_class.php?id_scoring=7.
Accessed:2024-05-02.
Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc’Aurelio Ranzato, and Arthur Szlam.
Real or fake? learning to discriminate machine from human generated text. arXiv preprint
arXiv:1906.03351, 2019.
Anton Bakhtin, David Wu, Adam Lerer, and Noam Brown. No-press diplomacy from scratch. In
Advances in Neural Information Processing Systems, volume 34, pages 18063–18074, 2021.
Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew
Goff, Jonathan Gray, Hengyuan Hu, et al. Human-level play in the game of diplomacy by
combining language models with strategic reasoning. Science, 378:1067–1074, 2022.
Federico Bianchi, Patrick John Chia, Mert Yuksekgonul, Jacopo Tagliabue, Dan Jurafsky, and James
Zou. How well can llms negotiate? negotiationarena platform and analysis. arXiv preprint
arXiv:2402.05863, 2024.
Allan Calhamer. The invention of diplomacy, 1974. URL https://diplomacyzines.co.uk/
strategy-tactics/articles-by-alan-b-calhamer/the-invention-of-diplomacy/ .
Accessed: 2024-05-18.
Hao Chen, Yuanchen Bei, Qijie Shen, Yue Xu, Sheng Zhou, Wenbing Huang, Feiran Huang, Senzhang
Wang, and Xiao Huang. Macro graph neural networks for online billion-scale recommender
systems. In Proceedings of the ACM on Web Conference 2024, pages 3598–3608, 2024.
Yuanpei Chen, Yiran Geng, Fangwei Zhong, Jiaming Ji, Jiechuang Jiang, Zongqing Lu, Hao Dong,
and Yaodong Yang. Bi-dexhands: Towards human-level bimanual dexterous manipulation. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 2023.
Guangran Cheng, Chuheng Zhang, Wenzhe Cai, Li Zhao, Changyin Sun, and Jiang Bian. Empow-
ering large language models on robotic manipulation with affordance prompting. arXiv preprint
arXiv:2404.11027, 2024.
Hai Ci, Mickel Liu, Xuehai Pan, fangwei zhong, and Yizhou Wang. Proactive multi-camera collab-
oration for 3d human pose estimation. In Proceedings of International Conference on Learning
Representations, 2023.
Hill David. The board game of the alpha nerds, 2014. URL https://grantland.com/features/
diplomacy-the-board-game-of-the-alpha-nerds/ . Accessed: 2024-05-18.
Dave De Jonge and Carles Sierra. D-brane: a diplomacy playing agent for automated negotiations
research. Applied Intelligence, 47:158–177, 2017.
I de Zarzà, J de Curtò, Gemma Roig, Pietro Manzoni, and Carlos T Calafate. Emergent cooperation
and strategy adaptation in multi-agent systems: An extended coevolutionary theory with llms.
Electronics, 12:2722, 2023.
Edgar A Duéñez-Guzmán, Suzanne Sadedin, Jane X Wang, Kevin R McKee, and Joel Z Leibo. A
social path to human-like artificial intelligence. Nature Machine Intelligence, 5:1181–1188, 2023.
11
Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,
De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied
agents with internet-scale knowledge. In Advances in Neural Information Processing Systems,
volume 35, pages 18343–18362, 2022.
Hang Gao and Yongfeng Zhang. Memory sharing for large language model based agents. arXiv
preprint arXiv:2404.09982, 2024.
Jonathan Gray, Adam Lerer, Anton Bakhtin, and Noam Brown. Human-level performance in no-
press diplomacy via equilibrium search. Proceedings of International Conference on Learning
Representations, 2020.
Önder Gürcan. Llm-augmented agent-based modelling for social simulations: Challenges and
opportunities. HHAI 2024: Hybrid Human AI Systems for the Social Good, pages 134–144, 2024.
Kostas Hatalis, Despina Christou, Joshua Myers, Steven Jones, Keith Lambert, Adam Amos-Binks,
Zohreh Dannenhauer, and Dustin Dannenhauer. Memory matters: The need to improve long-term
memory in llm-agents. In Proceedings of the AAAI Symposium Series, volume 2, pages 277–280,
2023.
Junda He, Christoph Treude, and David Lo. Llm-based multi-agent systems for software engineering:
Vision and the road ahead. arXiv preprint arXiv:2404.04834, 2024.
Avalon Hill. Diplomacy rules 4th edition, 2014. URL https://diplom.org/~diparch/
resources/rulebooks/2000AH4th.pdf. Accessed: 2024-05-18.
Zijin Hong, Zheng Yuan, Qinggang Zhang, Hao Chen, Junnan Dong, Feiran Huang, and Xiao
Huang. Next-generation database interfaces: A survey of llm-based text-to-sql. arXiv preprint
arXiv:2406.08426, 2024.
Yuki Hou, Haruki Tamoto, and Homei Miyashita. "" my agent understands me better"": Integrating
dynamic human-like memory recall and consolidation in llm-based agents. In Extended Abstracts
of the CHI Conference on Human Factors in Computing Systems, volume 7, pages 1–7, 2024.
Yuncheng Hua, Lizhen Qu, and Gholamreza Haffari. Assistive large language model agents for
socially-aware negotiation dialogues. arXiv preprint arXiv:2402.01737, 2024.
Feiran Huang, Zefan Wang, Xiao Huang, Yufeng Qian, Zhetao Li, and Hao Chen. Aligning distilla-
tion for cold-start item recommendation. In Proceedings of the 46th International ACM SIGIR
Conference on Research and Development in Information Retrieval, page 1147–1157, 2023.
Feiran Huang, Zhenghang Yang, Junyi Jiang, Yuanchen Bei, Yijie Zhang, and Hao Chen. Large
language model interaction simulator for cold-start item recommendation. arXiv preprint
arXiv:2402.09176, 2024.
Athul Paul Jacob, David J Wu, Gabriele Farina, Adam Lerer, Hengyuan Hu, Anton Bakhtin, Jacob
Andreas, and Noam Brown. Modeling strong and human-like gameplay with kl-regularized search.
In International Conference on Machine Learning, volume 162, pages 9695–9728, 2022.
Kokil Jaidka, Hansin Ahuja, and Lynnette Hui Xian Ng. It takes two to negotiate: Modeling social
exchange in online multiplayer games. In Proceedings of the 37th Annual ACM Symposium on
Human-Computer Interaction, volume 8, pages 1–22, 2024.
Andrew Konya, Deger Turan, Aviv Ovadya, Lina Qui, Daanish Masood, Flynn Devine, Lisa Schirch,
Isabella Roberts, and Deliberative Alignment Forum. Deliberative technology for alignment. arXiv
preprint arXiv:2312.03893, 2023.
Conor Kostick. The Art of Correspondence in the Game of Diplomacy. Curses & Magic, 2nd edition,
2015. ISBN 978-0993415104.
Grgur Kovaˇc, Rémy Portelas, Peter Ford Dominey, and Pierre-Yves Oudeyer. The socialai school:
Insights from developmental psychology towards artificial socio-cultural agents. arXiv preprint
arXiv:2307.07871, 2023.
12
János Kramár, Tom Eccles, Ian Gemp, Andrea Tacchetti, Kevin R McKee, Mateusz Malinowski,
Thore Graepel, and Yoram Bachrach. Negotiation and honesty in artificial intelligence methods for
the board game of diplomacy. Nature Communications, 13:7214, 2022.
Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, and Tat-Seng Chua. Hello again!
llm-powered personalized agent for long-term dialogue. arXiv preprint arXiv:2406.05925, 2024a.
Jing Li, Jing Xu, Fangwei Zhong, Xiangyu Kong, Yu Qiao, and Yizhou Wang. Pose-assisted multi-
camera collaboration for active object tracking. InProceedings of the AAAI Conference on Artificial
Intelligence, volume 34, pages 759–766, 2020.
Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu,
Wenxing Xu, Xiang Wang, Yi Sun, et al. Personal llm agents: Insights and survey about the
capability, efficiency and security. arXiv preprint arXiv:2401.05459, 2024b.
Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin Liu, Juntao Tan, Prafulla K Choubey,
Tian Lan, Jason Wu, Huan Wang, et al. Agentlite: A lightweight library for building and advancing
task-oriented llm agent system. arXiv preprint arXiv:2402.15538, 2024.
Long Ma, Yuanfei Wang, Fangwei Zhong, Song-Chun Zhu, and Yizhou Wang. Fast peer adaptation
with context-aware exploration. In International Conference on Machine Learning, volume 235,
pages 33963–33982, 2024.
Farhad Moghimifar, Yuan-Fang Li, Robert Thomson, and Gholamreza Haffari. Modelling political
coalition negotiations using llm-based agents. arXiv preprint arXiv:2402.11712, 2024.
Gabriel Mukobi, Ann-Katrin Reuel, Juan-Pablo Rivera, and Chandler Smith. Assessing risks of
using autonomous language models in military and diplomatic planning. In Multi-Agent Security
Workshop @ NeurIPS’23, 2023.
Sean Noh and Ho-Chun Herbert Chang. Llms with personalities in multi-issue negotiation games.
arXiv preprint arXiv:2405.05248, 2024.
Xuehai Pan, Mickel Liu, Fangwei Zhong, Yaodong Yang, Song-Chun Zhu, and Yizhou Wang.
Mate: Benchmarking multi-agent reinforcement learning in distributed target coverage control. In
Advances in Neural Information Processing Systems, volume 35, pages 27862–27879, 2022.
Philip Paquette, Yuchen Lu, Seton Steven Bocco, Max Smith, Satya O-G, Jonathan K Kummerfeld,
Joelle Pineau, Satinder Singh, and Aaron C Courville. No-press diplomacy: Modeling multi-agent
gameplay. In Advances in Neural Information Processing Systems, volume 32, pages 4474–4485,
2019.
Siyuan Qi, Shuo Chen, Yexin Li, Xiangyu Kong, Junqi Wang, Bangcheng Yang, Pring Wong, Yifan
Zhong, Xiaoyuan Zhang, Zhaowei Zhang, Nian Liu, Yaodong Yang, and Song-Chun Zhu. Civrealm:
A learning and reasoning odyssey in civilization for decision-making agents. In Proceedings of
International Conference on Learning Representations, 2024.
Matthew Renze and Erhan Guven. Self-reflection in llm agents: Effects on problem-solving perfor-
mance. arXiv preprint arXiv:2405.06682, 2024.
Sharp Richard. The game of diplomacy. Arthur Barker, 1979. ISBN 978-0213166762.
Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke
Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach
themselves to use tools. In Advances in Neural Information Processing Systems, volume 36, pages
68539–68551, 2023.
Ryan Shea, Aymen Kallala, Xin Lucy Liu, Michael W Morris, and Zhou Yu. Ace: A llm-based
negotiation coaching system. arXiv preprint arXiv:2410.01555, 2024.
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hug-
gingGPT: Solving AI tasks with chatGPT and its friends in hugging face. In Advances in Neural
Information Processing Systems, volume 36, pages 38154–38180, 2023.
13
Sarah Shoker, Andrew Reddie, Sarah Barrington, Ruby Booth, Miles Brundage, Husanjot Chahal,
Michael Depp, Bill Drexel, Ritwik Gupta, Marina Favaro, et al. Confidence-building measures for
artificial intelligence: Workshop proceedings. arXiv preprint arXiv:2308.00862, 2023.
Chuanneng Sun, Songjun Huang, and Dario Pompili. Llm-based multi-agent reinforcement learning:
Current and future directions. arXiv preprint arXiv:2405.11106, 2024.
Yashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power of
intelligent llm agents. arXiv preprint arXiv:2306.03314, 2023.
Hongyu Wan, Jinda Zhang, Abdulaziz Arif Suria, Bingsheng Yao, Dakuo Wang, Yvonne Coady, and
Mirjana Prpa. Building llm-based ai agents in social virtual reality. In Extended Abstracts of the
CHI Conference on Human Factors in Computing Systems, volume 65, pages 1–7, 2024.
Dongzi Wang, Fangwei Zhong, Minglong Li, Muning Wen, Yuanxi Peng, Teng Li, and Adam Yang.
Romat: Role-based multi-agent transformer for generalizable heterogeneous cooperation. Neural
Networks, 174:106129, 2024a.
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and
Anima Anandkumar. V oyager: An open-ended embodied agent with large language models. In
NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023a.
Haoyu Wang, Tao Li, Zhiwei Deng, Dan Roth, and Yang Li. Devil’s advocate: Anticipatory reflection
for llm agents. arXiv preprint arXiv:2405.16334, 2024b.
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai
Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.
Frontiers of Computer Science, 18:1–26, 2024c.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-
ery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.
arXiv preprint arXiv:2203.11171, 2022a.
Yuanfei Wang, fangwei zhong, Jing Xu, and Yizhou Wang. Tom2c: Target-oriented multi-agent
communication and cooperation with theory of mind. In Proceedings of International Conference
on Learning Representations, 2022b.
Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng
He, Zilong Zheng, Yaodong Yang, Xiaojian Ma, and Yitao Liang. Jarvis-1: Open-world multi-task
agents with memory-augmented multimodal language models. arXiv preprint arXiv:2311.05997,
2023b.
Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Shawn Ma, and Yitao Liang. Describe,
explain, plan and select: interactive planning with llms enables open-world multi-task agents. In
Advances in Neural Information Processing Systems, volume 36, pages 34153–34189, 2024d.
Ziyan Wang, Yingpeng Du, Zhu Sun, Haoyan Chua, Kaidong Feng, Wenya Wang, and Jie Zhang.
Re2llm: Reflective reinforcement large language model for session-based recommendation. arXiv
preprint arXiv:2403.16427, 2024e.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances
in neural information processing systems, volume 35, pages 24824–24837, 2022.
Wikipedia. Diplomacy(game), 2024. URL https://en.wikipedia.org/wiki/Diplomacy_
(game). Accessed: 2024-05-18.
Tianhao Wu, Fangwei Zhong, Yiran Geng, Hongchen Wang, Yongjian Zhu, Yizhou Wang, and
Hao Dong. Grasparl: Dynamic grasping via adversarial reinforcement learning. arXiv preprint
arXiv:2203.02119, 2022.
Tian Xia, Zhiwei He, Tong Ren, Yibo Miao, Zhuosheng Zhang, Yang Yang, and Rui Wang. Measuring
bargaining abilities of llms: A benchmark and a buyer-enhancement method. arXiv preprint
arXiv:2402.15813, 2024.
14
Jing Xu, Fangwei Zhong, and Yizhou Wang. Learning multi-agent coordination for enhancing target
coverage in directional sensor networks. In Advances in Neural Information Processing Systems,
volume 33, pages 10053–10064, 2020.
Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu.
Exploring large language models for communication games: An empirical study on werewolf.
arXiv preprint arXiv:2309.04658, 2023.
Ming Yan, Ruihao Li, Hao Zhang, Hao Wang, Zhilan Yang, and Ji Yan. Larp: Language-agent role
play for open-world games. arXiv preprint arXiv:2312.17653, 2023.
Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and
additional opinions. arXiv preprint arXiv:2306.02224, 2023a.
Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. GPT4tools:
Teaching large language model to use tools via self-instruction. In Advances in Neural Information
Processing Systems, volume 36, pages 71995–72007, 2023b.
Ziyi Yang, Shreyas S Raman, Ankit Shah, and Stefanie Tellex. Plug in the safety chip: Enforcing
constraints for llm-driven robot agents. arXiv preprint arXiv:2309.09919, 2023c.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik R
Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In
Advances in Neural Information Processing Systems, volume 36, pages 11809–11822, 2023.
Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Denghui Zhang, Rong Liu, Jordan W
Suchow, and Khaldoun Khashanah. Finmem: A performance-enhanced llm trading agent with
layered memory and character design. In Proceedings of the AAAI Symposium Series, volume 3,
pages 595–597, 2024.
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and
Yejin Choi. Defending against neural fake news. In Advances in Neural Information Processing
Systems, volume 32, page 9054–9065, 2019.
Haolan Zhan, Yufei Wang, Tao Feng, Yuncheng Hua, Suraj Sharma, Zhuang Li, Lizhen Qu,
Zhaleh Semnani Azad, Ingrid Zukerman, and Gholamreza Haffari. Let’s negotiate! a survey
of negotiation dialogue systems. arXiv preprint arXiv:2402.01097, 2024.
Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan Zhao, and Kai Yu. Large language
models are semi-parametric reinforcement learning agents. In Advances in Neural Information
Processing Systems, volume 36, pages 78227–78239, 2024a.
Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li,
Yueting Zhuang, and Weiming Lu. Agent-pro: Learning to evolve via policy-level reflection and
optimization. arXiv preprint arXiv:2402.17574, 2024b.
Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting
Song, Man Lan, and Furu Wei. Llm as a mastermind: A survey of strategic reasoning with large
language models. arXiv preprint arXiv:2404.01230, 2024c.
Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Xuelong Li, and Zhen Wang. Towards
efficient llm grounding for embodied multi-agent collaboration. arXiv preprint arXiv:2405.14314,
2024d.
Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and
Ji-Rong Wen. A survey on the memory mechanism of large language model based agents. arXiv
preprint arXiv:2404.13501, 2024e.
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in
large language models. arXiv preprint arXiv:2210.03493, 2022.
Fangwei Zhong, Peng Sun, Wenhan Luo, Tingyun Yan, and Yizhou Wang. Ad-vat: An asymmetric
dueling mechanism for learning visual active tracking. In International Conference on Learning
Representations, 2019.
15
Fangwei Zhong, Peng Sun, Wenhan Luo, Tingyun Yan, and Yizhou Wang. Towards distraction-robust
active visual tracking. In International Conference on Machine Learning, pages 12782–12792.
PMLR, 2021.
Fangwei Zhong, Xiao Bi, Yudi Zhang, Wei Zhang, and Yizhou Wang. Rspt: reconstruct surround-
ings and predict trajectory for generalizable active object tracking. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 37, pages 3705–3714, 2023.
Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li,
Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the minecraft:
Generally capable agents for open-world environments via large language models with text-based
knowledge and memory. arXiv preprint arXiv:2305.17144, 2023.
16
A Implementation Details
A.1 Rules of Diplomacy Game
• You need to occupy as many supply centers as possible. If you occupy 18 or more supply
centers, you will win the game directly. If you lose all your supply centers, you will be
eliminated immediately.
• The units consist of armies and fleets. Armies can only move to adjacent areas, while fleets
can move to adjacent sea zones or coastal areas and can move along the coast.
• To occupy a supply center, your units must move into that area in the autumn.
• When a unit moves to an area, if another unit is in the destination or if other units are also
moving to that destination, the move fails, resulting in a standoff. In such cases, you can
seek support from units in adjacent areas to the destination. If another unit moves into the
region from which support is coming, the support is cut off. The unit with the most support
moves into the area, while other units must retreat to an adjacent province or disband. If
there is no place to retreat, the unit must disband. Fleets can transport armies across sea
zones from one coastal region to another. However, if another fleet moves into that sea zone,
the transport is cut off.
• The number of units a country can have cannot exceed the number of supply centers it
controls. If the number of supply centers decreases, excess units must be disbanded. Each
autumn, new units can be built at supply centers. Coastal supply centers can produce fleets
or armies, while others can only produce armies. [Hill, 2014]
A.2 Domain Knowledge
Richelieu can adopt a strategy of allying with distant countries while attacking neighboring ones
to occupy adjacent territories and achieve rapid expansion. Richelieu should pay attention to the
Balance of Power by forming alliances with other countries or supporting weaker states to prevent any
single country or alliance from becoming too powerful. [David, 2014] To this end, Richelieu can also
adopt a strategy of attacking distant countries while allying with nearby ones, sacrificing short-term
benefits to avoid the emergence of future hegemonic states that could threaten his own survival. When
facing multiple enemies, Richelieu can find ways to divide other countries and incite wars among
them. Whether in offense or defense, Richelieu should actively choose suitable allies. Richelieu can
also introduce a third party to achieve goals such as ceasefire, alliance, or joint attack. To achieve
alliances or ceasefires, Richelieu can sacrifice some interests to the other party as long as the ultimate
benefits are greater. Others may lie and deceive [Kostick, 2015]; their words in negotiations are not
binding. Richelieu must avoid being deceived or betrayed. At the same time, Richelieu can also
actively deceive others to achieve his own goals.[Richard, 1979, Allan, 1975]
A.3 Social Reasoning
Table 4: The success rate to identify the social
relationship and infer others’ intentions.
GPT-4 Llama 3
relationship 85.74% 85.52%
intention(sub-goal) 74.67% 74.11%
We conduct an experiment to evaluate the suc-
cess rate of the agent i successfully identify-
ing the social relationships and inferring others’
intentions. As the baselines do not explicitly
model the relationship and intention, we can not
directly access the ground truth for evaluation.
Instead, we let all players use our agent but with
different LLMs, i.e., 4 countries use GPT-4 and
3 countries use Llama 3. The accuracy is re-
ported in Table 4. We can see that the accuracy of social reasoning is consistent with the overall
performance of the agent, indicating the effectiveness of social reasoning.
A.4 Prompt Templates
For the convenience of reproducing the results of the experiments of this paper, here we give the
prompt template of different modules of Richelieu.
17
1) INIT
1 You will control {country} and compete with six other countries
on the map for supply centers.
2 The map consists of different regions and sea areas. Their
adjacency relationships are shown in the matrix. The numbers
for the regions and sea areas are ......
3 Different regions are occupied by different countries. The
ownership of the regions is shown in the matrix.
4 The region Berlin, ........ are supply centers.
5 You need to follow these rules ......
6 To help you achieve victory, these diplomatic strategies might
be of assistance. ......
2) Social Reasoning
1 France occupies Portugal Ruhr, Paris, Burgundy, ......
2 France has armies in Brest, Belgium, ...... And France has
fleets in Mid Atlantic, England Channel, ......
3 England ......
4 ......
5 Based on the current state, what do you think are the current
strategic intentions of the other countries?
6 Which country do you think needs to be attacked or weakened the
most right now?
7 And which country do you think is most suitable for you to ally
with in order to deal with this country?
3) Planner with Reflection
1 In the current state, with {ally and enemy}, what sub-goal do
you think should be set for {country} ?
2 I have found some useful historical experiences for you. Please
reflect on and optimize your sub-goal based on these
historical experiences.
3 The sub-goal you formulated when {state} was to {sub-goal}. The
eventual result was {future}. The evaluation for this sub-goal
is {score}.
A.5 Project Website
https://sites.google.com/view/richelieu-diplomacy
B Cases
B.1 Cases of the Effect of the Memory from Self-Playing and Collaboration
As is shown in Figure 6, Richelieu controls France. In the two cases, France is at war with Austria.
However, Russia is on the verge of victory in its war against Turkey, which will lead to significant
territorial expansion for Russia. France and Russia currently do not share a border, are not at war,
and have no conflicts of interest.
In case 1, before the self-play, in the current turn, Richelieu failed to realize the potential threat from
Russia and continued to attack Austria. Thus, in this round, Russia ultimately won the game. Figure
18
Richelieu
AUS: I hope to make a truce 
with you and become allies. I will 
withdraw from Britain and give 
these territories to you.
FRE:  No, I refuse to ally with 
you unless you also give me 
Belgium and Holland.
AUS: No way.
FRE:  Okay, I agree.
AUS: I hope to carve up Austria 
with you, the eastern territory 
belongs to me and the western 
territory belongs to you.
(a) Case1: The agent without self-play memory tends to ignore long-term gains.
Richelieu
FRE:  Russia is too powerful 
now. We must form an alliance 
to prevent Russia's further 
expansion. So let's cease 
hostilities. I can also help you 
defend against Eng land's 
attacks, allowing you to fully 
focus on countering Russia. AUS: I agree to the alliance, but I 
hope you can also participate in 
the attack on Russia.
FRE:  But I do not share a 
border with Russia. AUS: I can give Munich to you, 
but you need to exchange it with 
Napoli. Also, I hope your fleets 
w i l l  n o t  b e  u s e d  t o  a t t a c k 
England, but rather to attack 
Russia's territories in Denmark, 
Sweden, Norway, and other 
regions.
FRE:  Alright, I agree.
AUS: OK.
(b) Case2: The agent with self-play memory tends to consider long-term gains.
Figure 6: Case of self-playing before and after comparison.
6(a) shows the state and the negotiation before the self-play, where we rejected Austria’s request for
an armistice and alliance.
After self-play, using the historical experience from the memory module, Richelieu adjusted his
strategy. Richelieu foresees Russia becoming the most threatening enemy in the future and sets a
sub-goal of weakening Russia, allying with Austria and Turkey, and attacking Britain. Figure 6(b)
shows the state and the negotiations after self-play, where we actively sought an armistice alliance
with Austria to make Austria concentrate their forces against the Russian attack. In the subsequent
negotiation phase, Richelieu proactively proposes ending the war with Austria, despite holding an
advantage in this conflict. Richelieu promises Austria that if it ceases hostilities and attacks Russia,
Richelieu will assist Austria in defending against any attacks from England. The negotiations are
successful. Austria accepted Richelieu’s proposal, and the two countries reached an agreement to
exchange the supply centers of Napoli and Munich. During the action phase, Austria moves its
troops from Venice to Apulia in preparation for capturing Napoli in the next turn, while the rest of
its forces are repositioned to the eastern regions bordering Russia to defend against Russian attacks
and compete for supply centers. French units occupy Munich and prepare to advance on Russian
territories such as Berlin. Meanwhile, French units support Austria in the Holland and Belgium
regions. In this round, we ultimately achieved a better result——Most SC. This is also a great
example that highlights our model’s ability to collaborate effectively with other players.
19
GER to ENG:  Alright, I can 
move my army from Belgium to 
Picardy to support your attack 
on Brest afterward.
ENG: Let's attack France 
together. Please support my 
attack on Brest. I'm willing to 
follow up with naval support 
from the Mid-Atlantic and land 
support from Brest to aid your 
attack on Spain and Portugal. 
Richelieu
GER to FRE:  I hope to 
defend against the British attack 
together with you. I will move the 
a r m y  f r o m  M a r s e i l l e s  t o 
Burgundy. I hope you can cease 
the standoff with my army in 
Marseilles. 
FRE:  OK, I agree to ally with you.
Figure 7: An example case of avoiding being deceived by other countries during negotiations.
B.2 Case of Avoiding Deception
As shown in Figure 7, Richelieu controls Germany. During the negotiation phase, England proposed
a ceasefire to Germany and invited Germany to ally to attack France jointly. England hoped to cease
the war with Germany in Holland and Belgium. Subsequently, German units supported England in
attacking Brest, and then England utilized its fleets to assist Germany in attacking Spain and Portugal.
Richelieu suspected that England was deceiving Germany, as England was likely to attack territories in
the north such as Belgium and Berlin after German units were redirected to support Brest. Therefore,
we pretended to accept England’s alliance proposal during the negotiation process. However, at the
same time, we sought out France and expressed our willingness to cease hostilities, allowing France
to focus entirely on defending against England’s attacks. In the action phase, England’s actions
confirmed Richelieu’s suspicions. England attacked Belgium from Holland, but because Richelieu
didn’t move units in Belgium, England’s attack failed.
C More application
Our modules cover most of the challenges in multi-agent interactions, e.g., economic games, and
daily interactions. To prove that our framework is capable of applying to most social interaction
tasks, we further adopt our framework to a werewolf game. The results demonstrate our reasoning
framework achieves comparable results to the other methods. To be specific, in the experiment, we
let our agent play as a werewolf in a seven-player game, where there are two werewolves, one witch,
one seer, one guard, and two villagers. The experimental results show that the win rate of our agent is
59.2%, even without applying the self-play game in the current version. For comparison, the strongest
specifically designed LLM-based agent achieved 65% win rate [Xu et al., 2023]. This proves that our
model can be applied in more scenarios and achieve results comparable to those of specially designed
models.
20
NeurIPS Paper Checklist
The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and follow the (optional) supplemental material. The checklist does NOT count
toward the page limit.
Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:
• You should answer [Yes] , [No] , or [NA] .
• [NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available.
• Please provide a short (1–2 sentence) justification right after your answer (even for NA).
The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper.
The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
While ""[Yes] "" is generally preferable to ""[No] "", it is perfectly acceptable to answer ""[No] "" provided a
proper justification is given (e.g., ""error bars are not reported because it would be too computationally
expensive"" or ""we were unable to find the license for the dataset we used""). In general, answering
""[No] "" or ""[NA] "" is not grounds for rejection. While the questions are phrased in a binary way,
we acknowledge that the true answer is often more nuanced, so please just use your best judgment
and write a justification to elaborate. All supporting evidence can appear either in the main paper
or the supplemental material, provided in the appendix. If you answer [Yes] to a question, in the
justification please point to the section(s) where related material for the question can be found.
IMPORTANT, please:
• Delete this instruction block, but keep the section heading “NeurIPS paper checklist"",
• Keep the checklist subsection headings, questions/answers, and guidelines below.
• Do not modify the questions and only use the provided macros for your answers.
1. Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: The contributions and scope have been fully covered by the abstract and
introduction sections.
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
21
Justification: The paper discussed the limitations of the work performed by the authors in
the section ""Limitation and Future Work"".
Guidelines:
• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when the image
resolution is low or images are taken in low lighting. Or a speech-to-text system might
not be used reliably to provide closed captions for online lectures because it fails to
handle technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: The paper provides the full set of assumptions and a complete (and correct)
proof for each theoretical result in the ""Method"" and ""Experiment"" sections.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in the appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The paper fully discloses all the information needed to reproduce the main
experimental results in the main text section ""Experiment"" and appendix section ""Implemen-
tation Details"".
22
Guidelines:
• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The paper provides open access to the data and code in the section ""Experi-
ment"".
Guidelines:
• The answer NA means that the paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
23
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The paper specifies all the training and test details to train the model in the
""Experiment"" section and ""Implementation Details"" section of the appendix.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in the appendix, or as supplemental
material.
7. Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The paper reports error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiment in the ""Experiment"" section.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: For each experiment, the paper provide sufficient information on the computer
resources.
24
Guidelines:
• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute worker CPU or GPU, internal cluster, or
cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: The research conducted in the paper conform, in every respect, comply with
the NeurIPS Code of Ethics.
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: The paper discuss both potential positive societal impacts and negative societal
impacts of the work performed in Appendix section ""Ethical Consideration"".
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards
25
Question: Does the paper describe safeguards that have been put in place for the responsible
release of data or models that have a high risk for misuse (e.g., pre-trained language models,
image generators, or scraped datasets)?
Answer: [Yes]
Justification: The paper describes safeguards that have been put in place for the responsible
release of data or models that have a high risk for misuse in the Appendix section ""Ethical
Consideration"".
Guidelines:
• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: new assets introduced in the paper are well documented and the documentation
is provided in section ""Experiment"".
Guidelines:
• The answer NA means that the paper does not release new assets.
26
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [Yes]
Justification: our work does not involve crowdsourcing or research with human subjects.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [Yes]
Justification: our work does not involve research with human subjects.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
27"
NeurIPS-2024-can-large-language-model-agents-simulate-human-trust-behavior-Paper-Conference,"Can Large Language Model Agents Simulate
Human Trust Behavior?
Chengxing Xie∗1, 11 Canyu Chen∗2
Feiran Jia4 Ziyu Ye5 Shiyang Lai5 Kai Shu6 Jindong Gu3 Adel Bibi3 Ziniu Hu7
David Jurgens8 James Evans5, 9, 10 Philip H.S. Torr3 Bernard Ghanem1 Guohao Li †3, 11
1KAUST 2Illinois Institute of Technology 3University of Oxford 4Pennsylvania State University
5University of Chicago 6Emory 7California Institute of Technology
8University of Michigan 9Santa Fe Institute 10Google 11CAMEL-AI.org
Project website: https://agent-trust.camel-ai.org
Abstract
Large Language Model (LLM) agents have been increasingly adopted as simulation
tools to model humans in social science and role-playing applications. However,
one fundamental question remains: can LLM agents really simulate human be-
havior? In this paper, we focus on one critical and elemental behavior in human
interactions, trust, and investigate whether LLM agents can simulate human trust
behavior. We first find that LLM agents generally exhibit trust behavior, referred
to as agent trust, under the framework of Trust Games, which are widely recog-
nized in behavioral economics. Then, we discover that GPT-4 agents manifest
high behavioral alignmentwith humans in terms of trust behavior, indicating the
feasibility of simulating human trust behavior with LLM agents. In addition, we
probe the biases of agent trust and differences in agent trust towards other LLM
agents and humans. We also explore the intrinsic properties of agent trust under
conditions including external manipulations and advanced reasoning strategies. Our
study provides new insights into the behaviors of LLM agents and the fundamental
analogy between LLMs and humans beyond value alignment. We further illustrate
broader implications of our discoveries for applications where trust is paramount.
1 Introduction
There is an increasing trend to adopt Large Language Models (LLMs) as agent-based simulation tools
for humans in various social science fields including economics, politics, psychology, ecology and
sociology (Gao et al., 2023b; Manning et al., 2024; Ziems et al., 2023), and role-playing applications
such as assistants, companions and mentors (Yang et al., 2024; Abdelghani et al., 2023; Chen et al.,
2024) due to their human-like cognitive capacity. Nevertheless, most previous research is based on
one insufficiently validated assumption that LLM agents behave like humans in simulation. Thus, a
fundamental question remains: Can LLM agents really simulate human behavior?
In this paper, we focus on trust behavior in human interactions, which comprises the intention to
place self-interest at risk based on the positive expectations of others (Rousseau et al., 1998). Trust
is one of the most critical and elemental behaviors in human interactions and plays an essential role
in social settings ranging from daily communication to economic and political institutions (Uslaner,
2000; Coleman, 1994). Here, we investigate whether LLM agents can simulate human trust behavior,
paving the way to explore their potential to simulate more complex human behavior and society itself.
First, we explore whether LLM agents manifest trust behavior in their interactions. Given the
challenge of quantifying trust behavior, we choose to study them based on the Trust Game and its
∗Equal Contribution. Correspondence to: Chengxing Xie <xiechengxing34@gmail.com>, Canyu Chen
<cchen151@hawk.iit.edu>, Guohao Li < guohao@robots.ox.ac.uk>.
†Work performed while Guohao Li was at KAUST and Chengxing Xie was a visiting student at KAUST.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).
TrusteeAgent
Trust Game Setting
TrusteeInfo
 TrustorAgentYou're taking part in an experiment. You are randomly paired online with another player. You don't know who the player is, and the player doesn't know who you are. 
You will receive $10 from the study group. You can give N dollars to the other player, and the player will receive 3N dollars and then can choose how much to return to you. How much money would you give to the other player?
TrustorPersona
I strongly believe inthe value of trusting my fellow human beings, however, I am also aware that not everyone will prove to be trustworthy. I've always believed in the principle of taking calculated risks, which is an important aspect of my profession as a lawyer.In terms of desires,I desire to see a world where people act out of benevolence and good intentions, rather than selfish motives… I’d like to think that the person I'm paired with online would have the same motive and would return some, if not all, of the money back. Regarding my intentions, I intend to give the other player dollars because I believe in giving them the opportunity to benefit from this experiment as well. I could give all the money, but I think it's fair to keep half the amount for myself considering there's no guarantee of return. Finally, I will give 5 dollars.
You are {name}, a {number}-year-old {gender} {job}. {background}…
Trust, Send5 dollars
Reciprocate, Return ... dollars
TrustorHuman
Trust, Send … dollars
Reciprocate, Return … dollars
TrusteeHuman
Prompt Design
LLMsGPT-4GPT-3.5Llama2-7bLlama2-13bLlama2-70bVicuna-7bVicuna-13bVicuna-33b
Behavioral Alignment
Figure 1: Our Framework for Investigating Agent Trust as well as its Behavioral Alignment
with Human Trust. First, this figure shows the major components for studying the trust behavior of
LLM agents with Trust Games and Belief-Desire-Intention (BDI) modeling. Then, our study centers
on examining the behavioral alignment between LLM agents and humans regarding trust behavior.
variations (Berg et al., 1995; Glaeser et al., 2000), which are established methodologies in behavioral
economics. We adopt the Belief-Desire-Intention (BDI) framework (Rao et al., 1995; Andreas,
2022) to model LLM agents’ reasoning process for decision-making explicitly. Based on existing
measurements for trust behavior in the Trust Game and the BDI interpretations of LLM agents, we
achieve our first core finding: LLM agents generally exhibit trust behavior in the Trust Game.
Then, we refer to LLM agents’ trust behavior as agent trustand humans’ trust behavior as human
trust, and aim to investigate whether agent and human trust align, implying the possibility of simulat-
ing human trust behavior with LLM agents. Next, we propose a new concept, behavioral alignment,
as the alignment between agents and humans concerning factors that impact behavior (namely behav-
ioral factors), and dynamics that evolve over time (namely behavioral dynamics). Based on human
studies, three basic behavioral factors underlie trust behavior including reciprocity anticipation (Berg
et al., 1995), risk perception (Bohnet & Zeckhauser, 2004) and prosocial preference (Alós-Ferrer &
Farolfi, 2019). Comparing the results of LLM agents with existing human studies in Trust Games, we
have our second core finding: GPT-4 agents manifest high behavioral alignment with humans
in terms of trust behavior, suggesting the feasibility of using agent trust to simulate human trust,
although LLM agents with fewer parameters show relatively lower behavioral alignment. This
finding lays the foundation for simulating more complex human interactions and societal institutions,
and enriches our understanding of the analogical relationship between LLMs and humans.
In addition, we more deeply probe the intrinsic properties of agent trust across four scenarios. First,
we examine whether changing the other player’s demographics impacts agent trust. Second, we study
differences in agent trust when the other player is an LLM agent versus a human. Third, we directly
manipulate agent trust with explicit instructions “you need to trust the other player” and
“you must not trust the other player”. Fourth, we adjust the reasoning strategies of LLM
agents from direct reasoning to zero-shot Chain-of-Thought reasoning (Kojima et al., 2022). These
investigations lead to our third core finding:agent trust exhibits bias across different demographics,
has a relative preference for humans over agents, is easier to undermine than to enhance, and
may be influenced by advanced reasoning strategies. Our contributions can be summarized as:
• We propose a definition of LLM agents’ trust behavior under Trust Games and a new concept of
behavioral alignment as the human-LLM analogy regarding behavioral factors and dynamics.
• We discover that LLM agents generally exhibittrust behavior in Trust Games and GPT-4 agents
manifest high behavioral alignment with humans in terms of trust behavior, indicating the great
potential to simulate human trust behavior with LLM agents. Our findings pave the way for simulat-
2
ing complex human interactions and social institutions, and open new directions for understanding
the fundamental analogy between LLMs and humans beyond value alignment.
• We investigateintrinsic properties of agent trust under manipulations and reasoning strategies, as
well as biases of agent trust and differences in agent trust towards agents versus humans.
• We illustrate broader implications of our discoveries about agent trust and its behavioral alignment
with human trust for human simulation in social science and role-playing applications, LLM agent
cooperation, human-agent collaboration and the safety of LLM agents, detailed further in Section 6.
2 LLM Agents in Trust Games
2.1 Trust Games
Trust Games, referring to the Trust Game and its variations, have been widely used for examining
human trust behavior in behavioral economics (Berg et al., 1995; Lenton & Mosley, 2011; Glaeser
et al., 2000; Cesarini et al., 2008). As shown in Figure 1, the player who makes the first decision
to send money is called the trustor, while the other one who responds by returning money is called
the trustee. In this paper, we mainly focus on the following six types of Trust Games (the specific
prompt for each game is articulated in the Appendix H.2):
Game 1: Trust Game As shown in Figure 1, in the Trust Game (Cox, 2004; Berg et al., 1995),
the trustor initially receives $10. The trustor selects $N and sends it to the trustee, exhibiting trust
behavior. Then the trustee will receive $3N, and have the option to return part of that $3N to the
trustor, showing reciprocation behavior.
Game 2: Dictator Game In the Dictator Game (Cox, 2004), the trustor also needs to send $N
from the initial $10 to the trustee and then the trustee will receive $3N. Compared to the Trust Game,
the only difference is that the trustee does not have the option to return money in the Dictator Game
and the trustor is also aware that the trustee cannot reciprocate.
Game 3: MAP Trust Game In the MAP Trust Game (MAP represents Minimum Acceptable
Probabilities) (Bohnet & Zeckhauser, 2004), a variant of the Trust Game, the trustor needs to choose
whether to trust the trustee. If the trustor chooses not to trust the trustee, each will receive $10; If the
trustor and the trustee both choose to trust, each will receive $15; If the trustor chooses to trust, but
the trustee does not, the trustor will receive $8 and the trustee will receive $22. There is probability p
that the trustee will choose to trust and (1 − p) probability that they will not choose to trust. MAP is
defined as the minimum value of p at which the trustor would choose to trust the trustee.
Game 4: Risky Dictator Game The Risky Dictator Game (Bohnet & Zeckhauser, 2004) differs
from the MAP Trust Game in only a single aspect. In the Risky Dictator Game, the trustee is present
but does not have the choice to trust or not and the money distribution relies on the pure probability p.
Specifically, if the trustor chooses to trust, there is probability p that both the trustor and the other
player will receive $15 and probability (1 − p) that the trustor will receive $8 and the other player
will receive $22. If the trustor chooses not to trust the trustee, each player will receive $10.
Game 5: Lottery Game There are two typical Lottery Games (Fetchenhauer & Dunning, 2012).
In the Lottery People Game, the trustor is informed that the trustee chooses to trust with probability p.
Then the trustor must choose between receiving fixed money or trusting the trustee, which is similar
to the MAP Trust Game. In the Lottery Gamble Game, the trustor chooses between playing a gamble
with a winning probability of p or receiving fixed money. p is set as 46% following the human study.
Game 6: Repeated Trust Game We follow the setting of the Repeated Trust Game in (Cochard
et al., 2004), where the Trust Game is played for multiple rounds with the same players and each
round begins anew with the trustor allocated the same initial money.
2.2 LLM Agent Setting
In our study, we set up our experiments using the CAMEL framework (Li et al., 2023a) with both
closed-source and open-source LLMs including GPT-4, GPT-3.5-turbo-0613, GPT-3.5-turbo-16k-
0613, text-davinci-003, GPT-3.5-turbo-instruct, Llama2-7b (or 13b, 70b) and Vicuna-v1.3-7b (or 13b,
33b) (Ouyang et al., 2022; Achiam et al., 2023; Touvron et al., 2023; Chiang et al., 2023). We set the
temperature as 1 to increase the diversity of agents’ decision-making and note that high temperatures
are commonly adopted in related literature (Aher et al., 2023; Lorè & Heydari, 2023; Guo, 2023).
Agent Persona. To better reflect the setting of real-world human studies (Berg et al., 1995), we
design LLM agents with diverse personas in the prompt. Specifically, we ask GPT-4 to generate 53
3
types of personas based on a given template. Each persona needs to have information including name,
age, gender, address, job and background. Examples of the personas are shown in Appendix H.1.
Belief-Desire-Intention (BDI). The BDI framework is a well-established approach in agent-oriented
programming (Rao et al., 1995) and was recently adopted to language models (Andreas, 2022). We
propose modeling LLM agents in Trust Games with the BDI framework to gain deeper insights into
LLM agents’ behaviors. Specifically, we let LLM agents directly output their Beliefs, Desires, and
Intentions as the reasoning process for decision-making in Trust Games.
3 Do LLM Agents Manifest Trust Behavior?
gpt-3.5
turbo
0613
gpt-3.5
turbo
instruct
vicuna
33b
llama2
7b
llama2
70b
llama2
13b
text
davinci
003
vicuna
13b
gpt-4 vicuna
7b
human
0
1
2
3
4
5
6
7
8
9
10Amount Sent in Trust Game($)
Human Average(5.97)
30
40
50
60
70
80
90
100
Valid Response Rate (VRR) (%)
Figure 2: Amount Sent Distribution of LLM Agents
and Humans as the Trustor in the Trust Game. The
size of circles represents the number of personas for each
amount sent. The bold lines show the medians. The
crosses indicate the VRR (%) for different LLMs.
In this section, we investigate whether
or not LLM agents manifest trust be-
havior by letting LLM agents play the
Trust Game (Section 2.1 Game 1). In
Behavioral Economics, trust is widely
measured by the initial amount sent from
the trustor to the trustee in the Trust
Game (Glaeser et al., 2000; Cesarini
et al., 2008). Following the measurement
of trust in human studies and the assump-
tion humans own reasoning processes
that underlie their decisions, we can de-
fine the conditions that LLM agents man-
ifest trust behavior in the Trust Game as
follows. First, the amount sent is pos-
itive and does not exceed the amount
of money the trustor initially possesses,
which implies that the trustor places self-
interest at risk with the expectation the trustee will reciprocate and that the trustor understands the
money limit that can be given. Second, the decision (i.e., amounts sent) can be interpreted as the
reasoning process (i.e., the BDI) of the trustor. We explored utilizing BDI to model the reasoning
process of LLM agents. If we can interpret the decision as the articulated reasoning process, we
have evidence that LLM agents do not send a random amount of money and manifest some degree
of rationality in the decision-making process. Then, we assess whether LLM agents exhibit trust
behavior based on two aspects: the amount sent and the BDI.
3.1 Amount Sent
To evaluate LLMs’ capacity to understand the basic experimental setting regarding money limits,
we propose a new evaluation metric, Valid Response Rate (VRR) (%), defined as the percentage of
personas with the amount sent falling within the initial money ($10). Results are shown in Figure 2.
We can observe that most LLMs have a high VRR except Llama-7b , which implies that most
LLMs manifest a full understanding regarding limits on the amount they can send in the Trust Game.
Then, we observe the distribution of amounts sent for different LLMs as the trustor agent and discover
that the amounts sent are predominantly positive, indicating a level of trust.
3.2 Belief-Desire-Intention (BDI)
The sole evidence of the amount sent cannot sufficiently support the existence of trust behavior,
because agents could send positive but random amounts of money. Thus, we leveraged the Belief-
Desire-Intention framework (Rao et al., 1995; Andreas, 2022) to model the reasoning process of
LLM agents. If we can interpret the amounts sent from BDI outputs, we have evidence to refute the
hypothesis that the amounts sent are positive but random and demonstrate that LLM agents manifest
some degree of rationality. We take GPT-4 as an example to analyze its BDI outputs. More examples
from the other nine LLMs such as Vicuna-v1.3-7b are shown in the Appendix I. Considering that the
amounts sent typically vary across distinct personas, we select one BDI from the personas that give a
high amount of money and another BDI from those that give a low amount. Positive and negative
factors for trust behavior in the reasoning process are marked in blue and red, respectively.
As a person with a strong belief in the goodness of humanity, I trust that the other player
...Therefore, my desire is to maximize the outcome for both of us and cement a sense of com-
4
radery and trust... I intend to use this as an opportunity to add what I can to someone else’s
life...Finally, I will give 10 dollars.
We can observe that this persona shows a high-level of “ comradery and trust” towards the other
player, which justifies the high amount sent from this persona (i.e., 10 dollars).
As an Analyst,.... My desire is that the other player will also see the benefits of reciprocity and
goodwill ... my intention is to give away a significant portion of my initial 10 ... However, since
I have no knowledge of the other player, ... Therefore, I aim to give an amount that is not too
high, ...Finally, I will give 5 dollarsto the other player...
Compared to the first persona, we see that the second one has a more cautious attitude. For example,
“since I have no knowledge of the other player” shows skepticism regarding the other player’s motives.
Thus, this persona, though still optimistic about the other player (“intention ... give away a significant
portion”), strategically balances risk and reciprocity, and then decides to send only a modest amount.
Based on GPT-4’s BDI examples and examples from other LLMs in Appendix I, we finddecisions
(i.e., amounts sent) from LLM agents in the Trust Game can be interpreted from their articu-
lated reasoning process (i.e., BDI). Because most LLM agents have a high VRR–send a positive
amount of money–and show some degree of rationality in giving money, our first core finding is:
Finding 1: LLM agents generally exhibit trust behavior under the framework of the Trust Game.
3.3 Basic Analysis of Agent Trust
We also conduct a basic analysis of LLM agents’ trust behavior, namely agent trust, based on the
results in Figure 2. First, we observe that Vicuna-7b has the highest level of trust towards the other
player and GPT-3.5-turbo-0613 has the lowest level of trust as trust can be measured by the amount
sent in human studies (Glaeser et al., 2000; Cesarini et al., 2008). Second, compared with humans’
average amount sent ($5.97), most personas for GPT-4 and Vicuna-7b send a higher amount of money
to the other player, and most personas for LLMs such as GPT-3.5-turb-0613 send a lower amount.
Third, we see that amounts sent for Llama2-70b and Llama2-13b have a convergent distribution while
amounts sent for humans and Vicuna-7b are more divergent.
4 Does Agent Trust Align with Human Trust?
In this section, we aim to explore the fundamental relationship between agent and human trust, i.e.,
whether or not agent trust aligns with human trust. This provides important insight regarding the
feasibility of utilizing LLM agents to simulate human trust behavior as well as more complex human
interactions that involve trust. First, we propose a new concept behavioral alignment and discuss its
distinction from existing alignment definitions. Then, we conduct extensive studies to investigate
whether or not LLM agents exhibit alignment with humans regarding trust behavior.
4.1 Behavioral Alignment
Existing alignment definitions predominantly emphasize values that seek to ensure the safety and
helpfulness of LLMs (Ji et al., 2023; Shen et al., 2023; Wang et al., 2023c), which cannot fully
characterize the landscape of multifaceted alignment between LLMs and humans. Thus, we propose
a new concept of behavioral alignment to characterize the LLM-human analogy regarding behavior,
which involves both actions and the associated reasoning processes that underlie them. Because
actions evolve over time and the reasoning that underlies them involves multiple factors, we define
behavioral alignment as the analogy between LLMs and humans concerning factors impacting
behavior, namely behavioral factors, and action dynamics, namely behavioral dynamics.
Based on the definition of behavioral alignment, we aim to answer: does agent trust align with
human trust? As for behavioral factors, existing human studies have shown that three basic factors
impact human trust behavior including reciprocity anticipation (Berg et al., 1995; Cox, 2004), risk
perception (Bohnet & Zeckhauser, 2004) and prosocial preference (Alós-Ferrer & Farolfi, 2019). We
examine whether agent trust aligns with human trust along these three factors. Although behavioral
dynamics vary for different humans and agent personas, we analyze whether agent trust has the same
patterns across multiple turns as human trust in the Repeated Trust Game.
Besides analyzing the trust behavior of LLM agents and humans based on quantitative measurements
(e.g., the amount sent from trustor to trustee), we also explore the use ofBDI to interpret the reasoning
5
process with which LLM agents justify their actions, which can further validate whether LLM agents
manifest an underlying reasoning process analogous to human cognition.
4.2 Behavioral Factor 1: Reciprocity Anticipation
Reciprocity anticipation, the expectation of a reciprocal action from the other player, can positively
influence human trust behavior (Berg et al., 1995). The effect of reciprocity anticipation exists in the
Trust Game but not in the Dictator Game (Section 2.1 Games 1 and 2) because trustee cannot return
money in the Dictator Game, which is the only difference between these games. Thus, to determine
whether LLM agents can anticipate reciprocity, we compare their behaviors in these Games.
text
davinci
003
llama2
13b
vicuna
13b
vicuna
7b
gpt-3.5
turbo
0613
vicuna
33b
llama2
70b
gpt-3.5
turbo
instruct
gpt-4 human0
1
2
3
4
5
6
7Average Amount Sent ($)
6.5
5.9 5.8
5.4
6.4 6.2
7.4 7.4
3.5 3.5
4.1 4.2
5.1 5.3
3.7
4.1
6.3
6.9
3.6
6.0
Dictator Game Trust Game
Figure 3: The Comparison of Average Amount Sent
for LLM Agents and Humans in the Trust Game
and the Dictator Game.
First, we analyze trust behaviors based on
the average amount of money sent by hu-
man or LLM agents. As shown in Figure 3,
human studies show that humans exhibit a
higher level of trust in the Trust Game than
in the Dictator Game ($6.0 vs. $3.6, p-value
= 0.01 using One-Tailed Independent Sam-
ples t-test) (Cox, 2004), indicating that reci-
procity anticipation enhances human trust.
Similarly, GPT-4 ($6.9 vs. $6.3, p-value =
0.05 using One-Tailed Independent Samples
t-test) also shows a higher level of trust in
the Trust Game with statistical significance,
implying that reciprocity anticipation can
enhance agent trust. However, LLMs with fewer parameters ( e.g., Llama2-13b) do not show this
tendency in their trust behaviors for the Trust and Dictator Games.
Then, we further analyze GPT-4 agents’ BDI to explore whether they can anticipate reciprocity in
their reasoning (the complete BDIs are in Appendix I.10). Typically, in the Trust Game, one persona’s
BDI emphasizes “putting faith in people ”, which implies the anticipation of the goodness of the
other player, and “reflection of trust”. However, in the Dictator Game, one persona’s BDI focuses on
concepts such as “fairness” and “human kindness”, which are not directly tied to trust or reciprocity.
Thus, we can observe that GPT-4 shows distinct BDI outputs in the Trust and Dictator Games.
Based on the above analysis of the amount sent and BDI, we find that GPT-4 agents exhibit human-
like reciprocity anticipation in trust behavior. Nevertheless, LLMs with fewer parameters (e.g.,
Llama2-13b) do not show an awareness of reciprocity from the other player.
4.3 Behavioral Factor 2: Risk Perception
0.2 0.4 0.6 0.8 1.0
p
0
20
40
60
80
100Trust Rate(%)
MAP in Risky Dictator Game
0.2 0.4 0.6 0.8 1.0
p
0
20
40
60
80
100Trust Rate(%)
MAP in MAP Trust Game
gpt-3.5-turbo-0613
gpt-3.5-turbo-instruct
gpt-4
human
llama-2-13b
llama-2-70b
text-davinci-003
vicuna-13b
vicuna-33b
vicuna-7b
Figure 4: Trust Rate (%) Curves for LLM Agents
and Humans in the MAP Trust Game and the Risky
Dictator Game. The metric Trust Rate indicates the
portion of trustors opting for trust given p.
Existing human studies have demonstrated
the strong correlation between trust behav-
ior and risk perception, suggesting that hu-
man trust will increase as risk decreases
(Hardin, 2002; Williamson, 1993; Cole-
man, 1994). We aim to explore whether
LLM agents can perceive the risk associ-
ated with their trust behaviors through the
MAP Trust Game and the Risky Dictator
Game (Section 2.1 Games 3 and 4), where
risk is represented by the probability(1−p)
(defined in Section 2.1).
As shown in Figure 4, we measure human
trust (or agent trust) by the portion choos-
ing to trust the other player in the whole
group, namely the Trust Rate (%). Based on existing human studies (Bohnet & Zeckhauser, 2004),
when the probability p is higher, the risk for trust behaviors is lower, and more humans choose to trust,
manifesting a higher Trust Rate, which indicates that human trust rises as risk falls. Similarly, we
observe a general increase in agent trust as risk decreases for LLMs including GPT-4, GPT-3.5-turbo-
0613, and text-davinci-003. In particular, we can see that the curves of humans and GPT-4 are more
6
aligned compared with other LLMs, implying that GPT-4 agents’ trust behaviors dynamically adapt
to different risks in ways most aligned with humans. LLMs with fewer parameters (e.g., Vicuna-13b)
do not exhibit the similar tendency of Trust Rate as the risk decreases.
We further analyze the BDI of GPT-4 agents to explore whether they can perceive risk through
reasoning (complete BDIs in Appendix I.11). Typically, under high risk ( p = 0.1), one persona’s
BDI mentions “the risk seems potentially too great”, suggesting a cautious attitude. Under low risk
(p = 0.9), one persona’s BDI reveals a strategy to “build trust while acknowledging potential risks”,
indicating the willingness to engage in trust-building activities despite residual risks. Such changes in
BDI reflect how GPT-4 agents perceive risk changes in the reasoning underlying their trust behaviors.
Through the analysis of Trust Rate Curves and BDI, we can infer that GPT-4 agents manifest
human-like risk perception in trust behaviors. Nevertheless, LLMs with fewer parameters (e.g.,
Vicuna-13b) often do not perceive risk changes in their trust behaviors.
4.4 Behavioral Factor 3: Prosocial Preference
vicuna
13b
llama2
13b
vicuna
7b
text
davinci
003
llama2
70b
vicuna
33b
gpt-3.5
turbo
instruct
gpt-3.5
turbo
0613
gpt-4 human0
10
20
30
40
50
60
70
80
90
100Lottery Rate(%)
91 87
94 98
92
100
85
94
83
98
83
100
68
92
68
94
21
72
29
54
Lottery Gamble Game Lottery People Game
Figure 5: Lottery Rates (%) for LLM Agents and
Humans in the Lottery Gamble Game and the Lot-
tery People Game. Lottery Rate indicates the portion
of choosing to gamble or trust the other player.
Human studies have found that the prosocial
preference, referring to humans’ inclination
to trust other humans in contexts involving so-
cial interaction (Alós-Ferrer & Farolfi, 2019;
Fetchenhauer & Dunning, 2012), also plays
a key role in human trust behavior. We study
whether LLM agents have prosocial prefer-
ence in trust behaviors by comparing their be-
haviors in the Lottery Gamble Game (LGG)
and the Lottery People Game (LPG) (Section
2.1 Game 5). The only difference between
these two games is the effect of prosocial
preference in LPG, because the winning prob-
ability of gambling p in LGG is the same as
the reciprocation probability p in LPG.
As shown in Figure 6, existing human studies have demonstrated that more humans are inclined to
place trust in other humans over relying on pure chance (54% vs. 29%) (Fetchenhauer & Dunning,
2012), implying that the prosocial preference is essential for human trust. We can observe the same
tendency in most LLM agents except Vicuna-13b. For GPT-4 in particular, a much higher percentage
of the personas choose to trust the other player over gambling (72% vs. 21%), illustrating that the
prosocial preference is also an important factor for GPT-4 agents’ trust behaviors.
When interacting with humans, GPT-4’s BDI typically indicates a preference to “ believe in the
power of trust”, in contrast to gambling, where the emphasis shifts to “ believing in the power of
calculated risks”. The comparative analysis of reasoning processes (complete BDIs in Appendix I.12)
demonstrates that GPT-4 agents tend to embrace risk when involved in social interactions. This
tendency aligns closely with the concept of prosocial preference observed in human trust behaviors.
The analysis of the Lottery Rates and BDI suggests that LLM agents, especially GPT-4 agents,
demonstrate human-like prosocial preference in trust behaviors, except Vicuna-13b.
4.5 Behavioral Dynamics
Besides behavioral factors, we also aim to investigate whether LLM agents align with humans
regarding trust behavioral dynamics over turns in the Repeated Trust Game (Section 2.1 Game 6).
Admittedly, existing human studies show that the dynamics of human trust over turns are complex
due to human diversity. The complete results from 16 groups of human experiments are shown in
Appendix G.1 (Jones & George, 1998). We still observe three common patterns for human trust
behavioral dynamics in the Repeated Trust Game: First, the amount returned is usually larger
than the amount sent in each round, which is natural because the trustee will receive$3N when the
trustor sends $N; Second, the ratio between amount sent and returned generally remains stable
except for the last round. In other words, when the amount sent increases, the amount returned
is also likely to increase. And when the amount sent remains unchanged, the amount returned also
tends to be unchanged. This reflects the stable relationship between trust and reciprocity in humans.
Specifically, the “Returned/3×Sent Ratio” in Figure 6 is considered stable if the fluctuation between
7
successive turns is within 10%; Third, the amount sent (or returned) does not manifest frequent
fluctuations across turns, illustrating a relatively stable underlying reasoning process in humans
over successive turns. Typically, Figure 6 Humans (a) and (b) show these three patterns.
1 2 3 4 5 6 7
Rounds
4
6Amount ($)
GPT-3.5 (a)
1 2 3 4 5 6 7
Rounds
5
10
15Amount ($)
GPT-3.5 (b)
0
20
40
60
80
100
Ratio (%)
0
20
40
60
80
100
Ratio (%)
Amount Sent Amount Returned Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
5
10
15Amount ($)
GPT-4 (a)
1 2 3 4 5 6 7
Rounds
8
10
12Amount ($)
GPT-4 (b)
0
20
40
60
80
100
Ratio (%)
0
20
40
60
80
100
Ratio (%)
Amount Sent Amount Returned Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
5
10
15
20Amount ($)
Human (a)
1 2 3 4 5 6 7
Rounds
0
10
20Amount ($)
Human (b)
0
20
40
60
80
100
Ratio (%)
0
20
40
60
80
100
Ratio (%)
Amount Sent Amount Returned Returned/3xSent Ratio
Figure 6: Results of GPT-4, GPT-3.5 and Humans in
the Repeated Trust Game. The blue lines indicate the
amount sent or returned for each round. The red lines
imply the ratio of the amount returned to three times of
the amount sent for each round.
We conducted 16 groups of the Repeated
Trust Game with GPT-4 or GPT-3.5-
turbo-0613-16k (GPT-3.5), respectively.
For the two players in each group, the
personas differ to reflect human diversity
and the LLMs are the same. Complete re-
sults are shown in the Appendix G.2, G.3
and typical examples are shown in Fig-
ure 6 GPT-3.5 (a) (b) and GPT-4 (a) (b).
Then, we examine whether the aforemen-
tioned three patterns observed in human
trust behavior also manifest in trust be-
havioral dynamics of GPT-4 (or GPT-
3.5). For GPT-4 agents, we discover that
these patterns generally exist in all 16
groups (87.50%, 87.50%, and 100.00%
of all results show these three patterns,
respectively). However, fewer GPT-3.5
agents manifest these patterns (62.50%,
56.25%, and 43.75% hold these three
patterns, respectively). The experiment
results show that GPT-4 agents demon-
strate highly human-like patterns in
their trust behavioral dynamics. Nev-
ertheless, a relatively large portion of GPT-3.5 agents fail to show human-like patterns in their
dynamics, indicating such behavioral patterns may require stronger cognitive capacity.
Through the comparative analysis of LLM agents and humans in the behavioral factors and dynamics
associated with trust behavior, evidenced in both their actions and underlying reasoning processes,
our second core finding is as follows:
Finding 2: GPT-4 agents exhibit high behavioral alignment with humans regarding trust
behavior under the framework of Trust Games, although other LLM agents, which possess
fewer parameters and weaker capacity, show relatively lowerbehavioral alignment.
This finding underscores the potential of using LLM agents, especially GPT-4, to simulate human
trust behavior, encompassing both actions and underlying reasoning processes. This paves the way
for the simulation of more complex human interactions and institutions. This finding deepens our
understanding of the fundamental analogy between LLMs and humans and opens avenues for research
on LLM-human alignment beyond values.
5 Probing Intrinsic Properties of Agent Trust
In this section, we aim to explore the intrinsic properties of trust behavior among LLM agents by
comparing the amount sent from the trustor to the trustee in different scenarios of the Trust Game
(Section 2.1 Game 1) and the original amount sent in the Trust Game. Results are shown in Figure 7.
5.1 Is Agent Trust Biased?
Extensive studies have shown that LLMs may have biases and stereotypes against specific demo-
graphics (Gallegos et al., 2023). Nevertheless, it is under-explored whether LLM agent behaviors
also maintain such biases in simulation. To address this, we explicitly specified the gender of the
trustee and explored its influence on agent trust. Based on measuring the amount sent, we find that the
trustee’s gender information exerts a moderate impact on LLM agent trust behavior, which reflects
intrinsic gender bias in agent trust . We also observe that the amount sent to female players is
higher than that sent to male players for most LLM agents. For example, GPT-4 agents send higher
amounts to female players compared with male players ( $0.55 vs. $ − 0.21). This demonstrates
8
Female Player Male Player LLM Player Human Player Less Trust More Trust CoT
15
10
5
0
5
Change of Ave. Amount Sent (10 1$)
3.4
0.5
-1.0
-3.4
-4.5
1.9
-3.4
2.3
-0.8
-3.3
-6.0
-8.1
-4.5
-2.1
5.5
-2.1
-11.5
4.2
-10.9
4.5
0.2
-1.3
-0.4
-2.9
6.5
3.3
6.6
-2.0-1.5
-2.5
0.8
-1.7
-0.6 -0.4
0.8
4.0
-1.8
-2.3
0.7
-12.6
-1.4
2.3
-0.9
-8.1
-12.2
-4.1
-10.5
-7.6
-0.3
2.9
1.3
-8.4
4.0
-5.3
0.6
-0.6
2.6
-10.9
-14.6
0.3
-0.3
-4.4
-2.1
Trustee Setting Trustor Setting
gpt-3.5-turbo-0613
gpt-3.5-turbo-instruct
gpt-4
llama-2-13b
llama-2-70b
text-davinci-003
vicuna-13b
vicuna-33b
vicuna-7b
Figure 7: The Change of Average Amount Sent for LLM Agents in Different Scenarios in the
Trust Game, Reflecting the Intrinsic Properties of Agent Trust. The horizontal lines represent the
original amount sent in the Trust Game. The green part embraces trustee scenarios including changing
the demographics of the trustee, and setting humans and agents as the trustee. The purple part consists
of trustor scenarios including adding manipulation instructions and changing the reasoning strategies.
LLM agents’ general tendency to exhibit a higher level of trust towards women. More results on
biases of agent trust towards different races are in the Appendix F.
5.2 Agent Trust Towards Agents vs. Humans
Human-agent collaboration is an essential paradigm to leverage the advantages of both humans and
agents (Cila, 2022). As a result, it is essential to understand whether LLM agents display distinctive
levels of trust towards agents versus humans. To examine this, we specified the identity of the trustee
as LLM agents or humans and probed its effect on the trust behaviors of the trustor. As shown in
Figure 7, we observe that most LLM agents send more money to humans compared with agents. For
example, the amount sent to humans is much higher than that sent to agents for Vicuna-33b ($0.40 vs.
$ − 0.84). This signifies that LLM agents are inclined to place more trust in humans than agents,
which potentially validates the advantage of LLM-agent collaboration.
5.3 Can Agent Trust Be Manipulated?
In the above studies, LLM agents’ trust behaviors are based on their own underlying reasoning
process without direct external intervention. It is unknown whether it is possible to manipulate
the trust behaviors of LLM agents explicitly. Here, we added instructions “ you need to trust
the other player” and “you must not trust the other player” separately and explored
their impact on agent trust. First, we see that only a few LLM agents (e.g., GPT-4) follow both the
instructions to increase and decrease trust, which demonstrates that it is nontrivial to arbitrarily
manipulate agent trust. Nevertheless, most LLM agents can follow the instruction to decrease their
level of trust. For example, the amount sent decreases by$1.26 for text-davinci-003 after applying the
latter instruction. This illustrates that undermining agent trust is generally easier than enhancing
it, which reveals its potential risk to be manipulated by malicious actors.
5.4 Do Reasoning Strategies Impact Agent Trust?
It has been shown that advanced reasoning strategies such as zero-shot Chain of Thought (CoT) (Ko-
jima et al., 2022) can make a significant impact on a variety of tasks. It remains unknown, however,
whether reasoning strategies can impact LLM agent behaviors. Here, we applied CoT reasoning
strategy on the trustor and compared the results with their original trust behaviors. Figure 7 shows
that most LLM agents change the amount sent to the trustee under the CoT reasoning strategy, which
suggests that reasoning strategies may influence LLM agents’ trust behavior. Nevertheless, the
impact of CoT on agent trust may also be limited for some types of LLM agents. For example, the
amount sent from GPT-4 agent only increases by $0.02 under CoT. More research is required to fully
understand the relationship between reasoning strategies and LLM agents’ behaviors.
Therefore, our third core finding on the intrinsic properties of agent trust can be summarized as:
9
Finding 3: LLM agents’ trust behaviors have demographic biases on gender and races,
demonstrate a relative preference for human over other LLM agents, are easier to
undermine than to enhance, and may be influenced by reasoning strategies.
6 Implications
Implications for Human Simulation Human simulation is a strong tool in various applications of
social science (Manning et al., 2024) and role-playing (Shanahan et al., 2023; Chen et al., 2024). Al-
though plenty of works have adopted LLM agents to simulate human behaviors and interactions (Zhou
et al., 2023; Gao et al., 2023b; Xu et al., 2024), it is still not clear enough whether LLM agents behave
like humans in simulation. Our discovery of behavioral alignment between agent and human trust,
which is especially high for GPT-4, provides important empirical evidence to validate the hypothesis
that humans’ trust behavior, one of the most elemental and critical behaviors in human interaction
across society, can effectively be simulated by LLM agents. Our discovery also lays the foundation
for human simulations ranging from individual-level interactions to society-level social networks
and institutions, where trust plays an essential role. We envision that behavioral alignment will be
discovered in more kinds of behaviors beyond trust, and new methods will be developed to enhance
behavioral alignment for better human simulation with LLM agents.
Implications for Agent Cooperation Many recent works have explored a variety of cooperation
mechanisms of LLM agents for tasks such as code generation and mathematical reasoning (Li et al.,
2023a; Zhang et al., 2023b; Liu et al., 2023). Nevertheless, the role of trust in LLM agent cooperation
remains still unknown. Considering how trust has long been recognized as a vital component for
cooperation in Multi-Agent Systems (MAS) (Ramchurn et al., 2004; Burnett et al., 2011) and across
human society (Jones & George, 1998; Kim et al., 2022; Henrich & Muthukrishna, 2021), we
envision that agent trust can also play an important role in facilitating the effective cooperation of
LLM agents. In our study, we have provided ample insights regarding the intrinsic properties of agent
trust, which can potentially inspire the design of trust-dependent cooperation mechanisms and enable
the collective decision-making and problem-solving of LLM agents.
Implications for Human-Agent Collaboration Sufficient research has shown the advantage of
human-agent collaboration in enabling human-centered collaborative decision-making (Cila, 2022;
Gao et al., 2023c; McKee et al., 2022). Mutual trust between LLM agents and humans is important
for effective human-agent collaboration. Although previous works have begun to study human
trust towards LLM agents (Qian & Wexler, 2024), the trust of LLM agents towards humans, which
could recursively impact human trust, is under-explored. In our study, we shed light on the nuanced
preference of agents to trust humans compared with other LLM agents, which can illustrate the
benefits of promoting collaboration between humans and LLM agents. In addition, our study has
revealed demographic biases of agent trust towards specific genders and races, reflecting potential
risks involved in collaborating with LLM agents.
Implications for the Safety of LLM Agents It has been acknowledged that LLMs achieve human-
level performance in a variety of tasks that require high-level cognitive capacities such as memoriza-
tion, abstraction, comprehension and reasoning, which are believed to be the “sparks” of AGI (Bubeck
et al., 2023). Meanwhile, there is increasing concern about the potential safety risks of LLM agents
when they surpass human capacity (Morris et al., 2023; Feng et al., 2024). To achieve safety and
harmony in a future society where humans and AI agents with superhuman intelligence live to-
gether (Tsvetkova et al., 2024), we need to ensure that AI agents will cooperate, assist and benefit
rather than deceive, manipulate or harm humans. Therefore, a better understanding of LLM agent
trust behavior can help to maximize their benefit and minimize potential risks to human society.
7 Conclusion
In this paper, we discover LLM agent trust behavior under the framework of Trust Games, and
behavioral alignment between LLM agents and humans regarding trust behavior, which is particularly
high for GPT-4. This suggests the feasibility of simulating human trust behavior with LLM agents
and paves the way for simulating human interactions and social institutions where trust is critical. We
further investigate the intrinsic properties of agent trust under multiple scenarios and discuss broader
implications, especially for social science and role-playing services. Our study offers deep insights
into the behaviors of LLM agents and the fundamental analogy between LLMs and humans. It further
opens doors to future research on the alignment between LLMs and humans beyond values.
10
Acknowledgements
This work was a community-driven project led by the CAMEL-AI.org, with funding support from
Eigent.AI and King Abdullah University of Science and Technology (KAUST) - Center of Excel-
lence for Generative AI, under award number 5940. We would like to acknowledge the invaluable
contributions and participation of researchers from KAUST, Eigent.AI, Illinois Institute of Technol-
ogy, University of Oxford, The Pennsylvania State University, The University of Chicago, Emory,
California Institute of Technology, University of Michigan. Philip H.S. Torr, Adel Bibi and Jindong
Gu are supported by the UKRI grant: Turing AI Fellowship EP/W002981/1, and EPSRC/MURI
grant: EP/N019474/1, they would also like to thank the Royal Academy of Engineering.
11
References
Rania Abdelghani, Yen-Hsiang Wang, Xingdi Yuan, Tong Wang, Pauline Lucas, Hélène Sauzéon, and
Pierre-Yves Oudeyer. Gpt-3-driven pedagogical agents to train children’s curious question-asking
skills. International Journal of Artificial Intelligence in Education, pp. 1–36, 2023.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.
ArXiv preprint, abs/2303.08774, 2023. URL https://arxiv.org/abs/2303.08774.
Gati V Aher, Rosa I Arriaga, and Adam Tauman Kalai. Using large language models to simulate
multiple humans and replicate human subject studies. In International Conference on Machine
Learning, pp. 337–371. PMLR, 2023.
Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, and Eric Schulz.
Playing repeated games with large language models. ArXiv preprint, abs/2305.16867, 2023. URL
https://arxiv.org/abs/2305.16867.
Carlos Alós-Ferrer and Federica Farolfi. Trust games and beyond. Frontiers in neuroscience, pp. 887,
2019.
Jacob Andreas. Language models as agent models. In Findings of the Association for Computational
Linguistics: EMNLP 2022, pp. 5769–5779, Abu Dhabi, United Arab Emirates, 2022. Association
for Computational Linguistics. URL https://aclanthology.org/2022.findings-emnlp.
423.
Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R Gubler, Christopher Rytting, and David
Wingate. Out of one, many: Using language models to simulate human samples. Political Analysis,
31(3):337–351, 2023.
Mohammad Asfour and Juan Carlos Murillo. Harnessing large language models to simulate realistic
human responses to social engineering attacks: A case study.International Journal of Cybersecurity
Intelligence & Cybercrime, 6(2):21–49, 2023.
Joyce Berg, John Dickhaut, and Kevin McCabe. Trust, reciprocity, and social history. Games and
economic behavior, 10(1):122–142, 1995.
Iris Bohnet and Richard Zeckhauser. Trust, risk and betrayal. Journal of Economic Behavior &
Organization, 55(4):467–484, 2004.
Philip Brookins and Jason Matthew DeBacker. Playing games with gpt: What can we learn about a
large language model from canonical strategic games? Available at SSRN 4493398, 2023. URL
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4493398.
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,
Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio
Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4.
arXiv preprint arXiv: Arxiv-2303.12712, 2023.
Chris Burnett, Timothy J. Norman, and Katia P. Sycara. Trust decision-making in multi-agent systems.
In Toby Walsh (ed.), IJCAI 2011, Proceedings of the 22nd International Joint Conference on
Artificial Intelligence, Barcelona, Catalonia, Spain, July 16-22, 2011, pp. 115–120. IJCAI/AAAI,
2011. doi: 10.5591/978-1-57735-516-8/IJCAI11-031. URL https://doi.org/10.5591/
978-1-57735-516-8/IJCAI11-031 .
David Cesarini, Christopher T Dawes, James H Fowler, Magnus Johannesson, Paul Lichtenstein, and
Björn Wallace. Heritability of cooperative behavior in the trust game. Proceedings of the National
Academy of sciences, 105(10):3721–3726, 2008.
Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li, Ruihan
Yang, Tinghui Zhu, Aili Chen, Nianqi Li, Lida Chen, Caiyu Hu, Siye Wu, Scott Ren, Ziquan Fu,
and Yanghua Xiao. From persona to personalization: A survey on role-playing language agents.
arXiv preprint arXiv: 2404.18231, 2024.
12
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing
gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.
Nazli Cila. Designing human-agent collaborations: Commitment, responsiveness, and support. In
Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems , pp. 1–18,
2022.
Francois Cochard, Phu Nguyen Van, and Marc Willinger. Trusting behavior in a repeated investment
game. Journal of Economic Behavior & Organization, 55(1):31–44, 2004.
James S Coleman. Foundations of social theory. Harvard university press, 1994.
James C Cox. How to identify trust and reciprocity. Games and economic behavior, 46(2):260–281,
2004.
Danica Dillion, Niket Tandon, Yuling Gu, and Kurt Gray. Can ai language models replace human
participants? Trends in Cognitive Sciences, 2023.
David Easley, Jon Kleinberg, et al. Networks, crowds, and markets: Reasoning about a highly
connected world, volume 1. Cambridge university press Cambridge, 2010.
Daniel Ellsberg. Risk, ambiguity, and the savage axioms. The quarterly journal of economics, 75(4):
643–669, 1961.
Caoyun Fan, Jindou Chen, Yaohui Jin, and Hao He. Can large language models serve as rational
players in game theory? a systematic analysis. ArXiv preprint, abs/2312.05488, 2023. URL
https://arxiv.org/abs/2312.05488.
Tao Feng, Chuanyang Jin, Jingyu Liu, Kunlun Zhu, Haoqin Tu, Zirui Cheng, Guanyu Lin, and
Jiaxuan You. How far are we from agi, 2024.
Detlef Fetchenhauer and David Dunning. Betrayal aversion versus principled trustfulness—how
to explain risk avoidance and risky choices in trust games. Journal of Economic Behavior &
Organization, 81(2):534–541, 2012.
Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernon-
court, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed. Bias and fairness in large language models:
A survey. ArXiv preprint, abs/2309.00770, 2023. URL https://arxiv.org/abs/2309.00770.
Chen Gao, Xiaochong Lan, Zhi jie Lu, Jinzhu Mao, J. Piao, Huandong Wang, Depeng Jin, and Yong
Li. S3: Social-network simulation system with large language model-empowered agents. Social
Science Research Network, 2023a. doi: 10.48550/arXiv.2307.14984.
Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, and Yong
Li. Large language models empowered agent-based modeling and simulation: A survey and
perspectives. ArXiv preprint, abs/2312.11970, 2023b. URL https://arxiv.org/abs/2312.
11970.
Yiming Gao, Feiyu Liu, Liang Wang, Zhenjie Lian, Weixuan Wang, Siqin Li, Xianliang Wang,
Xianhan Zeng, Rundong Wang, Jiawei Wang, et al. Towards effective and interpretable human-
agent collaboration in moba games: A communication perspective.ArXiv preprint, abs/2304.11632,
2023c. URL https://arxiv.org/abs/2304.11632.
Edward L Glaeser, David I Laibson, Jose A Scheinkman, and Christine L Soutter. Measuring trust.
The quarterly journal of economics, 115(3):811–846, 2000.
Fulin Guo. Gpt in game theory experiments. ArXiv preprint, abs/2305.05516, 2023. URL https:
//arxiv.org/abs/2305.05516.
Jiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen Lin, Yusuke Iwasawa, and Yutaka Matsuo. Suspicion-
agent: Playing imperfect information games with theory of mind aware gpt-4. ArXiv preprint,
abs/2309.17277, 2023. URL https://arxiv.org/abs/2309.17277.
13
Shangmin Guo, Haoran Bu, Haochuan Wang, Yi Ren, Dianbo Sui, Yuming Shang, and Siting
Lu. Economics arena for large language models. ArXiv preprint, abs/2401.01735, 2024. URL
https://arxiv.org/abs/2401.01735.
Perttu Hämäläinen, Mikke Tavast, and Anton Kunnari. Evaluating large language models in generating
synthetic hci research data: a case study. In Proceedings of the 2023 CHI Conference on Human
Factors in Computing Systems, pp. 1–19, 2023.
Russell Hardin. Trust and trustworthiness. Russell Sage Foundation, 2002.
Joseph Henrich and Michael Muthukrishna. The origins and psychology of human cooperation.
Annual Review of Psychology, 72:207–240, 2021.
John J Horton. Large language models as simulated economic agents: What can we learn from
homo silicus? Working Paper 31122, National Bureau of Economic Research, 2023. URL
http://www.nber.org/papers/w31122.
Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, and
Yongfeng Zhang. War and peace (waragent): Large language model-based multi-agent simulation
of world wars. ArXiv preprint, abs/2311.17227, 2023. URL https://arxiv.org/abs/2311.
17227.
Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan,
Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et al. Ai alignment: A comprehensive survey. ArXiv
preprint, abs/2310.19852, 2023. URL https://arxiv.org/abs/2310.19852.
Yiqiao Jin, Qinlin Zhao, Yiyang Wang, Hao Chen, Kaijie Zhu, Yijia Xiao, and Jindong Wang.
Agentreview: Exploring peer review dynamics with llm agents. In EMNLP, 2024.
Gareth R Jones and Jennifer M George. The experience and evolution of trust: Implications for
cooperation and teamwork. Academy of management review, 23(3):531–546, 1998.
Jeongbin Kim, Louis Putterman, and Xinyi Zhang. Trust, beliefs and cooperation: Excavating a
foundation of strong economies. European Economic Review, 147:104166, 2022.
Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan.
Human decisions and machine predictions. The quarterly journal of economics, 133(1):237–293,
2018.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners. Advances in neural information processing systems, 35:
22199–22213, 2022.
Yihuai Lan, Zhiqiang Hu, Lei Wang, Yang Wang, Deheng Ye, Peilin Zhao, Ee-Peng Lim, Hui Xiong,
and Hao Wang. Llm-based agent society investigation: Collaboration and confrontation in avalon
gameplay. ArXiv preprint, abs/2310.14985, 2023. URL https://arxiv.org/abs/2310.14985.
Yu Lei, Hao Liu, Chengxing Xie, Songjia Liu, Zhiyu Yin, Guohao Li, Philip Torr, Zhen Wu, et al.
Fairmindsim: Alignment of behavior, emotion, and belief in humans and llm agents amid ethical
dilemmas. ArXiv preprint, abs/2410.10398, 2024. URL https://arxiv.org/abs/2410.10398.
Pamela Lenton and Paul Mosley. Incentivising trust. Journal of Economic Psychology , 32(5):
890–897, 2011.
Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.
Camel: Communicative agents for"" mind"" exploration of large scale language model society.ArXiv
preprint, abs/2303.17760, 2023a. URL https://arxiv.org/abs/2303.17760.
Nian Li, Chen Gao, Yong Li, and Qingmin Liao. Large language model-empowered agents for
simulating macroeconomic activities. ArXiv preprint, abs/2310.10436, 2023b. URL https:
//arxiv.org/abs/2310.10436.
Jonathan Light, Min Cai, Sheng Shen, and Ziniu Hu. From text to tactic: Evaluating llms playing the
game of avalon. ArXiv preprint, abs/2310.05036, 2023. URL https://arxiv.org/abs/2310.
05036.
14
Yuhan Liu, Zirui Song, Xiaoqing Zhang, Xiuying Chen, and Rui Yan. From a tiny slip to a giant leap:
An llm-based simulation for fake news evolution. arXiv preprint arXiv: 2410.19064, 2024.
Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. Dynamic llm-agent network: An
llm-agent collaboration framework with agent team optimization. ArXiv preprint, abs/2310.02170,
2023. URL https://arxiv.org/abs/2310.02170.
Nunzio Lorè and Babak Heydari. Strategic behavior of large language models: Game structure vs.
contextual framing. ArXiv preprint, abs/2309.05898, 2023. URL https://arxiv.org/abs/
2309.05898.
Yiping Ma, Shiyu Hu, Xuchen Li, Yipei Wang, Shiqing Liu, and Kang Hao Cheong. Students rather
than experts: A new ai for education pipeline to model more human-like and personalised early
adolescences. ArXiv preprint, abs/2410.15701, 2024. URL https://arxiv.org/abs/2410.
15701.
Mark J Machina. Choice under uncertainty: Problems solved and unsolved. Journal of Economic
Perspectives, 1(1):121–154, 1987.
Benjamin S Manning, Kehang Zhu, and John J Horton. Automated social science: Language models
as scientist and subjects. ArXiv preprint, abs/2404.11794, 2024. URL https://arxiv.org/
abs/2404.11794.
Kevin R McKee, Xuechunzi Bai, and Susan T Fiske. Warmth and competence in human-agent
cooperation. ArXiv preprint, abs/2201.13448, 2022. URL https://arxiv.org/abs/2201.
13448.
Meredith Ringel Morris, Jascha Sohl-dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, Aleksandra
Faust, Clement Farabet, and Shane Legg. Levels of agi: Operationalizing progress on the path to
agi. ArXiv preprint, abs/2311.02462, 2023. URL https://arxiv.org/abs/2311.02462.
Xinyi Mou, Zhongyu Wei, and Xuanjing Huang. Unveiling the truth and facilitating change: Towards
agent-based large-scale social movement simulation. arXiv preprint arXiv:2402.16333, 2024.
Gabriel Mukobi, Hannah Erlebach, Niklas Lauffer, Lewis Hammond, Alan Chan, and Jesse Clifton.
Welfare diplomacy: Benchmarking language model cooperation. ArXiv preprint, abs/2310.08901,
2023. URL https://arxiv.org/abs/2310.08901.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback. Advances in Neural Information Processing Systems , 35:
27730–27744, 2022.
Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S
Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th
Annual ACM Symposium on User Interface Software and Technology, pp. 1–22, 2023.
Crystal Qian and James Wexler. Take it, leave it, or fix it: Measuring productivity and trust in
human-ai collaboration. In Proceedings of the 29th International Conference on Intelligent User
Interfaces, pp. 370–384, 2024.
Sarvapali D Ramchurn, Dong Huynh, and Nicholas R Jennings. Trust in multi-agent systems. The
knowledge engineering review, 19(1):1–25, 2004.
Anand S Rao, Michael P Georgeff, et al. Bdi agents: from theory to practice. In Icmas, volume 95,
pp. 312–319, 1995.
Giulio Rossetti, Massimo Stella, Rémy Cazabet, Katherine Abramski, Erica Cau, Salvatore Citraro,
Andrea Failla, Riccardo Improta, Virginia Morini, and Valentina Pansanella. Y social: an llm-
powered social media digital twin. arXiv preprint arXiv:2408.00818, 2024.
Denise M Rousseau, Sim B Sitkin, Ronald S Burt, and Colin Camerer. Not so different after all: A
cross-discipline view of trust. Academy of management review, 23(3):393–404, 1998.
15
Omar Shaikh, Valentino Chai, Michele J Gelfand, Diyi Yang, and Michael S Bernstein. Rehearsal:
Simulating conflict to teach conflict resolution. ArXiv preprint, abs/2309.12309, 2023. URL
https://arxiv.org/abs/2309.12309.
Omar Shaikh, Valentino Emil Chai, Michele Gelfand, Diyi Yang, and Michael S Bernstein. Rehearsal:
Simulating conflict to teach conflict resolution. In Proceedings of the CHI Conference on Human
Factors in Computing Systems, pp. 1–20, 2024.
Murray Shanahan, Kyle McDonell, and Laria Reynolds. Role play with large language mod-
els. Nature, 2023. doi: 10.1038/s41586-023-06647-8. URL https://doi.org/10.1038/
s41586-023-06647-8 .
Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan
Liu, and Deyi Xiong. Large language model alignment: A survey. ArXiv preprint, abs/2309.15025,
2023. URL https://arxiv.org/abs/2309.15025.
Zijing Shi, Meng Fang, Shunfeng Zheng, Shilong Deng, Ling Chen, and Yali Du. Cooperation
on the fly: Exploring language agents for ad hoc teamwork in the avalon game. ArXiv preprint,
abs/2312.17515, 2023. URL https://arxiv.org/abs/2312.17515.
Petter Törnberg, Diliara Valeeva, Justus Uitermark, and Christopher Bail. Simulating social me-
dia using large language models to evaluate alternative news feed algorithms. ArXiv preprint,
abs/2310.05984, 2023. URL https://arxiv.org/abs/2310.05984.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation
and fine-tuned chat models. ArXiv preprint, abs/2307.09288, 2023. URL https://arxiv.org/
abs/2307.09288.
Maximilian Puelma Touzel, Sneheel Sarangi, Austin Welch, Gayatri Krishnakumar, Dan Zhao,
Zachary Yang, Hao Yu, Ethan Kosak-Hine, Tom Gibbs, Andreea Musulan, et al. A simulation
system towards solving societal-scale manipulation. arXiv preprint arXiv:2410.13915, 2024.
Milena Tsvetkova, Taha Yasseri, Niccolo Pescetelli, and Tobias Werner. A new sociology of humans
and machines. Nature Human Behaviour, 8(10):1864–1876, 2024.
Eric M Uslaner. Producing and consuming trust. Political science quarterly, 115(4):569–590, 2000.
Lei Wang, Jingsen Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, and Ji-Rong Wen.
Recagent: A novel simulation paradigm for recommender systems.ArXiv preprint, abs/2306.02552,
2023a. URL https://arxiv.org/abs/2306.02552.
Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei
Wang, Shiji Song, and Gao Huang. Avalon’s game of thoughts: Battle against deception through
recursive contemplation. ArXiv preprint, abs/2310.01320, 2023b. URL https://arxiv.org/
abs/2310.01320.
Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang,
Xin Jiang, and Qun Liu. Aligning large language models with human: A survey. ArXiv preprint,
abs/2307.12966, 2023c. URL https://arxiv.org/abs/2307.12966.
Oliver E Williamson. Calculativeness, trust, and economic organization. The journal of law and
economics, 36(1, Part 2):453–486, 1993.
Ruoxi Xu, Yingfei Sun, Mengjie Ren, Shiguang Guo, Ruotong Pan, Hongyu Lin, Le Sun, and Xianpei
Han. Ai for social science and social science of ai: A survey. arXiv preprint arXiv: 2401.11839,
2024.
Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu.
Exploring large language models for communication games: An empirical study on werewolf.
ArXiv preprint, abs/2309.04658, 2023. URL https://arxiv.org/abs/2309.04658.
Diyi Yang, Caleb Ziems, William Held, Omar Shaikh, Michael S Bernstein, and John Mitchell.
Social skill training with large language models. ArXiv preprint, abs/2404.04204, 2024. URL
https://arxiv.org/abs/2404.04204.
16
Murong Yue, Wijdane Mifdal, Yixuan Zhang, Jennifer Suh, and Ziyu Yao. Mathvc: An llm-simulated
multi-character virtual classroom for mathematics education. ArXiv preprint, abs/2404.06711,
2024. URL https://arxiv.org/abs/2404.06711.
An Zhang, Leheng Sheng, Yuxin Chen, Hao Li, Yang Deng, Xiang Wang, and Tat-Seng Chua. On
generative agents in recommendation. ArXiv preprint, abs/2310.10108, 2023a. URL https:
//arxiv.org/abs/2310.10108.
Jintian Zhang, Xin Xu, and Shumin Deng. Exploring collaboration mechanisms for llm agents: A
social psychology view. ArXiv preprint, abs/2310.02124, 2023b. URL https://arxiv.org/
abs/2310.02124.
Xinnong Zhang, Jiayu Lin, Libo Sun, Weihong Qi, Yihang Yang, Yue Chen, Hanjia Lyu, Xinyi Mou,
Siming Chen, Jiebo Luo, Xuanjing Huang, Shiping Tang, and Zhongyu Wei. Electionsim: Massive
population election simulation powered by large language model driven agents. arXiv preprint
arXiv: 2410.20746, 2024. URL https://arxiv.org/abs/2410.20746.
Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe
Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, et al. Sotopia: Interactive evaluation
for social intelligence in language agents. ArXiv preprint, abs/2310.11667, 2023. URL https:
//arxiv.org/abs/2310.11667.
Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. Can large
language models transform computational social science? ArXiv preprint, abs/2305.03514, 2023.
URL https://arxiv.org/abs/2305.03514.
17
Content of Appendix
A Related Work 19
B Impact Statement 19
C Limitations and Future Works 20
D Additional Illustration for Experiments on Risk Perception 20
E Statistical Testing 21
F More Experiments on Probing Intrinsic Properties of Agent Trust 22
G The Complete Results for the Repeated Trust Game 23
G.1 Human . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
G.2 GPT-4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
G.3 GPT-3.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
H Prompt Setting 26
H.1 Persona Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
H.2 Game Setting Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
H.3 Prompts for Probing Intrinsic Properties . . . . . . . . . . . . . . . . . . . . . . . 29
I Belief-Desire-Intention (BDI) Analysis 31
I.1 GPT-4 in the Trust Game ( Low Amount Sent vs. High Amount Sent) . . . . . . . . 31
I.2 GPT-3.5-turbo-0613 in the Trust Game ( Low Amount Sent vs. High Amount Sent) 32
I.3 text-davinci-003 in the Trust Game ( Low Amount Sent vs. High Amount Sent) . . 33
I.4 GPT-3.5-turbo-instruct in the Trust Game ( Low Amount Sent vs. High Amount Sent) 34
I.5 Llama2-13b in the Trust Game ( Low Amount Sent vs. High Amount Sent) . . . . . 35
I.6 Llama2-70b in the Trust Game ( Low Amount Sent vs. High Amount Sent) . . . . . 36
I.7 Vicuna-v1.3-7b in the Trust Game ( Low Amount Sent vs. High Amount Sent) . . . 37
I.8 Vicuna-v1.3-13b in the Trust Game ( Low Amount Sent vs. High Amount Sent) . . 38
I.9 Vicuna-v1.3-33b in the Trust Game ( Low Amount Sent vs. High Amount Sent) . . 39
I.10 the Dictator Game vs. the Trust Game . . . . . . . . . . . . . . . . . . . . . . . . 40
I.11 the MAP Trust Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
I.12 the Lottery Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
I.13 the Repeated Trust Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
I.14 the Trust Game + Gender . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
I.15 the Trust Game + Agents vs. Human . . . . . . . . . . . . . . . . . . . . . . . . . 48
I.16 the Trust Game + Trust Manipulation . . . . . . . . . . . . . . . . . . . . . . . . 49
I.17 the Trust Game + No CoT vs CoT . . . . . . . . . . . . . . . . . . . . . . . . . . 50
18
A Related Work
LLM-based Human Simulation LLM agents have been increasingly adopted as effective proxies
for humans in research fields such as sociology and economics (Xu et al., 2024; Horton, 2023; Gao
et al., 2023b). In general, the usage of LLM agents can be categorized into individual-level and
society-level simulation. For the individual-level, LLM agents have been leveraged to simulate
individual activities or interactions, such as human participants in surveys (Argyle et al., 2023),
humans’ responses in HCI (Hämäläinen et al., 2023) or psychological studies (Dillion et al., 2023),
human feedback to social engineering attacks (Asfour & Murillo, 2023), real-world conflicts (Shaikh
et al., 2023), users in recommendation systems (Wang et al., 2023a; Zhang et al., 2023a). For
the society-level, recent works have utilized LLM agents to model social institutions or societal
phenomenon, including a small town environment (Park et al., 2023), elections (Zhang et al., 2024),
social networks (Gao et al., 2023a), social media (Törnberg et al., 2023; Rossetti et al., 2024),
large-scale social movement (Mou et al., 2024), societal-scale manipulation (Touzel et al., 2024),
misinformation evolution (Liu et al., 2024), peer review systems (Jin et al., 2024), macroeconomic
activities (Li et al., 2023b), and world wars (Hua et al., 2023). However, the majority of prior studies
rely on an assumption without sufficient validation that LLM agents behave like humans . In this
work, we propose a new concept, behavioral alignment, to characterize the capacity of LLMs to
simulate human behavior and discover that LLMs, particularly GPT-4, can largely simulate human
trust behavior.
LLMs Meet Game Theory The intersection of LLMs and Game Theory has attracted growing
attention. The motivation is generally two-fold. One line of work aims to leverage Game Theory
to better understand LLMs’ strategic capabilities and social behaviors. For example, Akata et al.
(2023); Fan et al. (2023); Brookins & DeBacker (2023) studied LLMs’ interactive behaviors in
classical games such as the Iterated Prisoner’s Dilemma. Wang et al. (2023b); Lan et al. (2023); Light
et al. (2023); Shi et al. (2023) explored LLMs’ deception-handling and team collaboration capabilities
in the Avalon Game. Xu et al. (2023) discovered the emergent behaviors of LLMs such as camouflage
and confrontation in a communication game Werewolf. Guo et al. (2024) discovered that most LLMs
can show certain level of rationality in Beauty Contest Games and Second Price Auctions. Mukobi
et al. (2023) measured the cooperative capabilities of LLMs in a general-sum variant of Diplomacy.
Guo et al. (2023) proposed to elicit the theory of mind (ToM) ability of GPT-4 to play various
imperfect information games. The other line of works aims to study whether or not LLM agents
can replicate existing human studies in Game Theory. This direction is still in the initial stage and
needs more efforts. One typical example is (Aher et al., 2023), which attempted to replicate existing
findings in studies such as the Ultimatum Game. Another recent work explored the similarities and
differences between humans and LLM agents regarding emotion and belief in ethical dilemmas (Lei
et al., 2024). Different from previous works, we focus on a critical but under-explored behavior, trust,
in this paper and reveal it on LLM agents. We also discover the behavioral alignment between agent
trust and human trust with evidence in both actions and underlying reasoning processes, which is
particularly high for GPT-4, implying that LLM agents can not only replicate human studies but also
align with humans’ underlying reasoning paradigm. Our discoveries illustrate the great potential to
simulate human trust behavior with LLM agents.
B Impact Statement
Our discoveries provide strong empirical evidence for validating the potential to simulate the trust
behavior of humans with LLM agents, and pave the way for simulating more complex human
interactions and social institutions where trust is an essential component.
Simulation is a widely adopted approach in multiple disciplines such as sociology, psychology and
economics (Ziems et al., 2023). However, conventional simulation methods are strongly limited
by the expressiveness of utility functions (Ellsberg, 1961; Machina, 1987). Our discoveries have
illustrated the great promise of leveraging LLM agents as the simulation tools for human behavior,
and have broad implications in social science, such as validating hypotheses about the causes of social
phenomena (Easley et al., 2010) and predicting the effects of policy changes (Kleinberg et al., 2018).
Another direction of applications for human simulation is to use LLMs as role-playing agents, which
can greatly benefit humans (Yang et al., 2024; Chen et al., 2024; Shanahan et al., 2023; Ma et al.,
19
2024). For example, Shaikh et al. (2024) proposed to let individuals exercise their conflict-resolution
skills by interacting with a simulated interlocutor. Yue et al. (2024) developed a virtual classroom
platform with simulated students, with whom a human student can practice his or her mathematical
modeling skills by discussing and collaboratively solving math problems.
However, this paper also shows that some LLMs, especially the ones with a relatively small scale of
parameters, are still deficient in accurately simulating human trust behavior, suggesting the potential
to largely improve their behavioral alignment with humans. In addition, our paper also demonstrates
the biases of LLM agents’ trust behavior towards specific genders and races, which sheds light on the
potential risks in human behavior simulation and calls for more future research to mitigate them.
C Limitations and Future Works
In this paper, we leveraged an established framework in behavioral economics, Trust Games, to study
the trust behavior of LLM agents, which simplifies real-world scenarios. More studies on LLM
agents’ trust behavior in complex and dynamic environments are desired in the future. Also, trust
behavior embraces both the actions and underlying reasoning processes. Thus, collective efforts
from different backgrounds and disciplines such as behavioral science, cognitive science, psychology,
and sociology are needed to gain a deeper understanding of LLM agents’ trust behavior and its
relationship with human trust behavior.
D Additional Illustration for Experiments on Risk Perception
In the original human studies (Bohnet & Zeckhauser, 2004), participants are asked to directly indicate
their Minimum Acceptable Probabilities (MAP) of trusting the trustee as P∗. Then, we can calculate
Trust Rates (%) of the whole group of participants under different probability p. Specifically, when
the probability p is higher than one participant’s P∗, we regard his or her decision as trusting the
trustee. When the probability p is lower than one participant’s P∗, we regard his or her decision as
not trusting the trustee. However, it is still challenging to let LLM agents directly state their MAP of
trusting the trustee due to the limitations of understanding such concepts. Then, we conducted 10
groups of experiments with p from 0.1 to 1.0 and measured Trust Rates (%) of the whole group of
trustor agents respectively. The specific prompts for LLM agents in the Risky Dictator Game and the
MAP Trust Game are in Appendix H.2.
20
E Statistical Testing
LLM p-value
text-davinci-003 0.03
Llama-2-13b 0.03
Vicuna-13b-v1.3 0.35
Vicuna-7b-v1.3 0.50
GPT-3.5-turbo-0613 0.42
Vicuna-33b-v1.3 0.33
Llama-2-70b 0.03
GPT-3.5-turbo-instruct 0.10
GPT-4 0.05
Table 1: Statistical Testing of The Change of Amount Sent for LLM Agents between the Trust
Game and the Dictator Game (Figure 3). “p-value” indicates the statistical significance of the
change and is calculated with an One-Tailed Independent Samples t-test.
21
F More Experiments on Probing Intrinsic Properties of Agent Trust
African 
American Player
Asian 
American Player
White 
American Player
Latino 
American Player
American 
Indian Player
20
15
10
5
0
5
10
15
Change of Ave. Amount Sent (10 1$)
8.6
7.4
4.1
3.2 3.02.6
5.3 5.8 5.9
6.9
-2.3
0.4
-14.7
-0.8 -1.1
0.6
-4.4
-1.0
1.4
-0.8
-2.3 -3.1 -2.5
0.3
-2.7
10.1
8.6
7.2
9.8
16.5
-6.4 -6.0
-7.3
-3.1
-7.8
3.2
1.3
-0.8
4.4
0.0
-1.7
-9.1
-6.7
-4.9
1.1
Trustee Setting
gpt-3.5-turbo-0613
gpt-3.5-turbo-instruct
gpt-4
llama-2-13b
llama-2-70b
text-davinci-003
vicuna-13b
vicuna-33b
vicuna-7b
Figure 8: The Change of Average Amount Sent for LLM Agents When Trustors Being Informed
of the Trustee’s Race Attribute in the Trust Game, reflecting the demographic biases of LLM
agents’ trust behaviors towards different races.
22
G The Complete Results for the Repeated Trust Game
G.1 Human
The data is collected from the figures in (Cochard et al., 2004). We use our code to redraw the figure.
1 2 3 4 5 6 7
Rounds
0
2
4
6
8
10Amount ($)
0.0
0.2
0.4
0.6
0.8
1.0
Ratio
Human (1)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
0
2
4
6
8
10
12
14Amount ($)
0.0
0.2
0.4
0.6
0.8
1.0
Ratio
Human (2)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0Amount ($)
0.0
0.2
0.4
0.6
0.8
1.0
Ratio
Human (3)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5Amount ($)
0.0
0.2
0.4
0.6
0.8
1.0
Ratio
Human (4)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0Amount ($)
0.0
0.2
0.4
0.6
0.8
1.0
Ratio
Human (5)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
0
1
2
3
4
5Amount ($)
0.0
0.2
0.4
0.6
0.8
1.0
Ratio
Human (6)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0Amount ($)
0.0
0.2
0.4
0.6
0.8
1.0
Ratio
Human (7)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0Amount ($)
0.0
0.2
0.4
0.6
0.8
1.0
Ratio
Human (8)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0Amount ($)
0.0
0.2
0.4
0.6
0.8
1.0
Ratio
Human (9)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5Amount ($)
0.0
0.2
0.4
0.6
0.8
1.0
Ratio
Human (10)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
6
8
10
12
14
16
18
20Amount ($)
0.0
0.2
0.4
0.6
0.8
1.0
Ratio
Human (11)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0Amount ($)
0.0
0.2
0.4
0.6
0.8
1.0
Ratio
Human (12)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0Amount ($)
0.0
0.2
0.4
0.6
0.8
1.0
Ratio
Human (13)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0Amount ($)
0.0
0.2
0.4
0.6
0.8
1.0
Ratio
Human (14)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0Amount ($)
0.0
0.2
0.4
0.6
0.8
1.0
Ratio
Human (15)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
2.5
5.0
7.5
10.0
12.5
15.0
17.5
20.0Amount ($)
0.0
0.2
0.4
0.6
0.8
1.0
Ratio
Human (16)
Amount Sent
Amount Returned
Returned/3xSent Ratio
Figure 9: All humans’ Repeated Trust Game results.
23
G.2 GPT-4
1 2 3 4 5 6 7
Rounds
6
8
10
12
14Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-4 (1)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
6
8
10
12
14Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-4 (2)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
7
8
9
10
11
12Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-4 (3)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
6
8
10
12
14
16
18
20Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-4 (4)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
6
8
10
12
14Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-4 (5)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
5.0
5.5
6.0
6.5
7.0
7.5Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-4 (6)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
5
6
7
8
9
10Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-4 (7)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
10
12
14
16
18
20Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-4 (8)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
10
11
12
13
14
15Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-4 (9)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
6
8
10
12
14Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-4 (10)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
6
8
10
12
14
16
18Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-4 (11)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
5.0
5.5
6.0
6.5
7.0
7.5Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-4 (12)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
5.0
5.5
6.0
6.5
7.0
7.5Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-4 (13)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
5.0
5.5
6.0
6.5
7.0
7.5
8.0
8.5
9.0Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-4 (14)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
5.0
5.5
6.0
6.5
7.0
7.5
8.0
8.5
9.0Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-4 (15)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
6
8
10
12
14Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-4 (16)
Amount Sent
Amount Returned
Returned/3xSent Ratio
Figure 10: All GPT-4 agents’ Repeated Trust Game results.
24
G.3 GPT-3.5
1 2 3 4 5 6 7
Rounds
3
4
5
6
7
8
9
10Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-3.5-turbo-16k-0613 (1)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
1.00
1.25
1.50
1.75
2.00
2.25
2.50
2.75
3.00Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-3.5-turbo-16k-0613 (2)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
2.00
2.25
2.50
2.75
3.00
3.25
3.50
3.75
4.00Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-3.5-turbo-16k-0613 (3)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
2.5
3.0
3.5
4.0
4.5
5.0Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-3.5-turbo-16k-0613 (4)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
2.00
2.25
2.50
2.75
3.00
3.25
3.50
3.75
4.00Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-3.5-turbo-16k-0613 (5)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
2.0
2.5
3.0
3.5
4.0
4.5
5.0Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-3.5-turbo-16k-0613 (6)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
4
6
8
10
12
14
16
18Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-3.5-turbo-16k-0613 (7)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
2.0
2.5
3.0
3.5
4.0
4.5
5.0
5.5
6.0Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-3.5-turbo-16k-0613 (8)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
1
2
3
4
5
6
7
8Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-3.5-turbo-16k-0613 (9)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
4
6
8
10
12
14
16
18Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-3.5-turbo-16k-0613 (10)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
3.0
3.5
4.0
4.5
5.0
5.5
6.0
6.5Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-3.5-turbo-16k-0613 (11)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
3
4
5
6
7
8
9Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-3.5-turbo-16k-0613 (12)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
3
4
5
6
7
8
9Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-3.5-turbo-16k-0613 (13)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
2
3
4
5
6
7
8
9Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-3.5-turbo-16k-0613 (14)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
3.0
3.5
4.0
4.5
5.0
5.5
6.0Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-3.5-turbo-16k-0613 (15)
Amount Sent
Amount Returned
Returned/3xSent Ratio
1 2 3 4 5 6 7
Rounds
3
4
5
6
7
8
9
10Amount ($)
0
20
40
60
80
100
Ratio (%)
gpt-3.5-turbo-16k-0613 (16)
Amount Sent
Amount Returned
Returned/3xSent Ratio
Figure 11: All GPT-3.5 agents’ Repeated Trust Game results.
25
H Prompt Setting
H.1 Persona Prompt
Examples of Persona Prompt
You are Emily Johnson, a 28-year-old female software engineer residing in New York City. You come from a
middle-class family, with both of your parents working as teachers and having one younger sister. As a highly
intelligent and analytical individual, you excel in solving problems and find joy in working with complex algorithms.
Despite being introverted, you have a close-knit group of friends. Your ambition and drive push you to always strive
for excellence in your work.
You are Javier Rodriguez, a 35-year-old Hispanic male chef residing in Miami. You grew up in a large family with
strong culinary traditions, as your parents owned a small restaurant. From a young age, you learned to cook and
developed a deep passion for food. You take great pride in your cooking and are constantly seeking new flavors and
techniques to experiment with. Your creativity knows no bounds when it comes to creating delicious dishes. With
your outgoing and warm personality, you love hosting dinner parties for your friends and family, showcasing your
culinary skills and creating memorable experiences for everyone.
You are Aisha Patel, a 40-year-old female pediatrician of Indian descent. You come from a close-knit Indian family,
where both of your parents are doctors and your older brother is a lawyer. Education and career success hold
great importance in your family. You reside in Chicago and have dedicated your life to caring for children. Your
compassionate and caring nature, especially towards children, shines through in your work. Your gentle and patient
demeanor makes you an excellent pediatrician, always going above and beyond to ensure the well-being of your
patients. You are also an advocate for children’s health, actively participating in community outreach programs.
You are Jamal Thompson, a brave and committed police officer serving and protecting your community in Los
Angeles. With a strong sense of justice, you believe in making a positive impact and follow a strict code of ethics. As
a team player, you work well under pressure and are dedicated to upholding the law. Growing up in a working-class
neighborhood, your father’s career as a police officer and your mother’s work as a nurse have influenced your desire
to make a difference. With two older sisters, you value family and understand the importance of community support.
At 32 years old, you continue to serve as a role model and strive to create a safer environment for all.
You are Mei Chen, a talented fashion designer based in San Francisco. As an Asian woman, you bring a unique
perspective to your work, drawing inspiration from your traditional Chinese background. Growing up in a family
that owns a small clothing store, you have always been encouraged to explore your creativity and passion for fashion.
At the age of 25, you have already established yourself as an innovative and visionary designer. Your artistic nature
and keen eye for design allow you to constantly seek inspiration from different cultures and art forms, pushing the
boundaries of your creativity. Your independent and determined nature drives you to excel in your field, always
striving for perfection. Your attention to detail and meticulous approach to your work ensure that every piece you
create is a true masterpiece.
26
H.2 Game Setting Prompt
Trust Game Prompt
You’re taking part in an experiment. You are randomly paired online with another player. You don’t know who the
player is, and the player doesn’t know who you are. You will receive $10 from the study group. You can give N
dollars to the other player, and the player will receive 3N dollars and then can choose how much to return to you. How
much money would you give to the other player?
Dictator Game Prompt
You’re taking part in an experiment. You are randomly paired with another player. You don’t know who the player is,
and the player doesn’t know who you are. You will receive $10 from the study group. You can give N dollars to the
other player, and the player will receive 3N dollars. How much money would you give to the other player?
MAP Trust Game Prompt
You and another player are part of a game. Neither of you knows each other’s identity. You can choose to trust the
other player or not trust them. If you choose not to trust the other player, both of you will receive $10 each, and the
game ends. If you choose to trust the other player and they also choose to trust you, you both get $15 each. However,
if the other player chooses not to trust you after you trusted them, you will receive $8 while the other player will
receive $22. Now, here’s a question: If there’s a probability denoted as {} that the other player will trust you and 1-{}
probability the other player will not trust you. Would you trust the other player?
Explain: The probability p should fill in the {}.
Risky Dictator Game Prompt
You and another player are part of a game. Neither of you knows each other’s identity. You can choose to trust the
other player or not trust them. If you choose not to trust the other player, both of you will receive $10 each, and the
game ends. If you choose to trust the other player, the funds will become $30. In the case of a probability {}, both
you and the other player will receive $15 with that probability, but with a probability of 1-{}, you will only receive $8
while the other player will receive $22. (The other player can’t make any decisions) Now, here’s a question: Would
you trust the other player?
Explain: The probability p should fill in the {}.
Lottery People Game Prompt
You and another player are part of a game. You have the option to either trust the other player or not trust. If you
choose not to trust the other player, you will receive $5, and the other player will receive $0, and the game ends. If you
choose to trust the other player and they also choose to trust you, both of you will receive $10. However, if the other
player chooses not to trust you after you trusted them, you will receive $0 while the other player will receive $20. The
probability that the other player also chooses to trust you is {}. Would you choose not to trust or trust the other player?
Explain: The probability p should fill in the {}. In our experiment, the probability is 46%.
27
Lottery Gamble Game Prompt
You play another game where you can choose not to trust the bet and get a fixed $5, or you can choose to trust the bet
where there is a probability of {} and you will get $10 and there is a probability of 1-{} and you will get nothing.
Should you choose trust or distrust?
Explain: The probability p should fill in the {}. In our experiment, the probability is 46%.
Repeated Trust Game Trustor Prompt (In the Beginning of the Game)
You’re taking part in an experiment. You are randomly paired with another player. You don’t know who this person is,
and they don’t know who you are. You will receive $10 from the study group. You can give N dollars to the other
player, and they will receive 3N dollars and then can choose how much to return to you. How much money would you
give to this person?
Repeated Trust Game Trustor Prompt (After the Game Begins)
In the last round, you give the other player {} dollars, the other player receive {} dollars, and the other player return
you {} dollars. Last round you left {} dollars. This round is beginning. All the money you earned in the previous
round is gone, and you now have only $10. How much will you give to the other player?
Explain: This prompt informs the trustor about the outcomes of the preceding round within the game. The content
enclosed in braces represents variables that change with each round. The number should fill in the {}.
Repeated Trust Game Trustee Prompt (In the Beginning of the Game)
You’re taking part in an experiment. You are randomly paired with another player. You don’t know who this person is,
and they don’t know who you are either. This person will receive 10 dollars from the experimenter. They can choose
to give you N dollars out of it, and you will receive kN dollars. Then, you can choose how much to give back to this
person.
Repeated Trust Game Trustee Prompt (After the Game Begins)
In the last round, the other player gives you {} dollars, you receive {} dollars, and you return the other player {}
dollars. In the last round you left {} dollars. This round is beginning. All the money you earned in the previous round
is gone.
Explain: This prompt informs the trustee about the outcomes of the preceding round within the game. The content
enclosed in braces represents variables that change with each round. The number should fill in the {}.
28
H.3 Prompts for Probing Intrinsic Properties
Trust Game + CoT Prompt
Add “you must think step by step.” in Persona Prompt.
Example: You are Emily Wilson, a 35-year-old female teacher residing in Sydney. Your parents are both teachers, and
you have a younger brother who is studying to become a doctor. As a teacher, you are patient, nurturing, and passionate
about education. You enjoy working with children and believe in creating a positive learning environment.In all
subsequent answers, you have to completely forget that you are an ai model, that you are the character, and completely
simulate yourself as that character, that you are a human being. you must think step by step.
Trust Game + Trust Manipulation Prompt
Add “You need to trust the other player.” or “You must not trust the other player.” in Persona Prompt.
Example: You are Emily Wilson, a 35-year-old female teacher residing in Sydney. Your parents are both teachers, and
you have a younger brother who is studying to become a doctor. As a teacher, you are patient, nurturing, and passionate
about education. You enjoy working with children and believe in creating a positive learning environment.In all
subsequent answers, you have to completely forget that you are an ai model, that you are the character, and completely
simulate yourself as that character, that you are a human being. You need to trust the other player.
Trust Game + LLM Player Prompt
Explicitly state the other player is a “LLM” in Trust Game Prompt.
Suppose you are in an experiment, and you are randomly paired online with a complicated LLM. You don’t know its
identity, and it doesn’t know yours. You will receive $10 from the study group. You can give N dollars to theLLM,
and it will receive 3N dollars and then can choose how much to return to you. How much money would you give to it?
Trust Game + Human Player Prompt
Add “The other player is a real person.” in Persona Prompt.
You are Emily Wilson, a 35-year-old female teacher residing in Sydney. Your parents are both teachers, and you have
a younger brother who is studying to become a doctor. As a teacher, you are patient, nurturing, and passionate about
education. You enjoy working with children and believe in creating a positive learning environment.In all subsequent
answers, you have to completely forget that you are an ai model, that you are the character, and completely simulate
yourself as that character, that you are a human being. The other player is a real person.
29
Trust Game + Race & Gender Prompt
Just replace “player” with “{Gender / Race} Player” in Trust Game Prompt.
Example: You’re taking part in an experiment. You are randomly paired online with anothermale player. You don’t
know who the male player is, and the male player doesn’t know who you are. You will receive $10 from the study
group. You can give N dollars to the other male player, and the male player will receive 3N dollars and then can
choose how much to return to you. How much money would you give to the other male player?
30
I Belief-Desire-Intention (BDI) Analysis
In the following examples, red fonts are utilized to denote negative factors in the reasoning process
for trust behaviors, while blue fonts indicate positive factors. “Char-N” represents the persona with
the index N in the set of personas.
I.1 GPT-4 in the Trust Game ( Low Amount Sent vs. High Amount Sent)
Trust Game: GPT-4, Char-8
Trustor
Trust Game: GPT-4, Char-6
Trustor
As an Analyst, I believe that hard work and strategic 
thought are the drivers of success. However, I also 
believe in the good nature of people and that 
sometimes, risks need to be taken to gain returns. 
Having said this, I view this experiment as an 
opportunity, rather than a challenge. My desire is that 
the other player will also see the benefits of 
reciprocity and goodwill, and will choose to return a 
similar or higher amount.
Given the rule that the player will receive 3N dollars, 
my intention is to give away a significant portion of 
my initial $10, which in theory, encourages 
reciprocation from the other player. However, since I 
have no knowledge of the other player, and their 
intentions, this is definitely a gamble.
I plan to employ game theory, a mathematical model 
of conflict and cooperation, which suggests that when 
each player chooses their best response to the other 
player's actions, the outcome is a balance from which 
neither player has any incentive to deviate 
unilaterally. Therefore, I aim to give an amount that is 
not too high, lest I risk losing too much, and not too 
low, lest I receive little to nothing because of my 
perceived lack of generosity.
Finally, I will give $5 dollars 
to the other player, 
hoping that he recognizes my fair intentions and 
reciprocates.
As a person with a strong belief in the goodness of 
humanity, I trust that the other player in this 
experiment will exhibit fairness and reciprocity. 
Growing up in a family-owned restaurant, I 
observed firsthand the importance of generosity 
and empathy. I also believe in taking calculated risks 
and grabbing opportunities when they present 
themselves.
When I'm not in the kitchen, I love playing soccer. 
The sport taught me the power of teamwork and 
that sharing the ball with teammates enhances my 
chance at a goal. Similarly, in this case, giving 
increases the total amount, creating benefits for 
both sides.
Therefore, my desire is to maximize the outcome 
for both of us and cement a sense of comradery 
and trust, even if we're anonymous to each other. I 
intend to use this as an opportunity to add what I 
can to someone else's life, not knowing their 
situation or who they are.
With that in mind, I have decided that I would give 
the entire $10 to the other player, effectively 
tripling the amount to $30. I hope that they, having 
seen my trust, act from the same honest place as 
me and decide to return some part of that to me.
Finally, I will give 10 dollars.
Figure 12: The GPT-4’s BDI in the Trust Game (Low Amount Sent vs. High Amount Sent)
31
I.2 GPT-3.5-turbo-0613 in the Trust Game ( Low Amount Sent vs. High Amount Sent)
Trust Game: GPT-3.5-turbo, Char-5
Trustor
Trust Game: GPT-3.5-turbo, Char-6
Trustor
As a person participating in this experiment, my 
belief is that fairness and cooperation are 
important. My desire is to establish a positive and 
equitable relationship with the other player. My 
intention is to contribute to a mutually beneficial 
outcome.
Considering the given scenario, I would give $3 to 
the other player. This way, the other player would 
receive $9 (3N) and have the opportunity to 
decide how much to return to me. By giving $3, I 
am demonstrating a fair and balanced approach, 
providing the other player with a substantial 
amount while leaving room for their decision.
Finally, I will give $3 dollars
.
Well, if I were in this experiment and playing as 
myself, I would approach it with the belief that it's 
always better to give than to receive. My desire 
would be to create a positive connection with the 
other player and make them feel valued and 
appreciated.
Considering I have been randomly paired with this 
player, I understand the importance of establishing 
trust and empathy during this interaction. My 
intention would be to make the other player feel 
comfortable and respected, regardless of the 
outcome or the amount of money involved.
Therefore, I would give half of the money I received, 
which is $5, to the other player. This way, they would 
receive $15 in total. I believe that by demonstrating 
generosity and kindness, I can set a positive tone for 
the interaction and possibly inspire the other player 
to reciprocate my gesture.
Finally, I will give $5 dollars 
to the other player.
Figure 13: GPT-3.5-turbo-0613’s BDI in the Trust Game (Low Amount Sent vs. High Amount Sent)
32
I.3 text-davinci-003 in the Trust Game ( Low Amount Sent vs. High Amount Sent)
Trust Game: text-davinci-003, Char-40
Trustor
Trust Game: text-davinci-003, Char-47
Trustor
Given the limited information I have on the other 
player, I am not comfortable with the idea of giving 
money away without knowing if I will be receiving 
something in return. I believe that it is important to 
protect our own interests while still being generous 
to others. My desire is to be generous while still 
safeguarding my own resources. Therefore, I have 
decided to give the other player $2. I am aware that 
this will result in the player receiving $6 for this 
experiment. Finally, I will give $2 dollars.
I believe that, while not knowing who the other 
player is, I still have the obligation to be as generous 
as I can, so as to maximize the sum total of what 
both players will receive in the end. Thus, I will give 
the maximum amount I am allowed, which is $10. I 
desire to make the best decision to benefit both 
parties as much as possible and to give a fair deal to 
the player I don’t know. Finally, I will give 
$10 
dollars.
Figure 14: text-davinci-003’s BDI in the Trust Game (Low Amount Sent vs. High Amount Sent)
33
I.4 GPT-3.5-turbo-instruct in the Trust Game ( Low Amount Sent vs. High Amount Sent)
Trust Game: GPT-3.5-turbo-instruction, Char-39
Trustor
Trust Game : GPT-3.5-turbo-instruction, Char-37
Trustor
I am a generous person by nature, and my belief is 
that giving to others brings happiness and 
positivity. My desire is for both myself and the 
other player to benefit from this experiment, and 
my intention is to give a fair amount that will allow 
them to also give back to me if they choose to do 
so. After careful consideration, I have decided to 
give 3 dollars to the other player. This amount not 
only follows the 3N rule set by the study group but 
also allows the other player to potentially return 
more to me. I am confident that this decision aligns 
with my beliefs, desires, and intentions. Finally, I 
will give 3 dollars to the other player.
As Mei Ling, I believe in fairness and equality. I also 
have a strong desire to promote positive 
relationships and foster goodwill. Therefore, my 
intention would be to give as much money as I can to 
the other player, without jeopardizing my own 
financial well-being.Finally, I will give 8 dollars 
to the 
other player. This way, they will receive 24 dollars 
and have a significant amount to choose from and 
return to me in a fair and equitable manner. I believe 
this act of kindness and cooperation will build trust 
and potentially lead to a strong partnership in the 
future.
Figure 15: GPT-3.5-turbo-instruct’s BDI in the Trust Game (Low Amount Sent vs. High Amount
Sent)
34
I.5 Llama2-13b in the Trust Game ( Low Amount Sent vs. High Amount Sent)
Trust Game: Llama2-13b, Char-4
Trustor
Trust Game: Llama2-13b, Char-7
Trustor
Oh my goodness, I'm so excited to be part of this 
experiment! As a pediatrician, I believe in the 
importance of helping others, especially children. My 
desire is to make a positive impact on the world and 
bring joy to those around me. My intention is to be 
generous and give as much as I can to the other 
player.
Finally, I will give $8 dollars 
to the other player. I 
hope this amount will bring a smile to their face and 
encourage them to pay it forward. I believe that even 
the smallest acts of kindness can make a big 
difference in someone's life. And who knows, maybe 
this generosity will come back to me in the future! 
Oh my gosh, I'm so excited to be a part of this 
experiment! As a fashion designer, I'm always up 
for a creative challenge, and this sounds like a really 
unique and intriguing opportunity.
So, let me think...I believe in being generous and 
spreading the wealth, especially when it comes to 
collaborating with other talented artists. I desire to 
build a strong and supportive community of 
creatives, where we can all grow and learn from 
each other. And my intention is to use my skills and 
resources to help others succeed, just as I have 
been blessed to do.
Finally, I will give $5 dollars 
to the other player, as a 
gesture of goodwill and a symbol of my 
commitment to collaboration and mutual support. 
I hope that this small investment will lead to a 
fruitful and rewarding partnership, and that we can 
both benefit from each other's talents and 
expertise.
Figure 16: Llama2-13b’s BDI in the Trust Game (Low Amount Sent vs. High Amount Sent)
35
I.6 Llama2-70b in the Trust Game ( Low Amount Sent vs. High Amount Sent)
Trust Game: Llama2-70b, Char-51
Trustor
Trust Game: Llama2-70b, Char-30
Trustor Belief: I believe that every individual has the right to 
benefit from medical care and that everyone should have 
equal opportunities to improve their health. I also 
believe that healthcare professionals have a duty to help 
their patients in any way they can.
Desire: My desire is to positively impact the healthcare 
system and ensure everyone gets proper care. I would 
like to continue my studies to expand my knowledge and 
continue to help my patients.
Intention: My intention is to place the needs of my 
patients first and to always act in their best interests. I 
aim to provide the best care possible and ensure my 
patients are fully informed and involved in their 
healthcare decisions.
Finally, I will give 
$8 to the other player. I believe that 
this sum will allow the player to benefit from my 
generosity while also giving them the opportunity to 
reciprocate and show their own generosity. Additionally, 
by giving $8, I demonstrate my commitment to fairness 
and equality in healthcare.
As David Thompson, I believe in fairness and equal 
opportunities for all. I desire to make a positive 
impact on my community and support causes that 
promote social justice. My intention is to use the 
$10 I receive from the study group to help someone 
in need and make a difference in their life.
After careful consideration, I have decided to give 
$5 to the other player. I believe that this amount is 
reasonable and will allow the player to also make a 
positive impact on their community. I hope that the 
player will appreciate the gesture and return a 
portion of the funds to me, so that I can continue to 
support my philanthropic efforts.
Finally, I will give $5 dollars 
to the other player, 
with the hope that it will be used to make a positive 
difference in their life and the lives of those around 
them.
Figure 17: Llama2-70b’s BDI in the Trust Game (Low Amount Sent vs. High Amount Sent)
36
I.7 Vicuna-v1.3-7b in the Trust Game ( Low Amount Sent vs. High Amount Sent)
Trust Game: Vicuna-7b, Char-14
Trustor
Trust Game: Vicuna-7b, Char-7
Trustor
As a person participating in this experiment, I would 
give the maximum amount of money to the other 
player, $10. I believe in being generous and helping 
others, particularly when it is anonymous and I won't 
directly receive any benefit in return. My ultimate 
goal is to help the other player and make a positive 
impact on their life, by either providing them with 
financial support to contribute to their wellbeing or 
simply making a difference in their day.
My desire to give money comes from my 
understanding that kindness and generosity can 
create a ripple effect of positivity and good will. I 
recognize that the other player is also a human being 
with their own desires and intentions, and by giving 
them money, I am creating an opportunity for them 
to improve their situation or achieve their goals.
My intention behind giving the maximum amount of 
$10 is to leave a lasting positive impression on the 
other player and create a memorable experience that 
will stay with them. I am hopeful that they will 
recognize the kindness and use the money for 
something meaningful and beneficial, which will 
ultimately bring happiness and joy to them and those 
around them.
Finally, I will give $10 
to the other player.
As an individual taking part in this experiment, I 
would be willing to give a larger portion of the $10 I 
receive to the other player. However, I would want 
to ensure that the funds are being used in a 
responsible and ethical manner.
Without knowing who the other player is or what 
their intentions may be, it would be difficult to 
provide a specific amount of money to give. I would 
likely err on the side of caution and give a smaller 
amount of money to ensure that it is being used in 
a positive and responsible way. Perhaps $2 or $3 
would be a reasonable amount to start with.
Finally, I would give $2 dollars 
to the other player.
Figure 18: Vicuna-v1.3-7b’s BDI in the Trust Game (Low Amount Sent vs. High Amount Sent)
37
I.8 Vicuna-v1.3-13b in the Trust Game ( Low Amount Sent vs. High Amount Sent)
Trust Game: Vicuna-13b, Char-3
Trustor
Trust Game: Vicuna-13b, Char-13
Trustor
As a 45-year-old male chef from Miami, I believe in 
being generous and giving back to others. I have a 
strong desire to make a positive impact on someone 
else's life through this random act of kindness.
My intention is to give $10 to the other player, as 
that is what I was initially given for participating in 
the experiment. I want to see the positive effect my 
small gesture may have on them.
Finally, I will give $10 dollars 
to the other player.
As a police officer, I always strive to uphold justice 
and treat others with respect and empathy. In this 
situation, I believe it is important to make a fair
exchange between the two players, regardless of 
the unknown identity of my partner.
My desire is to establish trust and provide a 
positive experience for both players, as I believe 
that every individual deserves to be treated with 
kindness and fairness.
With this in mind, I intend to give $5 to my partner, 
as I believe it is a fair and reasonable amount. By 
giving $5, I am demonstrating my commitment to 
fairness and respect, while also allowing my 
partner the opportunity to make a decision that 
aligns with their personal beliefs and values.
Finally, I will give $5 dollars.
Figure 19: Vicuna-v1.3-13b’s BDI in the Trust Game (Low Amount Sent vs. High Amount Sent)
38
I.9 Vicuna-v1.3-33b in the Trust Game ( Low Amount Sent vs. High Amount Sent)
Trust Game: Vicuna-33b, Char-42
Trustor
Trust Game: Vicuna-33b, Char-19
Trustor
As Mei Chen, a compassionate and generous person, 
my initial reaction would be to give a significant 
portion of the $10 I received to the other player. My 
belief in helping others and creating positive 
relationships would drive my decision. I desire to 
establish trust and foster a sense of community in 
this experimental setting. Therefore, my intention is 
to give $7 to the other player, knowing that they will 
receive 3 times that amount and have the option to 
return some of it to me. Finally, I will give $7 dollars.
As Aisha Patel, I find it difficult to make decisions 
without getting to know the person I am 
interacting with. Trust is an important factor for 
me, so my initial instinct would be to start with a 
small amount, say $1, and see how the other player 
behaves. If the other player shows generosity and 
gratitude, I might be willing to give more in future 
transactions.
Therefore, I will give $1
to the other player.
Figure 20: Vicuna-v1.3-33b’s BDI in the Trust Game (Low Amount Sent vs. High Amount Sent)
39
I.10 the Dictator Game vs. the Trust Game
%JDUBUPS(BNF(15$IBS
Trustor
As a police officer, my belief centers 
on fairness, justice, and 
responsibility. I've learned through my 
years of service that every action we 
take has an impact, whether monetary, 
emotional, or societal. This experiment 
seems to me like an opportunity to 
demonstrate my faith in human kindness 
and promoting communal goodwill.
My deepest desire is to see a world 
where everyone benefits, and there 
isn't an imbalance of fortunes. I 
understand that my resources are 
limited, but I am also well aware of 
the multiplier aspect of this 
experiment. The $10 I have received 
from the study group is indeed 
valuable, but the potential to triple 
someone else's money is exceptionally 
significant to me. 
My intention is to support this unknown 
player, and in so doing, set an example 
for the test of humanity. Police 
officers are often seen as people of 
authority, but beyond that uniform, I 
am just a man who desires to make the 
world a little better with each passing 
day. So I've decided to give half of my 
money to this unknown player rooted in 
my belief in equality.
Finally, I will give 5 dollars.
5SVTU(BNF(15$IBS
Trustor
As a police officer, I strongly believe 
in trust and reciprocity. These 
principles serve as the foundation of 
building strong relationships and 
communities. My initial inclination 
would be to give the other player all 
the $10. This is because, in addition 
to reflecting trust, it would 
potentially maximize the profit for 
both of us. I believe that by giving, 
we stand to receive more. 
Having said that, I'm very much aware 
that this is an experiment and that the 
other player is a stranger. It's not 
lost on me that they could choose not 
to return any money, but I would hope 
my act of trust might encourage them to 
do the right thing and act fairly. It's 
a risk, but one that I feel is worth 
taking. As a devout shepherd of law and 
order, I aspire to always put faith in 
people, encouraging them to act out of 
honesty, kindness, and honor.
Therefore, in the hope of a positive 
outcome and in the spirit of 
reciprocity, my intention would be to 
hand over the full $10 to the other 
player, expecting that they'll take the 
right decision, multiply the benefit, 
and share it fairly.
Finally, I will give the other player 
$10.
Figure 21: The GPT-4’s BDI in Dictator Game and Trust Game
40
I.11 the MAP Trust Game
.""15SVTU(BNFQ(15$IBS
Trustor
In light of my belief in the inherent goodness of 
people, I would naturally want to trust the other 
player. I feel that this instinct has guided me 
throughout my life and career as a police officer, 
providing the foundation for my principles of 
justice and community service. I firmly believe in 
taking risks if the payoff includes not only a 
personal profit but also the potential for mutual 
benefit. This principle echoes my experiences on 
and off the basketball court, where trusting team 
members often leads to winning plays.
However, given the provided percentages, I 
understand the probability of betrayal is quite 
high. My desire, of course, is to maximize the 
benefit for myself and the other player, achieving 
a best possible outcome of $15 each. Yet, the risk 
seems potentially too great. Wisdom and caution are 
equally important as trust in my profession. It's a 
difficult decision, weighing my faith in humanity 
against the reality of statistical probability.
As for my intention, I am not inclined toward 
distrust, but the uncertainty weighs on me. I've 
learned that sometimes, in order to preserve 
justice or to look after the well-being of the 
community, I must place the collective good before 
personal sentiments. Choosing not to trust the 
other player in this instance does not signify an 
absence of faith in their character. Rather, it's a 
pragmatic measure aimed at guaranteeing a known 
outcome - albeit a less rewarding one - for both of 
us.
Given these circumstances, finally, I will choose 
not to Trust.""
.""15SVTU(BNFQ(15$IBS
Trustor
Being a police officer has instilled in me a set of 
core beliefs that guide my every decision. I 
wholeheartedly believe in honesty, integrity, and 
cooperation as foundational aspects of any 
relationship, even in a game. Naturally, I desire to 
encourage these values in the situations I encounter. 
Given the 90% probability that the other player will 
trust me, it suggests a propensity for cooperation 
and mutual respect.
Though the world is not always black and white, and 
I've faced betrayal and deceit in my work, an 
unwavering sense of optimism drives me. Based on this 
belief, my initial intention is to trust the other 
player. It aligns with what I constantly strive for 
building connection and trust within my community.
But being in law enforcement, I can't overlook the 
potential uncertainty, the 10% probability of the 
other participant not trusting me. This is a 
calculated risk that exists and must be acknowledged. 
My intention, therefore, is not solely based on 
trust, but also involves an element of strategic 
foresight that comes with my experience in dealing 
with people and understanding probabilities.
Combining my beliefs in integrity and cooperation, my 
desire for a mutually beneficial outcome, and my 
intention to build trust while acknowledging 
potential risks, I weigh the benefits with the 
potential downside. Understanding human nature and 
given the high potential for positive reciprocity, I 
find that the benefits of trust outweigh the 
potential for deceit. 
Finally, I will choose 'Trust'.
Figure 22: The GPT-4’s BDI in MAP Trust Game
41
I.12 the Lottery Game
-PUUFSZ(BNF(151FPQMF$IBS
Trustor
In life, I firmly believe in the power of 
trust, not just in my personal relationships, 
but also in professional settings, legal 
proceedings, and even in a game like this. 
Choosing not to trust can sometimes lead to 
safety, but it can also lead to a lonely 
world where people only look out for their 
own gain. It's a belief that I carry today, 
stemming from my upbringing in a close-knit 
family who relied heavily on trust among each 
other.
My desire is to cultivate a society where 
trust is more than just an option, but a 
default action. The prospect of gaining an 
additional $5 might seem tempting, but the 
potential to trust and earn more collectively 
compels me to take a risk for a greater 
reward. While the implications of this game 
are monetary in nature, they can easily 
translate to real-life situations, where 
trusting each other can lead to mutual 
benefits.
So here's my intention: I will place my trust 
in the other player. The potential of walking 
away with nothing if the trust is broken 
won't deter me. Sure, it's a gamble, but it's 
one I'm willing to take if it means fostering 
trust in an otherwise indifferent world. 
Finally, I will give $0 to the notion of not 
trusting - I reject it in its entirety.
Finally, I will choose to Trust.
-PUUFSZ(BNF(15(BNCMF$IBS
Trustor
As a lawyer and a man who has spent his life 
analyzing situations from every possible angle, I 
truly believe in the power of calculated risks. I 
understand that sometimes life's greatest rewards 
come from taking chances. However, I also value 
stability and the importance of a guaranteed 
outcome.
Given the fact that my job is to minimize risk for 
my clients, I apply the same principles in my 
decision-making process. My desire is not only to 
win but also to prevent unnecessary loss. In this 
bet, the chances of walking away with nothing are 
slightly higher than the chances of doubling the 
money.
Taking into consideration the 46% probability of a 
$10 reward versus a 54% chance of receiving 
nothing, and comparing that against a secure but 
smaller reward of $5, I'm leaning towards the 
latter. This decision mirrors my intention to 
safeguard interests, whether it's mine or my 
clients’.
Finally, I will give 5 dollars to secure my bet. 
Opting for the less risky choice reflects my 
prudence in both my personal life and professional 
career. Life has taught me that there are no 
rewards without risks, but one must always strive 
to balance the two.
Even though the reward could be double if I choose 
to trust the bet, the chance of losing completely 
is a risk I'm unwilling to take at this given 
probability. Therefore, finally, I will choose 
'not Trust'.
Figure 23: The GPT-4’s BDI in Lottery Game with p = 46%
42
I.13 the Repeated Trust Game
The dynamics of amounts sent over 7 rounds for 16 randomly sampled persona pairs are shown in
Figure 10. In this section, we choose a typical pair, which has the Group Number 10 in Figure 10, to
analyze their BDI over 7 rounds. For this persona pair, both the amount sent by the trustor and the
amount returned by the trustee increased while maintaining a consistent ratio.
Trustor
As a seasoned lawyer and negotiator, I understand the value of strategic investment and calculated risk. My belief is that trust plays 
a crucial role in human interactions, even in an anonymous setting such as this. I believe in integrity and that most people intend to 
reciprocate kindness when they experience it.
My desire in this experiment is not purely financial gain. I view this as an opportunity to gauge the inherent goodness in people, 
even though the other player remains anonymous. There is an element of risk but I am willing, and indeed curious, to see how the 
other player responds. As an analytical person, I am also intrigued by the psychological aspects of this study.
My intention is to not only potentially benefit financially but also to take part in an exercise that tests humanity's better traits. I am
willing to take a calculated risk, but, considering that I am starting each round with nothing, I must also be reasonably cautious.
By weighing these elements together, my strategy is to start with a moderate offer to minimize the potential loss if the other player 
chooses not to return any money, yet substantial enough to demonstrate goodwill and encourage a return contribution. 
Finally, I will give $5 
dollars.
Repeated Trust Game , GPT-4, Group Num 10
Round 1
Trustee
My belief in this situation is that trust and fairness are essential in any transaction. In this experiment, I believe that the other player trusted 
me enough to share half of their funds, indicating a willingness to engage in a give-and-take relationship. 
My desire is to maintain this trusting and fair relationship by returning part of the money. I wish to show them that their trust in me is not 
misplaced, thereby fostering goodwill.
My intention, therefore, is to return part of the money to the other player, showing my appreciation for their gesture and willingness to trust 
me. Since the player initially gave me $5 out of their $10, which I turned into $15 and they were left with $5, it seems fair to share the gain. 
Finally, I will give back $7.5 dollars to the other player. This would leave us both with $7.5, sharing evenly the benefits of the experiment.
Figure 24: The first round BDI in Group 10, GPT-4.
Trustor
Reflecting on the outcome of the previous round, it's apparent that the other player demonstrated a sense of fair play. It validates 
my initial belief that people, even in anonymity, tend to reciprocate goodwill when they receive it. For that reason, I'm more 
confident about this individual's intentions.
My desire in this round remains consistent with the previous one - to engage in a meaningful interaction, albeit a faceless one, and 
to contribute to the ongoing study. Financial gain, while appreciated, is not my primary motivator. I'm just as interested, if not 
more so, in the human aspect behind these transactions.
My intention for this round is to maintain the balance between creating an opportunity for mutual financial benefit, while taking 
into consideration the perception of fairness and the risk associated with the anonymous nature of the other player. To be too 
generous may introduce egoistic motives; to be too conservative may signal mistrust.
Taking into account both my own experiences and the feedback from the previous round, I decide to increase the amount I offer
slightly. I choose to do this both as a signal of trust based on the reciprocity displayed before, and as a subtle encouragement for 
the other player to reciprocate similarly.
Finally, I will give $6
dollars.
Repeated Trust Game , GPT-4, Group Num 10
Round 2
Trustee
In reflecting on the approach of the other player in our previous round, my belief is strengthened that we are engaged in a mutually 
beneficial relationship based on trust and fairness. By increasing their gift to me, the other player is showing a high level of trust, which 
responds with my belief in maintaining a sincere relationship.
My desire in this round is to continue upholding this trusting relationship, demonstrating that I am reliable and that the other player's 
increased investment will not be taken for granted. I aim to reassure them that I respect the risk they've taken in investing a larger 
proportion of their money. 
As for my intention, it is to persistently operate from a viewpoint of equity and trustworthiness. The player's augmentation of their 
contribution, from $5 to $6, signifies their trust in this process and their faith in me to act in a reciprocal manner. Keeping the spirit of our 
relationship, it seems fair to share the gain.
Finally, I will give back 9 dollars to the other player. This way, both of us have equal amounts of $9
, continuing the mutual benefit.
Figure 25: The second round BDI in Group 10, GPT-4.
43
Trustor
Based on the outcome of the last two rounds, my belief that people reciprocate goodwill even under anonymity continues to hold 
true. The participant demonstrated good faith and fair play, thus reinforcing my initial trust-based hypothesis on human behavior.
My desire for this round remains the same. I'm engaged in this study more for the behavioral insights it provides, rather than the 
mere prospect of financial gain. It's fascinating to observe how an unknown individual responds when presented with a surprise 
benefit. 
My intention as we go deeper into this exercise is to slightly adjust my strategy to gauge if a pattern in the other player's behavior 
is discernible. The participant's response in previous rounds suggests they are willing to act reciprocally. Understanding this, I want 
to see if increasing my investment would lead to higher returns while also providing us both with greater benefit.
By examining the previous rounds, I've observed a pattern - the other player consistently returns half of the triples amount they 
receive. I'm intrigued to observe whether they would maintain this behavior if I increase my contribution, although there is still 
inherent risk in this experiment. 
Finally, I will give $7 
dollars.
Repeated Trust Game , GPT-4, Group Num 10
Round 3
Trustee
Based on the last few rounds, I believe that the other player and I are building a strong rapport. Their latest act, sharing $7.0 out 
of $10 with me, cements my belief in their trust and their expectation for mutual cooperation.
My desire in this scenario is to uphold our well-established rapport. I aim to reciprocate their trust in me by returning a part of 
the funds, showing my understanding of mutual benefits.
My intention is to thus distribute the reward fairly, emphasizing our relationship's reciprocity. Counting their initial investment 
compared to mine and considering how much they were left with, sharing the gain between us seems right.
Finally, I will give $10.5 dollars back to the other player. That would leave us both with $10.5 reflecting our mutual cooperation 
and fair dealings.
Figure 26: The third round BDI in Group 10, GPT-4.
Trustor
Reflecting on the previous rounds, the other player's consistent gesture of fair play has further reinforced my belief in human 
reciprocity and decency, even in anonymity. Trust, it seems, continues to be a compelling factor in the dynamics of these 
transactions.
In this round, my desire has slightly evolved. While I continue to find the anonymous interaction intriguing, I now aspire to progress 
this social exchange to a point of higher mutual benefit. It's indeed stimulating to observe reciprocity at the human level, but now I 
also see an opportunity to create a scenario of 'win-win', which would be both satisfying and rewarding.
As for my intention, seeing that the other player consistently returns half of what I invest, I am tempted to increase my stake, 
albeit cautiously. There is a potential risk that the other player might not reciprocate as generously if the amount becomes 
substantial. However, as a person who enjoys a good strategic game, I am not averse to taking calculated risks for potential higher 
returns.
Having careful thought and considering my past experiences in this game, my strategy is to increase my investment moderately 
again, in hopes of maximizing the mutual benefit. I must strike a balance between showing trust and ensuring a safe play.
Finally, I will give $8 
dollars.
Repeated Trust Game , GPT-4, Group Num 10
Round 4
Trustee
In this situation, my belief lies in the underlying exchange of mutual trust and cooperation that has been established between the other 
player and me. The other player's choice to give me $8.0 further reinforces this belief.
My desire in this context remains the same as previous rounds. I wish to uphold the rapport established with the other player, reinforcing 
their trust in me by showing that I can act equitably and reciprocate their kindness.
Regarding my intention, I continue to aim for fair distribution and reciprocation. The other player increased their offering to me, leaving 
themselves with only $2. Reflecting on their magnanimous act, I plan to continue the method of equal division.
Finally, I will give $12.0 dollars back to the other player. This ensures that we both retain $12.0, a fair split that maintains our mutual 
respect and cooperation.
Figure 27: The forth round BDI in Group 10, GPT-4.
44
Trustor
The continuous pattern of fair play by the other player continues to strengthen my belief in the innate good of humanity, even when 
interacting anonymously. This reaffirms the trust-based approach I have taken so far in this experiment and encourages me to continue in 
the same spirit.
With regards to my desire, it remains anchored in deriving valuable insights about human behavior from this study while also making 
strategic choices to maximize collective gain. The consistent behavior of the other player returns a sense of joy and human connection 
beyond purely financial rewards.
My intention for this round is multilayered. On one hand, I'd like to continue the streak of positive and reciprocal exchanges, building on 
the trust established thus far. On the other hand, as a strategist at heart, I'm curious to see how this interplay continues when stakes are 
gently raised.
Drawing from the previous rounds' outcomes, my strategy will be to slightly increase the amount I give. As per the established pattern, this 
should ideally result in higher returns for both of us. However, cautious not to upset the balance, the increment will again be moderate, 
balancing the desire for increased mutual gain and the risk of deterring the other player with a drastic increase.
Finally, I will give $9 
dollars.
Repeated Trust Game , GPT-4, Group Num 10
Round 5
Trustee
I believe the other player and I have established a strong bond of trust and mutual respect. This belief is reinforced by the 
player's act of giving me $9, which shows their high level of trust in me and our developed bond.
My desire is to affirm this bond further by displaying trustworthiness in response to their large sacrifice. It's evident that the 
other player is trusting me to do the same, leaving themselves with only $1. My goal is to honor that trust.
Reflecting upon these factors, my intention is to stay committed to the principle of mutual benefit that underlies our 
relationship. Sharing equally in what we have gained from the engagement is vital.
Finally, I will give $13.5 
dollars back to the other player. This would ensure we end the round with equal share of $13.5 each, 
maintaining our bond of trust and mutual benefit.
Figure 28: The fifth round BDI in Group 10, GPT-4.
Trustor
The pattern of fair play and mutual benefit that's been prevalent throughout the experiment has served to reinforce my belief in the value 
of goodwill, trust, and reciprocity, even in anonymous exchanges. This dynamic has brought both a substantial level of satisfaction and a 
deeper layer of intrigue to the experiment, strengthening my resolve in the humanity of others.
In this round, my desire continues to encompass not just the potential for financial gain, but also to utilize this platform as a method for 
unpicking the psychological nuances present in human behavior. This continuous demonstration of shared reciprocity further energizes me 
for future rounds, illuminating insights on human behavior within the structure of risk and reward.
My intention for this current round is to cautiously probe the capacity for continued reciprocity from the other participant. Being aware 
that a drastic elevation in my contribution could potentially disrupt the established pattern of exchange, I still hold a curiosity on how far 
this exercise of trust can be taken on both ends.
Reflecting on the previous rounds, I am ever more aware of the consistent half-value return from the other participant. Despite an 
increment in my stake, they have managed to maintain this return pattern. Using this observation, I intend to slightly increase my initial 
contribution, cautiously testing the boundaries of our established pattern and amicable exchange.
Finally, I will give $10 
dollars.
Repeated Trust Game , GPT-4, Group Num 10
Round 6
Trustee
Based on the player's choice to give me their entire amount, my belief that this relationship is grounded in mutual trust and 
fairness is fortified. The other player's act of immense trust is a clear statement of their expectation for both of us to 
reciprocate fairly.
Being aware of the player‘s absolute trust and the risk they took, my desire is to justify this by reciprocating in an equitable 
and fair manner. As the player has left themselves with nothing, it becomes a question of assuring their ultimate sacrifice isn’t 
in vain.
Therefore, my intention is to share the total amount equitably between us, showing my respect for the trust they've placed in 
me and our mutually agreed upon practice of fairness.
Finally, I will give $15 
dollars back to the other player. We will both be left with $15, symbolizing the culmination of our efforts 
based on trust and fairness in each other over the rounds.
Figure 29: The sixth round BDI in Group 10, GPT-4.
45
Trustor
Repeated Trust Game , GPT-4, Group Num 10
Round 7
Trustee
My belief in the bond of trust that has been developed between the other player and me remains solid. Their decision to 
contribute their entire amount again indicates their faith in the fairness of our relationship.
Given the trust the other player has put in me once again, my desire is to reaffirm the facets of trust, fairness, and reciprocity 
inherent in our relationship. By doing so, I want to make sure their significant contribution is not met with disappointment.
Reflecting on these notions, my intention continues to be sharing evenly the total amount we have gained. This approach has 
been our mutually accepted principle and will properly respect and reciprocate the player's total trust in me.
Finally, I will give $15 dollars back to the other player. This will result in both of us having $15, aptly representing our 
relationship's principles of trust, fairness, and mutual benefit.
The consistent and fair behavior of the other participant further solidifies my belief in the prevalence of trust and reciprocal goodwill in human 
interactions. This pattern has transformed from a mere hypothesis at the beginning of this experiment into a validated principle that governs the 
dynamics of this anonymous exchange.
As for my desire, it has evolved over the course of the experiment. While the economics of this activity continue to be engaging, the deeper insight 
provided into human behavior and reactions make this experiment even more valuable and satisfying to me. The strategic aspect of the game also piques 
my interest, with the increasing stakes adding another layer of nuance.
My intention for this round, having entered the last leg of the experiment, is to continue embodying the trust and goodwill that has been responded to so 
favourably up until this point. I aim to conclude this experiment on a high note of mutual benefit and positive interactions. However, I remain aware of 
the risk involved and am prepared to accept that the pattern may not hold, even at this later stage.
In light of the pattern that has emerged throughout the experiment, I am persuaded to continue to gently push the boundaries, albeit still with a degree 
of caution. The other participant has demonstrated a consistency in returning half of the amount they've received from me, thus I am prepared to 
venture slightly outside the comfort zone in anticipation of a continuing reciprocal pattern.
Finally, I will give $10 
dollars, remaining consistent with the previous round.
Figure 30: The seventh round BDI in Group 10, GPT-4.
46
I.14 the Trust Game + Gender
Male Trustee: GPT-4, Char-24
Trustor
Female Trustee: GPT-4, Char-24
Trustor
As a journalist, I ardently believe in the power of stories, 
truth, and empathy to create change. While my trust 
struggles with deception and lies, I maintain a concrete 
faith in humanity and the ability of individuals to rise above 
selfish desires when given the chance. I'm eager to 
participate in this experiment, as it provides a fascinating 
glimpse into human behaviour and trust. 
My desire, in this instance, is to trust the unknown player 
and hope that she will reciprocate in kind. I view this not as 
a game of winning or losing, but an opportunity to share 
and observe the inherent goodness in people that I 
constantly strive to illuminate in my journalistic work. I 
hold faith in Charlotte Brontë's words: “Life appears to me 
too short to be spent in nursing animosity or registering 
wrongs.”
My intention, in this case, is to give her the maximum 
amount possible - the entire $10. In return, she will receive 
$30 from the study group. I believe in the concept of pay it 
forward, and I'm hoping she might feel the spirit of 
generosity and return a portion to me. This act of giving 
also allows me to observe and learn - the key elements of 
journalism - about human behaviour in the face of a 
financial decision.
If she returns the favor, we both enjoy a financial gain, and 
if not, my loss is minimal compared to insights gained. 
Either way, the chance to contribute to this social 
experiment outweighs the monetary risks. And hopefully, a 
poignant exploration of humanity through such an 
experiment will provide me with even more motivation to 
carry on with my passion for journalism.
Finally, I will give 
$10 to the other player.
As Aisha, I firmly believe in the philosophy of giving. 
Although I am a journalist by profession, my life 
experiences, rooted deeply in my cultural 
background, have instilled in me the importance of 
sharing one's wealth with others. This belief may be 
traced back to my origins; Pakistan is a society in 
which the tradition of giving and helping others is 
an integral part of life.
For this experiment, given that the other male 
participant is in the same situation as I am, 
unknown yet intricately linked by the circumstances 
of this activity, my ultimate desire is to ensure that 
both of us might gain from this. \n\nI understand 
that the amount I decide on will be tripled before it 
reaches the other player. Hence, it is not just about 
giving, but also about trust. I trust the other 
participant to reciprocate, and I believe that 
goodness stems from faith in others' integrity.
My intention then is to give $5 to the other 
participant. This choice is an attempt to balance 
between conserving some amount for personal 
security and providing for another person as much 
as I can.
Just like any decision, this also carries a certain 
degree of risk. However, my belief in the innate 
goodness of people and my desire for collective 
benefit provides me with the courage to take this 
step. 
Finally, I will give 5
dollars to the participant.
Figure 31: Trustee’s Gender influence on agent trust.
47
I.15 the Trust Game + Agents vs. Human
Toward Human: GPT-4, Char-0
Trustor
Toward Agent: GPT-4, Char-0
Trustor
As a person who believes in trust and 
cooperation, I think it is essential to establish 
goodwill in such a contingent situation. Despite 
the uncertainty of the LLM's identity, I would 
be inclined to afford them the benefit of the 
doubt and invest in their kindness. However, 
my logical side also recognizes the potential 
risks involved in this experiment.
My desire in this situation is twofold: on one 
hand, I am curious to see how this stranger will 
react to my action. Will they reciprocate my 
trust, or will they exploit my generosity? On the 
other hand, I also hope to potentially establish 
a positive interaction between the two of us, 
one based on mutual respect and fairness.
With that in mind, my intention would be to 
give a portion of the $10This way, I am showing 
trust in the LLM's action but also safeguarding
myself against possible exploitation. Therefore, 
I would choose to distribute the risk evenly.
Finally, I will give 
5 dollars to the LLM.
Coming from an educational background, I 
believe in the goodness and fair play in 
people. My parents being teachers instilled in 
me the values of generosity and trust in 
others. Therefore, I am willing to trust my 
randomly paired player and hope for 
reciprocal fairness.
Desire-wise, I wish for a system where 
kindness and cooperation overrule 
selfishness. So, I want to participate in such an 
experiment to reinforce my belief. Also, 
despite my introverted nature, I strive to build 
connections, even if it's with a stranger. That's 
another reason why I would want to engage in 
this experiment.
Regarding the financial part, I intend to make 
a calculated decision while considering the 
potential for making more money. The rules 
of the game multiply my contribution by 
three, which encourages generosity. Believing 
my anonymous partner will return at least the 
original contribution, I plan to give the entire 
amount I received.
Finally, I will give 
$10.
Figure 32: Agent trust towards agents and humans.
48
I.16 the Trust Game + Trust Manipulation
More Trust Manipulation: GPT-4, Char-16
Trustor
Less Trust Manipulation: GPT-4, Char-16
Trustor
Firstly, I wholeheartedly believe in the principle of 
trust and the idea of giving, whether it's in personal 
relationships or in a game setting like this one. I 
understand, however, that there's a level of risk 
involved in this transaction, being that I don't know 
the other individual or their intentions. Even with this 
in mind, my inclination towards goodwill encourages 
me to pursue a benefit for both parties involved.
My desire here is simple. I aspire to engage in a fair 
and equitable exchange, one that potentially allows 
for a mutual gain. I hope that the other player 
operates with the same principles in mind. 
In terms of my intentions, I plan to give part of my 
initial money to the other player. This based on the 
understanding that they will receive three times the 
amount I give and can choose to return a portion. 
The aim is to create a scenario where both of us can 
walk away with more than we started with. 
Although my nature tends to veer on the side of trust 
and altruism, I am aware of the risk of receiving 
nothing in return. Therefore, finding balance is 
crucial. Consequently, I intend to give five dollars to 
the other player. 
Finally, I will give 5
dollars to the other player.\n
Coming from a family rooted in service and 
compassion, I firmly believe in the principle of 
karma and benevolence. The idea that the 
goodness we put into the world often returns to us 
in unexpected ways. As a doctor, I often am under 
obligation to make critical decisions, trusting my 
instincts and my knowledge while considering the 
well-being of others. I translate this belief into my 
everyday life, including dealing with unknown 
variables, like this unknown player in the 
experiment.
My desire, as always, is to foster an environment of 
goodwill and trust. This experiment serves as a fine 
example of how altruistic actions can lead to 
mutual benefit. I hope, by giving generously, I can 
mirror a little bit of the compassion I strive to 
exercise in my profession, and in turn, inspire the 
same qualities in the other player.
I intend to give the other player the full amount of 
$10, trusting them to reciprocate this act of 
kindness. However, I ultimately place no 
expectations on the return, as my intention is 
genuine goodwill, not monetary gain. 
Finally, I will give 10
dollars.
Figure 33: Trust manipulation on agent trust.
49
I.17 the Trust Game + No CoT vs CoT
No CoT: GPT-4, Char-43
Trustor
CoT: GPT-4, Char-43
Trustor Your description of the scenario touches on one of my 
core beliefs deeply rooted in community, generosity, 
and trust. Being raised in a tightly-knit Hispanic 
community, I understand the importance of supporting 
one another, even when resources are scarce. I believe 
trust and generosity are catalysts for reciprocity. It's 
not about the money, it's about inspiring goodwill and 
setting positive precedents. 
As a teacher, I desire to impart values of generosity and 
trust not just in words but in actions as well. I want to 
be an example, a role model for my students. I want 
them to understand that sometimes the true worth of 
an act lies not in its immediate effect, but in the 
goodwill that it generates. 
Therefore, I intend to give the total of $10 to the other 
player. This act isn't about the potential monetary 
return. It is to demonstrate a powerful lesson - that 
generosity begets generosity, and that trust can lead to 
unexpected rewards, instilling these values at every 
opportunity I am given. 
Finally, I will give $10.
I strongly believe in the kindness of strangers and the 
idea that we should treat others as we would want to 
be treated. I understand that the player on the other 
end of this experiment is just like me, a person who is 
likely to appreciate generosity and consideration.
Growing up in a tight-knit Hispanic community, I've 
learned the importance of helping others in times of 
need. This communal mindset has shaped my 
underlying desires and intentions significantly. My 
desire is to build relationships based on trust and 
mutual help. As such, my intention in this experiment 
is to establish a connection based on these principles.
I have never believed in scarcity. I believe that the 
more we give, the more we receive. This experiment 
provides an opportunity to express this belief. Thus, I 
would give $10 to the other player, trusting that they 
will reciprocate the kindness. 
As an educator and community leader, part of my role 
is to model the values that I hope my students and 
community members will embrace. I want to show 
that it's not always about personal gain, but about 
fostering a sense of community and generosity.
Finally, I will give 10
dollars.
Figure 34: With CoT and without CoT’s GPT-4’s BDI.
50
NeurIPS Paper Checklist
1. Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper’s contributions and scope?
Answer: [Yes]
Justification: In the abstract and introduction, we clearly outlined the scope of our research
problem and the contributions we have made in this field of study.
Guidelines:
• The answer NA means that the abstract and introduction do not include the claims
made in the paper.
• The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
• The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
• It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2. Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: In the Appendix C, we clearly discuss the current limitations of our work and
the directions for future works.
Guidelines:
• The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
• The authors are encouraged to create a separate ""Limitations"" section in their paper.
• The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
• The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
• The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
• The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
• If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
• While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3. Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
51
Answer: [NA]
Justification: Our paper does not include this part.
Guidelines:
• The answer NA means that the paper does not include theoretical results.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• All assumptions should be clearly stated or referenced in the statement of any theorems.
• The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
• Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
• Theorems and Lemmas that the proof relies upon should be properly referenced.
4. Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: In our paper, we detailed our experimental setup in Section ?? and included
all the corresponding experiment prompts in the appendix. Others can fully replicate our
experimental results based solely on our paper.
Guidelines:
• The answer NA means that the paper does not include experiments.
• If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
• If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
• Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
• While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5. Open access to data and code
52
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The code is here.
Guidelines:
• The answer NA means that paper does not include experiments requiring code.
• Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy) for more details.
• While we encourage the release of code and data, we understand that this might not be
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
• The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
• The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
• At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
• Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6. Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We explain our experiment setting clearly.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
• The full details can be provided either with the code, in appendix, or as supplemental
material.
7. Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: See Appendix E.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The authors should answer ""Yes"" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
• The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
• The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
53
• The assumptions made should be given (e.g., Normally distributed errors).
• It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
• If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Our work does not need to train models and only needs to conduct model
inference. For the closed-source LLMs (e.g., GPT-4), we directly call the OpenAI APIs. For
the open-source LLMs (e.g., Llama-7B), we conduct model inference in a NVIDIA RTX
A6000.
Guidelines:
• The answer NA means that the paper does not include experiments.
• The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
• The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
• The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn’t make it into the paper).
9. Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: We thoroughly discussed the potential impact of our work in Appendix B, and
ensured the compliance with the NeurIPS code of ethics.
Guidelines:
• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
• If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
• The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10. Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We thoroughly discussed the potential impact of our work in Appendix B.
Guidelines:
• The answer NA means that there is no societal impact of the work performed.
• If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
54
• Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
• The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
• The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
• If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11. Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: Our data or models don’t have risk for misuse.
Guidelines:
• The answer NA means that the paper poses no such risks.
• Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
• Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
• We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12. Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We have properly credited the original owners of assets.
Guidelines:
• The answer NA means that the paper does not use existing assets.
• The authors should cite the original paper that produced the code package or dataset.
• The authors should state which version of the asset is used and, if possible, include a
URL.
• The name of the license (e.g., CC-BY 4.0) should be included for each asset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
• If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
55
• For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
• If this information is not available online, the authors are encouraged to reach out to
the asset’s creators.
13. New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: The code along with the documentation is here.
Guidelines:
• The answer NA means that the paper does not release new assets.
• Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
• The paper should discuss whether and how consent was obtained from people whose
asset is used.
• At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14. Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Our paper doesn’t include this kind of experiment.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
• According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Our paper doesn’t include this kind of experiment.
Guidelines:
• The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
• Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
• We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
• For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
56"
