{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7f23718fa08d4961bbc7b15326608840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d04472afb08411ab8a5faef688ba304",
              "IPY_MODEL_5bd3920f8b154956be13e3293e19d047",
              "IPY_MODEL_53f815c2a4934d7ba53e6f654d1fe24b"
            ],
            "layout": "IPY_MODEL_acf6499983e94ebf8bc7a0dd6ecee7fe"
          }
        },
        "8d04472afb08411ab8a5faef688ba304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48a6810331214b849490a4258cff238a",
            "placeholder": "​",
            "style": "IPY_MODEL_0d99a2dbf7534841ba8928c6ec627389",
            "value": "modules.json: 100%"
          }
        },
        "5bd3920f8b154956be13e3293e19d047": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1e2c4b300ce4688abc3f3d2ec4f7776",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_697b3c127d7c48849adad34d03f2658c",
            "value": 349
          }
        },
        "53f815c2a4934d7ba53e6f654d1fe24b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c7c88d3b1c04c599f9fa2608c80b39a",
            "placeholder": "​",
            "style": "IPY_MODEL_ee63ef443011435a9290d7b4fc429be4",
            "value": " 349/349 [00:00&lt;00:00, 18.6kB/s]"
          }
        },
        "acf6499983e94ebf8bc7a0dd6ecee7fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48a6810331214b849490a4258cff238a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d99a2dbf7534841ba8928c6ec627389": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1e2c4b300ce4688abc3f3d2ec4f7776": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "697b3c127d7c48849adad34d03f2658c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c7c88d3b1c04c599f9fa2608c80b39a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee63ef443011435a9290d7b4fc429be4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81757c8aa4ef434a96e778de2bf027cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9630a8f3bb2040ae917cfc28bacff368",
              "IPY_MODEL_d7fb75b9b28f4f6c920a058f5e655a24",
              "IPY_MODEL_f608a4b6949e48a68544ceb954622f67"
            ],
            "layout": "IPY_MODEL_339718118b0f45b691395783af8c903a"
          }
        },
        "9630a8f3bb2040ae917cfc28bacff368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00d49b74fde84373a88c47976185a511",
            "placeholder": "​",
            "style": "IPY_MODEL_8a3ccc81aee14dd3a4d85b297ec26892",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "d7fb75b9b28f4f6c920a058f5e655a24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bef0930708874c8d88216e2a042f7013",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f01a396a9148421c91dbc3daa4f61876",
            "value": 116
          }
        },
        "f608a4b6949e48a68544ceb954622f67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e0c13b9aa7e495ca52c7f9b2697e10f",
            "placeholder": "​",
            "style": "IPY_MODEL_9b386594b7f14be6bd22cdf521881065",
            "value": " 116/116 [00:00&lt;00:00, 6.21kB/s]"
          }
        },
        "339718118b0f45b691395783af8c903a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00d49b74fde84373a88c47976185a511": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a3ccc81aee14dd3a4d85b297ec26892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bef0930708874c8d88216e2a042f7013": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f01a396a9148421c91dbc3daa4f61876": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5e0c13b9aa7e495ca52c7f9b2697e10f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b386594b7f14be6bd22cdf521881065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55c8e3f8377d48de9905df7f5f0a987b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_426a1b6ad6d64822b79afb83f70bb9b3",
              "IPY_MODEL_e76fd7c008454acf90777eb1fd99d434",
              "IPY_MODEL_d3b6a19e37894cd4941d885e52d9b526"
            ],
            "layout": "IPY_MODEL_5e4f38528f424120a77182e8b5c26f2c"
          }
        },
        "426a1b6ad6d64822b79afb83f70bb9b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79abbaadb4a4485fadd8a5ced47376b1",
            "placeholder": "​",
            "style": "IPY_MODEL_11d4722a337148adb7301a0a2a24991c",
            "value": "README.md: "
          }
        },
        "e76fd7c008454acf90777eb1fd99d434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13a7d60304484b6990ae4f25ff1d8d42",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5821ce374744fdcba8ed8bfba28ff9e",
            "value": 1
          }
        },
        "d3b6a19e37894cd4941d885e52d9b526": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a1a7692aedb448eb87e1fc18cf9d5fc",
            "placeholder": "​",
            "style": "IPY_MODEL_ba7859b627964556a21b6300b0d01f36",
            "value": " 10.5k/? [00:00&lt;00:00, 561kB/s]"
          }
        },
        "5e4f38528f424120a77182e8b5c26f2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79abbaadb4a4485fadd8a5ced47376b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11d4722a337148adb7301a0a2a24991c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13a7d60304484b6990ae4f25ff1d8d42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d5821ce374744fdcba8ed8bfba28ff9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a1a7692aedb448eb87e1fc18cf9d5fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba7859b627964556a21b6300b0d01f36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27df6fd930c848ac92c9afbe3943ef04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02d4d6cfd36346ee9f74d45f39ec7353",
              "IPY_MODEL_87b3b6b8f9cf40159abe7177380c6812",
              "IPY_MODEL_579b8b70f9bf4bf28d4d13b338976c12"
            ],
            "layout": "IPY_MODEL_a2cf6b2c2f9f4d5db84a4fd47d2feab7"
          }
        },
        "02d4d6cfd36346ee9f74d45f39ec7353": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cda6b9f35e5489e9bca24aaac602e50",
            "placeholder": "​",
            "style": "IPY_MODEL_5ae583732b7b435e92fe490f066de6be",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "87b3b6b8f9cf40159abe7177380c6812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dabc122c4204393b9b43ddd71abb58c",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f5485eb9f8e4db4be0504dac858a64d",
            "value": 53
          }
        },
        "579b8b70f9bf4bf28d4d13b338976c12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01523f1b31274e8aa9f9e99018f15687",
            "placeholder": "​",
            "style": "IPY_MODEL_e35b89b848394b78b538299b76d6a013",
            "value": " 53.0/53.0 [00:00&lt;00:00, 2.61kB/s]"
          }
        },
        "a2cf6b2c2f9f4d5db84a4fd47d2feab7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cda6b9f35e5489e9bca24aaac602e50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ae583732b7b435e92fe490f066de6be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dabc122c4204393b9b43ddd71abb58c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f5485eb9f8e4db4be0504dac858a64d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "01523f1b31274e8aa9f9e99018f15687": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e35b89b848394b78b538299b76d6a013": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0372025423942d7a1c448741eebf2ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b077969df15140d0b4ffc2b7b72d8bd2",
              "IPY_MODEL_43176ec262ab4c28bbebcbe50281a968",
              "IPY_MODEL_c9cf319e6a434d94912a65ffb783fa5f"
            ],
            "layout": "IPY_MODEL_5459959ffcbb4573ad99671b15931bae"
          }
        },
        "b077969df15140d0b4ffc2b7b72d8bd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_261364d7bb28417fb46700baa57675fc",
            "placeholder": "​",
            "style": "IPY_MODEL_3e2c4c862c10407e8dd2a90fbc56c9b7",
            "value": "config.json: 100%"
          }
        },
        "43176ec262ab4c28bbebcbe50281a968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4732078b57fc43388f63f806503a35df",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_48da6ec8ce5d43fb97dd11472f40c9b1",
            "value": 612
          }
        },
        "c9cf319e6a434d94912a65ffb783fa5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b06875d4ef24cea92bcfeba6dcf0499",
            "placeholder": "​",
            "style": "IPY_MODEL_6f8676ceb1e94ae7bd93a4065e907334",
            "value": " 612/612 [00:00&lt;00:00, 38.0kB/s]"
          }
        },
        "5459959ffcbb4573ad99671b15931bae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "261364d7bb28417fb46700baa57675fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e2c4c862c10407e8dd2a90fbc56c9b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4732078b57fc43388f63f806503a35df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48da6ec8ce5d43fb97dd11472f40c9b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b06875d4ef24cea92bcfeba6dcf0499": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f8676ceb1e94ae7bd93a4065e907334": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a93a69d63094fe4a80fa8bc847eea67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a153bc0efe540e3963672d164dd4e08",
              "IPY_MODEL_661752b4a62f496091df1eb481dfb2c4",
              "IPY_MODEL_2a88a86923004dd3b1818eca14ada5a2"
            ],
            "layout": "IPY_MODEL_7a4e93e98b514d129141aa9e87c5e625"
          }
        },
        "3a153bc0efe540e3963672d164dd4e08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e68fb863947b4f38abefdcaebe571f4f",
            "placeholder": "​",
            "style": "IPY_MODEL_350c75696595456fba2f2d21b5ae2977",
            "value": "model.safetensors: 100%"
          }
        },
        "661752b4a62f496091df1eb481dfb2c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7878dba07e07414c96bb60a452e00d7b",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_187dfdac58a44d1ca6b5f6513208993a",
            "value": 90868376
          }
        },
        "2a88a86923004dd3b1818eca14ada5a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c0bbe356539406c87837f1763b6c981",
            "placeholder": "​",
            "style": "IPY_MODEL_a198ab981e5e42d69cd85034ee0cf383",
            "value": " 90.9M/90.9M [00:01&lt;00:00, 104MB/s]"
          }
        },
        "7a4e93e98b514d129141aa9e87c5e625": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e68fb863947b4f38abefdcaebe571f4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "350c75696595456fba2f2d21b5ae2977": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7878dba07e07414c96bb60a452e00d7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "187dfdac58a44d1ca6b5f6513208993a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c0bbe356539406c87837f1763b6c981": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a198ab981e5e42d69cd85034ee0cf383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e9be35aa25d4f07b0af8a2f7cf0f024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_868b4032f39c48f897f6b2122bc66d67",
              "IPY_MODEL_3ef46e0a37e4474280e98b32c11614c7",
              "IPY_MODEL_266301831422459fb14c6b80a74bde93"
            ],
            "layout": "IPY_MODEL_e58f369b96f54b9e86b13c16270eb033"
          }
        },
        "868b4032f39c48f897f6b2122bc66d67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9080fccbbe7476a9d562c1cd5166ad3",
            "placeholder": "​",
            "style": "IPY_MODEL_2d6d7f89915947c1aebaded1d4fef351",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "3ef46e0a37e4474280e98b32c11614c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8591251b8c44011a5cc82098957e448",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96e45c20b577460fbb1c6bf333713dc9",
            "value": 350
          }
        },
        "266301831422459fb14c6b80a74bde93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7492457c1e0841438e96420c2f60e028",
            "placeholder": "​",
            "style": "IPY_MODEL_22563f8216394a7baf4781e85b3d76e1",
            "value": " 350/350 [00:00&lt;00:00, 26.6kB/s]"
          }
        },
        "e58f369b96f54b9e86b13c16270eb033": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9080fccbbe7476a9d562c1cd5166ad3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d6d7f89915947c1aebaded1d4fef351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8591251b8c44011a5cc82098957e448": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96e45c20b577460fbb1c6bf333713dc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7492457c1e0841438e96420c2f60e028": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22563f8216394a7baf4781e85b3d76e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27f660e6c3fa49b0bb6a5f0f774f56c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6fba63dfc2b4410bb9f72c5f01842752",
              "IPY_MODEL_7f90db41ecde413fb9d54299c21b7b13",
              "IPY_MODEL_45938068781743e0bd0d303be934dc64"
            ],
            "layout": "IPY_MODEL_e57cace810b84f068b8871b3b361004b"
          }
        },
        "6fba63dfc2b4410bb9f72c5f01842752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d81434e941ad4abc97570bd4c1bf4314",
            "placeholder": "​",
            "style": "IPY_MODEL_9ee5d91f61304656a340a534157337e2",
            "value": "vocab.txt: "
          }
        },
        "7f90db41ecde413fb9d54299c21b7b13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd1914e460d849d3bb4a701b783ab184",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3837584b19e54ddd980c0ac30501b624",
            "value": 1
          }
        },
        "45938068781743e0bd0d303be934dc64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb789b8f7d464b71b97ed8d795e73b76",
            "placeholder": "​",
            "style": "IPY_MODEL_b2b11e66c36d4b9d8f761fbf9177a966",
            "value": " 232k/? [00:00&lt;00:00, 9.88MB/s]"
          }
        },
        "e57cace810b84f068b8871b3b361004b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d81434e941ad4abc97570bd4c1bf4314": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ee5d91f61304656a340a534157337e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd1914e460d849d3bb4a701b783ab184": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3837584b19e54ddd980c0ac30501b624": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb789b8f7d464b71b97ed8d795e73b76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2b11e66c36d4b9d8f761fbf9177a966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9fe340919971436e8647b509bb059e6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4625079ee6094e15b15165088da9a01e",
              "IPY_MODEL_6c7d5d04ac8845bf9c81f81e6e21931f",
              "IPY_MODEL_fdf9eec965f64b7496e69a5ce850568f"
            ],
            "layout": "IPY_MODEL_fd8fa08ed15c4f189b85033b043b800d"
          }
        },
        "4625079ee6094e15b15165088da9a01e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0dfc0d9d44f4aa78f22f24c08763950",
            "placeholder": "​",
            "style": "IPY_MODEL_11151c0abf224f83a9947b8a977033f7",
            "value": "tokenizer.json: "
          }
        },
        "6c7d5d04ac8845bf9c81f81e6e21931f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae9dd7008bf340999e61dbf49448efb3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_390cfe34b5bd4ce9a3825ba02ddd422b",
            "value": 1
          }
        },
        "fdf9eec965f64b7496e69a5ce850568f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c97e6bfceca5439dbf808eb2f4d080a4",
            "placeholder": "​",
            "style": "IPY_MODEL_5176af9dd0024b918786bb3cae47c436",
            "value": " 466k/? [00:00&lt;00:00, 24.5MB/s]"
          }
        },
        "fd8fa08ed15c4f189b85033b043b800d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0dfc0d9d44f4aa78f22f24c08763950": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11151c0abf224f83a9947b8a977033f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae9dd7008bf340999e61dbf49448efb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "390cfe34b5bd4ce9a3825ba02ddd422b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c97e6bfceca5439dbf808eb2f4d080a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5176af9dd0024b918786bb3cae47c436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4196eaee97d4d5bba73637f42360396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6d26e973d9c4f36b21d206351b2cdb2",
              "IPY_MODEL_d9b785ec3f2e44f79ef31dbbddc75ee1",
              "IPY_MODEL_34f91ba9fecb49baa851496eff9a3bac"
            ],
            "layout": "IPY_MODEL_6338b68301264d70a125a87aa998ebf9"
          }
        },
        "e6d26e973d9c4f36b21d206351b2cdb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_218bb6538f9c405bad356b56132bdc13",
            "placeholder": "​",
            "style": "IPY_MODEL_eac8aae02fb6437f9f53d4ac994a2189",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "d9b785ec3f2e44f79ef31dbbddc75ee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca4fa4e937a1412c8eb85d1155592fc2",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ba5fdd0cc4b422d8512303b693fdd0b",
            "value": 112
          }
        },
        "34f91ba9fecb49baa851496eff9a3bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebc4a68fc10c4926936737dbe77cf7be",
            "placeholder": "​",
            "style": "IPY_MODEL_99a8925382ff491e83abed4d2b375682",
            "value": " 112/112 [00:00&lt;00:00, 11.8kB/s]"
          }
        },
        "6338b68301264d70a125a87aa998ebf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "218bb6538f9c405bad356b56132bdc13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eac8aae02fb6437f9f53d4ac994a2189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca4fa4e937a1412c8eb85d1155592fc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ba5fdd0cc4b422d8512303b693fdd0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ebc4a68fc10c4926936737dbe77cf7be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99a8925382ff491e83abed4d2b375682": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d401872910a4375a625233c56510c91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04d0c3fda9dd4996a395b542781e807c",
              "IPY_MODEL_bedbcaf04efe42c6829a36c522454f2e",
              "IPY_MODEL_3e45beca46384e8e93411c37d575dff3"
            ],
            "layout": "IPY_MODEL_c4158b08b68e490ebfd1a62e14381ea8"
          }
        },
        "04d0c3fda9dd4996a395b542781e807c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7364c55f6907405ebeb14c2687f45afd",
            "placeholder": "​",
            "style": "IPY_MODEL_0b56ae4ddcad495e97ec933af623b965",
            "value": "config.json: 100%"
          }
        },
        "bedbcaf04efe42c6829a36c522454f2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_012e350d7e27498eaf5c91f042192ce4",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f4c86ce7e20b47ec802162238eea6a1b",
            "value": 190
          }
        },
        "3e45beca46384e8e93411c37d575dff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7343c5f5c5d3480a9afc52fa07b3e5ed",
            "placeholder": "​",
            "style": "IPY_MODEL_996dc67723584fc0b4bf9bd827334a1c",
            "value": " 190/190 [00:00&lt;00:00, 18.2kB/s]"
          }
        },
        "c4158b08b68e490ebfd1a62e14381ea8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7364c55f6907405ebeb14c2687f45afd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b56ae4ddcad495e97ec933af623b965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "012e350d7e27498eaf5c91f042192ce4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4c86ce7e20b47ec802162238eea6a1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7343c5f5c5d3480a9afc52fa07b3e5ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "996dc67723584fc0b4bf9bd827334a1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a9FvxdOXtdd",
        "outputId": "ccc3f706-d419-4563-c752-feef7925c9dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✅ Setup complete.\n"
          ]
        }
      ],
      "source": [
        "%pip -q install -U spacy networkx langchain-community chromadb sentence-transformers pypdf matplotlib\n",
        "\n",
        "import sys, subprocess\n",
        "subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], check=True)\n",
        "\n",
        "import os, re, json, math, time, random, platform, datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "DATA_DIR = \"./data_week6\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "print(\"✅ Setup complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "\n",
        "corpus_csv_path = os.path.join(DATA_DIR, \"corpus.csv\")\n",
        "\n",
        "def build_corpus_csv_from_files():\n",
        "    CORPUS_DIR = Path(\"corpus\"); CORPUS_DIR.mkdir(exist_ok=True)\n",
        "    files = [p for p in CORPUS_DIR.iterdir() if p.is_file()]\n",
        "\n",
        "    try:\n",
        "        from google.colab import files as colab_files\n",
        "        print(\"Upload PDFs/TXTs (or click 'Cancel' to reuse existing ./corpus):\")\n",
        "        uploaded = colab_files.upload()\n",
        "        for name, data in (uploaded or {}).items():\n",
        "            (CORPUS_DIR / name).write_bytes(data)\n",
        "        files = [p for p in CORPUS_DIR.iterdir() if p.is_file()]\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    docs_text = []\n",
        "    for p in files:\n",
        "        try:\n",
        "            if p.suffix.lower() == \".pdf\":\n",
        "                pages = PyPDFLoader(str(p)).load()\n",
        "                text = \"\\n\".join([pg.page_content or \"\" for pg in pages]).strip()\n",
        "                if text:\n",
        "                    docs_text.append({\"doc_id\": p.stem, \"text\": text})\n",
        "            elif p.suffix.lower() in [\".txt\", \".text\", \".md\"]:\n",
        "                recs = TextLoader(str(p), encoding=\"utf-8\").load()\n",
        "                text = \"\\n\".join([r.page_content or \"\" for r in recs]).strip()\n",
        "                if text:\n",
        "                    docs_text.append({\"doc_id\": p.stem, \"text\": text})\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Could not read {p.name}: {e}\")\n",
        "\n",
        "    if not docs_text:\n",
        "        print(\"No files found; creating demo corpus (professor-style).\")\n",
        "        demo = pd.DataFrame({\n",
        "            \"doc_id\": [f\"doc{i}\" for i in range(1,6)],\n",
        "            \"text\": [\n",
        "                \"Method X was introduced by Author A and compared on Dataset D1 with F1=0.78.\",\n",
        "                \"Author A collaborated with Author B; Method X improved Metric F1 on D1.\",\n",
        "                \"Dataset D2 was used to evaluate Method Y introduced by Author C.\",\n",
        "                \"Paper P3 applies Method X to Dataset D2 and reports Accuracy 0.82.\",\n",
        "                \"Survey S1 links Method Y, Dataset D2, and Metric AUC.\"\n",
        "            ]\n",
        "        })\n",
        "        demo.to_csv(corpus_csv_path, index=False)\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(docs_text)\n",
        "    df.to_csv(corpus_csv_path, index=False)\n",
        "    print(f\"💾 Saved: {corpus_csv_path} with {len(df)} documents.\")\n",
        "\n",
        "if not os.path.exists(corpus_csv_path):\n",
        "    build_corpus_csv_from_files()\n",
        "\n",
        "corpus = pd.read_csv(corpus_csv_path)\n",
        "print(\"✅ Corpus loaded:\", corpus.shape)\n",
        "display(corpus.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "-JMAfxHMYDFN",
        "outputId": "9d48fb8f-0da4-4b9b-8c7f-0d8a5ee8b7fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload PDFs/TXTs (or click 'Cancel' to reuse existing ./corpus):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a90584d4-35e4-43ce-b0c4-3925ac68b3db\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a90584d4-35e4-43ce-b0c4-3925ac68b3db\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving NeurIPS-2024-can-large-language-model-agents-simulate-human-trust-behavior-Paper-Conference.pdf to NeurIPS-2024-can-large-language-model-agents-simulate-human-trust-behavior-Paper-Conference.pdf\n",
            "Saving NeurIPS-2024-richelieu-self-evolving-llm-based-agents-for-ai-diplomacy-Paper-Conference.pdf to NeurIPS-2024-richelieu-self-evolving-llm-based-agents-for-ai-diplomacy-Paper-Conference.pdf\n",
            "Saving NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference.pdf to NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference.pdf\n",
            "💾 Saved: ./data_week6/corpus.csv with 3 documents.\n",
            "✅ Corpus loaded: (3, 2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                              doc_id  \\\n",
              "0  NeurIPS-2024-mdagents-an-adaptive-collaboratio...   \n",
              "1  NeurIPS-2024-richelieu-self-evolving-llm-based...   \n",
              "2  NeurIPS-2024-can-large-language-model-agents-s...   \n",
              "\n",
              "                                                text  \n",
              "0  MDAgents: An Adaptive Collaboration of LLMs fo...  \n",
              "1  Richelieu: Self-Evolving LLM-Based Agents for ...  \n",
              "2  Can Large Language Model Agents Simulate\\nHuma...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4009df5b-4fbe-435e-bb47-7acfec4b3b6a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>MDAgents: An Adaptive Collaboration of LLMs fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NeurIPS-2024-richelieu-self-evolving-llm-based...</td>\n",
              "      <td>Richelieu: Self-Evolving LLM-Based Agents for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NeurIPS-2024-can-large-language-model-agents-s...</td>\n",
              "      <td>Can Large Language Model Agents Simulate\\nHuma...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4009df5b-4fbe-435e-bb47-7acfec4b3b6a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4009df5b-4fbe-435e-bb47-7acfec4b3b6a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4009df5b-4fbe-435e-bb47-7acfec4b3b6a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-238f266d-fbb5-44fe-9e06-4749a1b97af5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-238f266d-fbb5-44fe-9e06-4749a1b97af5')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-238f266d-fbb5-44fe-9e06-4749a1b97af5 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(corpus\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"doc_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference\",\n          \"NeurIPS-2024-richelieu-self-evolving-llm-based-agents-for-ai-diplomacy-Paper-Conference\",\n          \"NeurIPS-2024-can-large-language-model-agents-simulate-human-trust-behavior-Paper-Conference\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"MDAgents: An Adaptive Collaboration of LLMs for\\nMedical Decision-Making\\nYubin Kim1 Chanwoo Park1 Hyewon Jeong1\\u266e Yik Siu Chan1\\nXuhai Xu1 Daniel McDuff2 Hyeonhoon Lee3\\nMarzyeh Ghassemi1 Cynthia Breazeal1 Hae Won Park1\\n1Massachusetts Institute of Technology\\n2Google Research\\n3Seoul National University Hospital\\n{ybkim95,cpark97,hyewonj,yiksiuc,xoxu,mghassem,cynthiab,haewon}@mit.edu\\ndmcduff@google.com\\nhhoon@snu.ac.kr\\nAbstract\\nFoundation models are becoming valuable tools in medicine. Yet despite their\\npromise, the best way to leverage Large Language Models (LLMs) in complex\\nmedical tasks remains an open question. We introduce a novel multi-agent frame-\\nwork, named Medical Decision-making Agents (MDAgents) that helps to address\\nthis gap by automatically assigning a collaboration structure to a team of LLMs.\\nThe assigned solo or group collaboration structure is tailored to the medical task at\\nhand, a simple emulation inspired by the way real-world medical decision-making\\nprocesses are adapted to tasks of different complexities. We evaluate our frame-\\nwork and baseline methods using state-of-the-art LLMs across a suite of real-world\\nmedical knowledge and medical diagnosis benchmarks, including a comparison of\\nLLMs\\u2019 medical complexity classification against human physicians2. MDAgents\\nachieved the best performance in seven out of ten benchmarks on tasks requiring\\nan understanding of medical knowledge and multi-modal reasoning, showing a sig-\\nnificant improvement of up to 4.2%(p < 0.05) compared to previous methods\\u2019 best\\nperformances. Ablation studies reveal that MDAgents effectively determines medi-\\ncal complexity to optimize for efficiency and accuracy across diverse medical tasks.\\nNotably, the combination of moderator review and external medical knowledge in\\ngroup collaboration resulted in an average accuracy improvement of 11.8%. Our\\ncode can be found at https://github.com/mitmedialab/MDAgents.\\n1 Introduction\\nMedical Decision-Making (MDM) is a multifaceted and intricate process in which clinicians collabo-\\nratively navigate diverse sources of information to reach a precise and specific conclusion [97]. For\\ninstance, a primary care physician (PCP) may refer a patient to a specialist when faced with a complex\\ncase, or a patient visiting the emergency department or urgent care might be triaged and then directed\\nto a specialist for further evaluation [5, 54]. MDM involves interpreting complex and multi-modal\\ndata, such as imaging, electronic health records (EHR), physiological signals, and genetic informa-\\ntion, while rapidly integrating new medical research into clinical practice [68, 78]. Recently, Large\\nLanguage Models (LLMs) have shown potential for AI support in MDM [22, 37, 48, 64, 72, 90]. It\\nis known that they are able to process and synthesize large volumes of medical literature [74] and\\nclinical information [1], as well as support probabilistic [94] and causal [39] reasoning, makes LLMs\\npromising tools. However, there is no silver bullet in medical applications that require careful design.\\n\\u266eHyewon Jeong received her MD degree from Yonsei University College of Medicine, South Korea.\\n2Appendix F contains a detailed comparison results between human physicians and LLMs.\\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\\nMedical KnowledgeA 19-year-old Caucasian male presents to your office with hypopigmented skin. He undergoes a skin biopsy and is found to have an absence of melanocytes in the epidermis. Which of the following is the most likely diagnosis? A. Tinea versicolor B. Albinism C. Vitiligo D. Solar lentigo E. Melanoma\\nDifferential Diagnosis (DDx)\\nMulti-modal Reasoning\\nSex: M, Age: 47 Geographical region: North America Pathology: PSVT Symptoms:-I feel pain. The pain is: tugging, Burning \\u2026Differential diagnosis:PSVT: 0.22, Anemia: 0.16,Panic attack: 0.14, Atrial fibrillation: 0.11, Anaphylaxis: 0.11, Cluster headache: 0.09, Chagas: 0.07, Scombroid food poisoning: 0.07, HIV (initial infection): 0.01\\nWhat does the circle in image D surround? A: Abnormal mitotic figures B: Central keratinization C: Frank atypia D: Areas of necrosis \\nMultidisciplinary Team (MDT)\\nLog Ans\\nIntegrated Care Team (ICT)\\nReport\\nPrimary Care Clinician (PCC)\\nPromptingAns\\nCollaborative Discussion\\nLowModerateHigh\\nM-turnsN-rounds\\nMedical Query2. Recruitment1. Complexity Check\\n Team 1\\nTeam 2Report\\nTeam NAnsReport Generation3. Analysis and Synthesis4. Final Decision\\nLLM Checker\\nFigure 1: Medical Decision-Making Agents (MDAgents) framework. Given a medical query from\\ndifferent medical datasets, the framework performs 1) medical complexity check, 2) recruitment, 3)\\nanalysis and synthesis, and 4) decision-making steps.\\nWhile decision-making tools including multi-agent LLMs [ 11, 86] have shown promise in non-\\nmedical domains [31, 32, 44, 46, 62, 65], their evaluation in health applications has been limited.\\nTo date, their \\u201cgeneralist\\u201d design has not effectively integrated the real-world systematic MDM\\nprocess [57] which requires an adaptive, collaborative, and tiered approach. Clinicians consider the\\ncurrent and past history of the patient, available evidence from medical literature, and their domain\\nexpertise and experience [20] for MDM. One example of MDM is to triage patients in emergency\\nroom based on the severity and complexity of their medical conditions [12, 26, 87]. Patients with\\npathognomonic, single uncomplicated acute conditions, or stable chronic conditions that their PCP\\ncould manage [85] could be low complexity cases. On the other hand, patients with injuries that\\ninvolve multiple organs, chronic conditions with side effects, or superimposed diseases who often\\nrequire multiple collaborative discussions (MDT) or sequential consultations (ICT) among specialty\\nphysicians [27, 61] are considered high complexity cases 3.\\nInspired by the way that clinicians make decisions in practice, we propose Medical Decision-making\\nAgents (MDAgents), an adaptive medical decision-making framework that leverages LLMs to\\nemulate the hierarchical diagnosis procedures ranging from individual clinicians to collaborative\\nclinician teams (Figure 1). MDAgents work in three steps: 1) Medical complexity check; 2)\\nRecruitment based on medical complexity; 3) Analysis and synthesis and 4) Final decision-making to\\nreturn the answer. Our contributions are threefold:\\n1. We introduce MDAgents, the first adaptive decision-making framework for LLMs that mirrors\\nreal-world MDM processes via dynamic collaboration among AI agents based on task complexity.\\n2. MDAgents demonstrate superior performance in accuracy over previous solo and group methods\\non 7 out of 10 medical benchmarks, and we show an effective trade-off between performance and\\nefficiency (i.e. the number of API calls) by varying the number of agents.\\n3. We provide rigorous testing under various hyperparameters (e.g. temperatures), demonstrating\\nbetter robustness of MDAgents compared to solo and group methods. Furthermore, our ablations\\nevidence MDAgents\\u2019 ability to find the appropriate complexity level for each MDM instance.\\n2 Related Works\\nLanguage Models in Medical Decision-Making LLMs have shown promise in a range of ap-\\nplications within the medical field [14, 37, 40, 48, 63, 75, 76, 90, 96]. They can answer questions\\nfrom medical exams [43, 52], perform biomedical research [36], clinical risk prediction [37], and\\nclinical diagnosis [55, 67]. Medical LLMs are also evaluated on generative tasks, including creating\\nmedical reports [79], describing medical images [ 81], constructing differentials [53], performing\\ndiagnostic dialogue with patients [77], and generating psychiatric evaluations of interviews [24]. To\\nadvance the capabilities of medical LLMs, two main approaches have been explored: (1) training with\\n3Detailed examples of low-, moderate- and high-complexity cases are provided in Appendix E.1.\\n2\\nTable 1: Comparison between our framework and previous methods (Solo and Group). Among these\\nworks, MDAgents is the only one to perform all key dimensions of LLM decision-making.\\nMethod MDAgents\\n(Ours) Single Voting [82] Debate [17] MedAgents [72] ReConcile [10]\\nInteraction Type\\nMultiple Roles \\u2713 \\u2717 \\u2713 \\u2713 \\u2713 \\u2713\\nEarly Stopping \\u2713 \\u2717 \\u2713 \\u2713 \\u2713 \\u2717\\nRefinement \\u2713 \\u2717 \\u2717 \\u2713 \\u2713 \\u2717\\nComplexity Check \\u2713 \\u2717 \\u2717 \\u2717 \\u2717 \\u2717\\nMulti-party Chat \\u2713 \\u2717 \\u2717 \\u2713 \\u2717 \\u2717\\nConversation Pattern Flexible Static Static Static Static Static\\ndomain-specific data [28], and (2) applying inference-time techniques such as prompt engineering\\n[67] and Retrieval Augmented Generation (RAG) [92]. While initial research has been concentrated\\non pre-training and fine-tuning with medical knowledge, the rise of large general-purpose LLMs has\\nenabled training-free methods where models leverage their latent medical knowledge. For example,\\nGPT-4 [60], with richer prompt crafting, surpasses the passing score on USMLE by over 20 points and\\nwith prompt tuning can outperform fine-tuned models including Med-PaLM [58, 59]. The promise\\nof general-purpose models has thus inspired various techniques such as Medprompt and ensemble\\nrefinement to improve LLM reasoning [ 67], as well as RAG tools that use external resources to\\nimprove the factuality and completeness of LLM responses [ 38, 92]. Frameworks like MEDIQ\\n[49] and UoT [33] advance LLM reliability in clinical settings by enhancing information-seeking\\nthrough adaptive question-asking and uncertainty reduction, supporting more realistic diagnostic\\nprocesses. Our approach leverages these techniques and the capabilities of general-purpose models\\nwhile acknowledging that a solitary LLM [37, 48, 90] may not fully encapsulate the collaborative\\nand multidisciplinary nature of real-world MDM. We thus emphasize joinging multiple expert LLMs\\nfor effective collaboration in order to solve complicated medical tasks with greater accuracy.\\nMulti-Agent Collaboration An array of studies have explored effective collaboration frameworks\\nbetween multiple LLM agents [47, 86] to enhance capability above and beyond an individual LLM\\n[80]. A common framework is role-playing, where each agent adopts a specific role (e.g. an Assistant\\nAgent or a Manager Agent), a task is then broken down into sub-steps and solved collaboratively\\n[47, 86]. While role-playing focuses on collaboration and multi-step problem-solving [88], another\\nframework, \\u201cmulti-agent debate\\u201d, prompts each agent to solve the task independently [17]. Then, they\\nreason through other agents\\u2019 answers to converge on a shared response, this approach can improve\\nthe factuality, mathematical ability and reasoning capabilities of the multi-agent solution [17, 50].\\nSimilar frameworks include voting [82], multi-disciplinary collaboration [ 72], group discussions\\n(ReConcile [10]), and negotiating [ 23]. Table 1 compares existing setups across key dimensions\\nin multi-agent interaction. Although these frameworks have shown improvement in the respective\\ntasks, they rely on a pre-determined number of agents and interaction settings. When applied on a\\nwider variety of tasks , this static architecture may lead to suboptimal multi-agent configurations,\\nnegatively impacting performance [51]. Furthermore, multi-agent approaches run the risk of being\\ncomputationally inefficient or expensive to employ and need to justify these costs with noticable\\nperformance gains [ 17]. Given that different models and frameworks could generalize better to\\ndifferent tasks [93], we propose a framework that dynamically assigns the optimal collaboration\\nstrategy at inference time based on the complexity of the query. We apply our strategy to MDM, a\\ntask that requires teamwork and should benefit from multi-agent collaboration [72].\\n3 MDAgents: Medical Decision-making Agents\\nThe design of MDAgents (Figures 1 and 2) incorporates four stages: 1) Medical Complexity Check\\n- The system evaluates the medical query, categorizing it as low, moderate, or high complexity\\nbased on clinical decision-making techniques [6, 7, 21, 71, 84]. 2) Expert Recruitment - Based on\\ncomplexity, the framework activates a single Primary Care Clinician (PCC) for low complexity issues,\\nor a Multi-disciplinary Team (MDT) or Integrated Care Team (ICT) for moderate or high complexities\\n[7, 18, 21, 34, 45, 71]. 3) Analysis and Synthesis - Solo queries use prompting techniques like\\nChain-of-Thought (CoT) and Self-Consistency (SC). MDTs involve multiple LLM agents forming a\\nconsensus, while ICTs synthesize information for the most complex cases. 4) Decision-making -\\nThe final stage synthesizes all inputs to provide a well-informed answer to the medical query.\\n3\\nQuestion: What does the image of Gross Sectioned Myelolipoma demonstrate?Step 2. Expert Recruitment\\n(A) fibrosis(B) inflammation(C) hemorrhage(D) calcification\\nStep 3. Initial Assessment\\n(1) Answer: (A) fibrosis\\n(2) Answer: (A) fibrosis\\n(3) Answer:(C) hemorrhage\\n(4) Answer:(C) hemorrhage\\n(5) Answer: (A) fibrosis\\nStep 4. Collaborative Discussion\\nStep 1. Complexity CheckLowModerate     HighReason: While the identification of gross pathology might seem straightforward to a specialist in pathology, the distinction between fibrosis, hemorrhage, and other conditions can sometimes be subtle and subject to interpretation \\u2026\\nTurn 1Turn 2Turn \\ud835\\udc95\\n Turn \\ud835\\udc7bRound \\ud835\\udc93\\t\\u2208{\\ud835\\udfcf,\\ud835\\udfd0,\\u2026,\\ud835\\udc79}: Communication channel\\nif          :            Terminate\\n(\\u2026) while the expertise of Agent 1 (Pathologist     ) in tissue examination is crucial and their argument for (A) fibrosis is noted, the gross pathological features described by Agent 3 (Surgeon      ) and Agent 4 (Oncologist      ) seem to align more closely with the typical presentation of a hemorrhage. Furthermore, hemorrhage is a well-documented complication in myelolipomas, particularly larger ones or those subjected to trauma.Therefore, based on the analysis of the conversation history, the final decision is aligned with interpretation (C) hemorrhage (red arrow)as argued by Agent 3 (Surgeon      ) in Round 1, Turns 1 and 3, and Agent 4 (Oncologist      ) in Round 1, Turn 2. This conclusion is supported by the gross pathological features consistent with hemorrhage rather than fibrosis, despite the reasoned opinions for fibrosis by Agents 1 (Pathologist  ), 2 (Radiologist      ), and 5 (Endocrinologist      ).\\nStep 5. Review and Final Decision\\nGP Moderator\\nPathologist(1)\\nOncologist(4)\\nEndocrinologist(5)\\nSurgeon(3)\\nRadiologist(2)GP Moderator\\n\\ud83e\\udef1\\n\\ud83e\\udef2\\n\\ud835\\udc93\\nFigure 2: Illustrative example of MDAgents in a moderate complexity case from the PMC-VQA\\ndataset. More detailed case studies can be found in Figure 11 and 12 in the Appendix.\\n3.1 Agent Roles\\nModerator. The moderator agent functions as a general practitioner (GP) or emergency department\\ndoctor who first triages the medical query. This agent assesses the complexity of the problem and\\ndetermines whether it should be handled by a single agent, a MDT, or an ICT. The moderator\\nensures the appropriate pathway be selected based on the query\\u2019s complexity and oversees the entire\\ndecision-making process.\\nRecruiter. The recruiter agent is responsible for assembling the appropriate team of specialist\\nagents based on the complexity assessment of the moderator. The recruiter may assign a single\\nPCP agent for low-complexity cases, while MDT or ICT with relevant expertise will be formed for\\nmoderate and high-complexity cases.\\nGeneral Doctor/Specialist. These agents are domain-specific or general physicians recruited by\\nthe recruiter agent. Depending on the complexity of the case, they may work independently or as part\\nof a team. General physicians handle less complex, routine cases, whereas specialists are recruited\\nfor their specific expertise in more complex scenarios. These agents engage in the collaborative\\ndecision-making process, contributing their specialized knowledge to reach a consensus or provide\\ndetailed reports for high-complexity cases.\\n3.2 Medical Complexity Classification (Line 1 of Algorithm 1, Appendix C.2)\\nThe first step in the MDAgents framework is to determine the complexity of a given medical query q\\nby the moderator LLM which functions as a generalist practitioner (GP). The moderator aims to\\nact as a classifier to return the complexity level of the given medical query, it is provided with the\\ninformation on how medical complexity should be defined and is instructed to classify the query into\\none of three different complexity levels:\\n1. Low - Simple, well-defined medical issues that can be resolved by a single PCP agent. These\\ntypically include common, acute illnesses or stable chronic conditions where the medical needs\\nare predictable and require minimal interdisciplinary coordination.\\n2. Moderate - The medical issues involve multiple interacting factors, necessitating a collaborative\\napproach among an MDT. These scenarios require integration of diverse medical knowledge areas\\nand coordination between specialists through consultation to develop effective care strategies.\\n3. High - Complex medical scenarios that demand extensive coordination and combined expertise\\nfrom an ICT. These cases often involve multiple chronic conditions, complicated surgical or trauma\\ncases, and decision-makings that integrates specialists from different healthcare departments.\\n3.3 Expert Recruitment (Line 3, 7, 17 of Algorithm 1)\\nGiven a medical query, the goal of therecruiter LLM is to enlist domain experts as individuals, in\\ngroups, or as multiple teams, based on the complexity levels determined by the moderator LLM.\\nSpecifically, we assign medical expertise and roles to multiple LLMs, instructing them to either act\\nindependently as solo medical agents or collaborate with other medical experts in a team. In Figure 9\\nin the Appendix, we also provide frequently recruited agents for each benchmark as a reference.\\n4\\n3.4 Medical Collaboration and Refinement\\nThe initial assessment protocol of our decision-making framework categorizes query complexity into\\nlow, moderate, and high. This categorization is grounded in established medical constructs such as\\nacuity [25] for straightforward cases, comorbidity [69] and case management complexity [13] for\\nintermediate and multi-disciplinary care requirements, and severity of illness [16] for high complexity\\ncases requiring comprehensive management. We outlines the specific refinement approach:\\nLow - Straightforward cases (Line 2-4 of Algorithm 1). For queries classified under Low com-\\nplexity, characterized by straightforward clinical decision pathways, a single PCP agent (Figure 10\\n(a)) is deployed by the definition in Section 3.2. The domain expert who is recruited by recuriter\\nLLM, applies few-shot prompting to the problem. The output answer, denoted as ans, is directly\\nobtained from the agent\\u2019s response to Q without the need for iterative refinement, formalized as\\nans = Agent(Q), with Agent representing the engaged PCP agent.\\nModerate - Intermediate complexity cases (Line 6-14 of Algorithm 1). In addressing more\\ncomplex queries, the utilization of an MDT (Figure 10 (b) and (c)) approach has been increasingly\\nrecognized for its effectiveness in producing comprehensive and nuanced solutions [45]. The MDT\\nframework leverages the collective expertise of professionals from diverse disciplines, facilitating\\na holistic examination of the query at hand. This collaborative method is particularly advanta-\\ngeous in scenarios where the complexity of a problem transcends the scope of a single domain,\\nnecessitating a fusion of insights from various specialties [ 7, 71]. The MDT approach not only\\nenhances decision-making quality through the integration of multidimensional perspectives but also\\nsignificantly improves the adaptability and efficiency of the problem-solving process [21].\\nBuilding upon this foundation, our framework specifically addresses queries of moderate complexity\\nthrough a structured, multi-tiered collaborative approach. An MDT recruited by recruiter LLM (see\\nFigure 10 in Appendix) starts an iterative discussion process aimed at reaching a consensus with at\\nmost R rounds (Line 10-12). For each round r \\u2208 R, agents Ai, i\\u2208 1, . . . , Nindicate participation\\nand preferred interlocutors. The system facilitates message exchanges for T turns. If consensus is not\\nreached and agents agree to continue, a new round begins with access to previous conversations. For\\nevery round, consensus within the MDT is determined by parsing and comparing their opinions. In\\nthe event of a disagreement, the moderator agent, consistent with the one described in Section 3.2\\nreviews the MDT\\u2019s discourse and formulates feedback for each agent.\\nHigh - Complex care cases (Line 17-24 of Algorithm 1). In contrast to the MDT approach, the ICT\\n(Figure 10 (d)) paradigm is essential for addressing the highest tier of query complexity in healthcare.\\nThis structured progression through the ICT ensures a depth of analysis that is specialized and focused\\nat each stage of the decision-making process. Beginning with the Initial Assessment Team, moving\\nthrough various diagnostic teams, and culminating with the Final Review & Decision Team, our ICT\\nmodel aligns specialist insights into a cohesive narrative that informs the ultimate decision (Appendix\\nAlgorithms 1 Lines 19-21). A key component of this process is the report generation process described\\nin Appendix with the prompt, where each team, led by a lead clinician, collaboratively produces\\na comprehensive report synthesizing their findings. This phased approach, supported by evidence\\nfrom recent healthcare studies, has been shown to enhance the precision of clinical decision-making,\\nas each team builds upon the foundation laid by the previous, ensuring a meticulous and refined\\nexamination of complex medical cases [34]. The resultant reports, accumulating throughout the ICT\\nprocess, are not only reflective of comprehensive medical evaluations but also of a systematic and\\nlayered analysis that is critical in the management of intricate health scenarios [18].\\n3.5 Decision-making\\nIn the final stage of our framework, the decision-maker agent synthesizes the diverse inputs generated\\nthroughout the decision-making process to arrive at a well-informed final answer to the medical query\\nq. This synthesis involves several components depending on the complexity level of the query:\\n1. Low: Directly utilizes the initial response from the primary decision-making agent.\\n2. Moderate: Incorporates the conversation history ( Interaction) between the recruited agents to\\nunderstand the nuances and disagreements in their responses.\\n3. High: Considers detailed reports (Reports) generated by the agents, which include comprehensive\\nanalyses and justifications for their diagnostic suggestions.\\nThe decision-making process is formulated as ans = Agent(\\u00b7) where the final answer, ans is deter-\\nmined by integrating the outputs from analysis and synthesis step based on its medical complexities.\\nThis integration employs ensemble techniques such as temperature ensembles to ensure the decision\\nis robust and reflects a consensus among the models when applicable (see Appendix C.2 for details).\\n5\\nTable 2: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive setting. Bold represents\\nthe best and Underlined represents the second best performance for each benchmark and model. All\\nbenchmarks except for MedVidQA were evaluated with GPT-4(V) and MedVidQA was evaluated\\nwith Gemini-Pro(Vision). Full experimental results with other models are listed in Table 9-12 in\\nAppendix.\\nCategory Method\\nMedical Knowledge Retrieval Datasets\\nMedQA\\nT\\nPubMedQA\\nT\\nPath-VQA\\nI T\\nPMC-VQA\\nI T\\nMedVidQA\\nV T\\nSingle-agent\\nZero-shot 75.0 \\u00b11.3 61.5 \\u00b12.2 57.9 \\u00b11.6 49.0 \\u00b13.7 37.9 \\u00b18.4\\nFew-shot 72.9 \\u00b111.4 63.1 \\u00b111.7 57.5 \\u00b14.5 52.2 \\u00b12.0 47.1 \\u00b18.6\\n+ CoT [83] 82.5 \\u00b14.9 57.6 \\u00b19.2 58.6 \\u00b13.1 51.3 \\u00b11.5 48.6 \\u00b15.5\\n+ CoT-SC [82] 83.9 \\u00b12.7 58.7 \\u00b15.0 61.2 \\u00b12.1 50.5 \\u00b15.2 49.2 \\u00b18.2\\nER [67] 81.9 \\u00b12.1 56.0 \\u00b17.0 61.4 \\u00b14.1 52.7 \\u00b12.9 48.5 \\u00b14.1\\nMedprompt [59] 82.4 \\u00b15.1 51.8 \\u00b14.6 59.2 \\u00b15.7 53.4 \\u00b17.9 44.5 \\u00b12.0\\nMulti-agent\\n(Single-model)\\nMajority V oting 80.6 \\u00b12.9 72.2 \\u00b16.9 56.9 \\u00b119.7 36.8 \\u00b16.7 50.8 \\u00b17.4\\nWeighted V oting 78.8 \\u00b11.1 72.2 \\u00b16.9 62.1 \\u00b113.9 25.4 \\u00b19.0 57.8 \\u00b12.1\\nBorda Count 70.3 \\u00b18.5 66.9 \\u00b13.0 61.9 \\u00b18.1 27.9 \\u00b15.3 54.5 \\u00b14.7\\nMedAgents [72] 79.1 \\u00b17.4 69.7 \\u00b14.7 45.4 \\u00b18.1 39.6 \\u00b13.0 51.6 \\u00b14.8\\nMeta-Prompting [70] 80.6 \\u00b11.2 73.3 \\u00b12.3 55.3 \\u00b12.3 42.6 \\u00b14.2 -\\nMulti-agent\\n(Multi-model)\\nReconcile [10] 81.3 \\u00b13.0 79.7 \\u00b13.2 57.5 \\u00b13.3 31.4 \\u00b11.2 -\\nAutoGen [86] 60.6 \\u00b15.0 77.3 \\u00b12.3 43.0 \\u00b18.9 37.3 \\u00b16.1 -\\nDyLAN [51] 64.2 \\u00b12.3 73.6 \\u00b14.2 41.3 \\u00b11.2 34.0 \\u00b13.5 -\\nAdaptive MDAgents (Ours) 88.7\\u00b14.0 75.0 \\u00b11.0 65.3 \\u00b13.9 56.4 \\u00b14.5 56.2 \\u00b16.7\\nCategory Method\\nClinical Reasoning and Diagnostic Datasets\\nDDXPlus\\nT\\nSymCat\\nT\\nJAMA\\nT\\nMedBullets\\nT\\nMIMIC-CXR\\nI T\\nSingle-agent\\nZero-shot 70.3 \\u00b12.0 88.7 \\u00b12.3 62.0 \\u00b12.0 67.0 \\u00b11.4 40.0 \\u00b15.3\\nFew-shot 69.4 \\u00b11.0 86.7 \\u00b13.1 69.0 \\u00b14.2 72.0 \\u00b12.8 35.3 \\u00b15.0\\n+ CoT [83] 72.7 \\u00b17.7 78.0 \\u00b12.0 66.0 \\u00b15.7 70.0 \\u00b10.0 36.2 \\u00b15.2\\n+ CoT-SC [82] 52.1 \\u00b16.4 83.3 \\u00b13.1 68.0 \\u00b12.8 76.0 \\u00b12.8 51.7 \\u00b14.0\\nER [67] 61.3 \\u00b12.4 82.7 \\u00b12.3 71.0 \\u00b11.4 76.0 \\u00b15.7 50.0 \\u00b10.0\\nMedprompt [59] 59.5 \\u00b117.7 87.3 \\u00b11.2 70.7 \\u00b14.3 71.0 \\u00b11.4 53.4 \\u00b14.3\\nMulti-agent\\n(Single-model)\\nMajority V oting 67.8 \\u00b14.9 91.9 \\u00b12.2 70.0 \\u00b15.7 70.0 \\u00b10.0 49.5 \\u00b110.7\\nWeighted V oting 65.9 \\u00b13.3 90.5 \\u00b12.9 66.1 \\u00b14.1 66.0 \\u00b15.7 53.5 \\u00b12.2\\nBorda Count 67.1 \\u00b16.7 78.0 \\u00b111.8 61.0 \\u00b15.6 66.0 \\u00b12.8 45.3 \\u00b16.8\\nMedAgents [72] 62.8 \\u00b15.6 90.0 \\u00b10.0 66.0 \\u00b15.7 77.0 \\u00b11.4 43.3 \\u00b17.0\\nMeta-Prompting [70] 52.6 \\u00b16.1 77.3 \\u00b12.3 64.7 \\u00b13.1 49.3 \\u00b11.2 42.0 \\u00b14.0\\nMulti-agent\\n(Multi-model)\\nReconcile [10] 68.4 \\u00b17.4 90.6 \\u00b12.5 60.7 \\u00b15.7 59.5 \\u00b18.7 33.3 \\u00b13.4\\nAutoGen [86] 67.3 \\u00b111.8 73.3 \\u00b13.1 64.6 \\u00b11.2 55.3 \\u00b13.1 43.3 \\u00b14.2\\nDyLAN [51] 56.4 \\u00b12.9 75.3 \\u00b14.6 60.1 \\u00b13.1 57.3 \\u00b16.1 38.7 \\u00b11.2\\nAdaptive MDAgents (Ours) 77.9 \\u00b12.1 93.1 \\u00b11.0 70.9 \\u00b10.3 80.8 \\u00b11.7 55.9 \\u00b19.1\\n* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement\\n* T : text-only, I : image+text, V : video+text\\n* All experiments were tested with 3 random seeds\\n4 Experiments and Results\\nIn this section, we evaluate our framework and baseline methods across different medical benchmarks\\nin Solo, Group, and Aaptive settings. Our experiments and ablation studies highlight the framework\\u2019s\\nperformance, demonstrating robustness and efficiency by modulating agent numbers and temperatures.\\nResults also show a beneficial convergence of agent opinions in collaborative settings.\\n4.1 Setup\\nTo verify the effectiveness of our framework, we conduct comprehensive experiments with baseline\\nmethods on ten datasets including MedQA, PubMedQA, DDXPlus, SymCat, JAMA, MedBullets,\\nPath-VQA, PMC-VQA, MIMIC-CXR and MedVidQA. A detailed explanation and statistics for each\\ndataset are deferred to Appendix A and Figure 8. We use 50 samples per dataset for testing, and the\\ninference time for each complexity was: low - 14.7s, moderate - 95.5s, and high - 226s in average.\\nFor all the quantitative experiments in this section, we compare three settings: (1)Solo: Using a single\\nLLM agent in the decision-making state. (2) Group: Implementing multi-agents to collaborate during\\nthe decision-making process. (3) Adaptive: Our proposed method MDAgents, adaptively constructs\\nthe inference structure from PCP to MDT and ICT. We use 3-shot prompting for low-complexity\\ncases and zero-shot prompting for moderate and high-complexity cases across all settings.\\nMedical Question Answering With MedQA [35], PubMedQA [36], MedBullets [9], and JAMA\\n[9], we focus on question answering through text, involving both literature-based and conceptual\\nmedical knowledge questions. Specifically, PubMedQA tasks models to answer questions using\\nabstracts from PubMed, requiring synthesis of biomedical information. MedQA tests the model\\u2019s\\nability to understand and respond to multiple-choice questions derived from medical educational\\nmaterials and examinations. MedBullets provides USMLE Step 2/3 type questions that demand\\n6\\nthe application of medical knowledge and clinical reasoning. JAMA Clinical Challenge presents\\nchallenging real-world clinical cases with diagnosis or treatment decision-making questions, testing\\nthe model\\u2019s clinical reasoning (Figure 8 in Appendix shows complexity distribution for each dataset)\\nDiagnostic Reasoning DDXPlus [ 73] and SymCat [ 2] involve clinical vignettes that require\\ndifferential diagnosis, closely mimicking the diagnostic process of physicians. These tasks test the\\nmodel\\u2019s ability to reason through symptoms and clinical data to suggest possible medical conditions,\\nevaluating the AI\\u2019s diagnostic reasoning abilities similar to a clinical setting. SymCat [ 2] uses\\nsynthetic patient records constructed from a public disease-symptom data source and is enhanced\\nwith additional contextual information through the NLICE method.\\nMedical Visual Interpretation Path-VQA [30], PMC-VQA [95], MedVidQA [29], and MIMIC-\\nCXR [3] challenge models to interpret medical images and videos, requiring integration of visual\\ndata with clinical knowledge. PathVQA focuses on answering questions based on pathology images,\\ntesting AI\\u2019s capability to interpret complex visual information from medical images. PMC-VQA\\nevaluates AI\\u2019s proficiency in deriving answers from both text and images found in scientific publi-\\ncations. MedVidQA extends to video-based content, where AI models need to process information\\nfrom medical procedure videos. MIMIC-CXR-VQA specifically targets chest radiographs, utilizing a\\ndiverse and large-scale dataset designed for visual question-answering tasks in the medical domain.\\nBaseline Methods\\n\\u2022 Solo: The baseline methods considered for the Solo setting include the following: Zero-shot [41]\\ndirectly incorporates a prompt to facilitate inference, while Few-shot [8] involves a small number of\\nexamples. Few-shot CoT [83] integrates rationales before deducing the answer. Few-shot CoT-SC\\n[82] builds upon Few-shot CoT by sampling multiple chains to yield the majority answer. Ensemble\\nRefinement (ER) [67] is a prompting strategy that conditions model responses on multiple reasoning\\npaths to bolster the reasoning capabilities of LLMs. Medprompt [59] is a composition of several\\nprompting strategies that enhances the performance of LLMs and achieves state-of-the-art results\\non multiple benchmark datasets, including medical and non-medical domains.\\n\\u2022 Group: We tested five group decision-making methods: V oting [82], MedAgents [72], Reconcile\\n[10], AutoGen [86], and DyLAN [ 51]. Autogen was based on four agents, with one User, one\\nClinician, one Medical Expert, and one Moderator, with one response per agent [86]. DyLAN setup\\nfollowed the base implementations of four agents with no specific roles and four maximum rounds\\nof interaction [51]. While the methods support multiple models, GPT-4 was used for all agents.\\n4.2 Results\\nIn Table 2, we report the classification accuracy on MedQA, PubMedQA, DDXPlus, SymCat, JAMA,\\nMedBullets, Path-VQA, PMC-VQA and MedVidQA dataset. We compare our method (Adaptive)\\nwith several baselines in both Solo and Group settings.\\nAdaptive method outperforms Solo and Group settings. As depicted in Figure 4 and Table 2,\\nMDAgents significantly outperforms (p < 0.05) both Solo and Group setting methods, showing best\\nperformance in 7 out of 10 medical benchmarks tested. This reveals the effectiveness of adaptive\\nstrategies integrated within our system, particularly when navigating through the text-only (e.g.,\\nDDXPlus where it outperformed the best performance of single-agent by 5.2% and multi-agent by\\n9.5%) and text-image datasets (e.g., Path-VQA, PMC-VQA and MIMIC-CXR). Our approach not\\nonly comprehends textual information with high precision but also adeptly synthesizes visual data, a\\npivotal capability in medical diagnostic evaluations.\\n(a) (b) (c) (d)\\nLow Moderate High\\nFigure 3: Experiment with the MedQA dataset ( N=25 randomly sampled questions). (a) LLM\\u2019s\\ncapability to classify complexity. (b-d) Evaluating 25 medical problems by solving each one 10 times\\nat various complexity levels. The x-axis represents the accuracy achieved for each problem, while the\\ny-axis shows the number of problems that reached that level of accuracy.\\n7\\nWhy Do Adaptive Decision-making Framework Work Well? It is important to accurately assign\\ndifficulty levels to medical questions. For instance, if a medical question is obviously easy, utilizing a\\nteam of specialists (such as an IDT) might be excessive and potentially lead to overly pessimistic\\napproaches. Conversely, if a difficult medical question is only tackled by a PCP, the problem might\\nnot be adequately addressed. The core issue here is the LLM\\u2019s capability to classify the difficulty of\\nmedical questions appropriately. If an LLM inaccurately classifies the difficulty level, the chosen\\nmedical solution may not be suitable, potentially leading to the wrong decision making. Therefore,\\nunderstanding what constitutes an appropriate difficulty level is essential.\\nWe hypothesize that an LLM, functioning as a classifier, will select the optimal complexity level\\nfor each MDM problem. This hypothesis is supported by Figure 3, which illustrates that the model\\nappropriately matches the complexity levels; low, moderate, and high of the given problem. To\\ndetermine this, we assessed the accuracy of solutions across various difficulty levels. Specifically, we\\nevaluated 25 medical problems by repeating each problem for 10 times at each difficulty level. By\\nmeasuring the success rate, we aimed to identify the difficulty level that yielded the highest accuracy.\\nThis approach ensures that the LLM\\u2019s complexity classification aligns with the most effective and\\naccurate medical solutions, thereby optimizing the application of medical expertise to each question.\\nFormally, for any given problemP, we denote the probability that the correct answer can be solved\\nat a specific complexity level as pcomplexity-level(P), where complexity-level \\u2208 {low, moderate, high}.\\narg max(P) \\u2208 {low, moderate, high} refers to the complexity level that has the highest proba-\\nbility among plow(P), pmoderate(P), and phigh(P). Similarly, arg min(P) is the complexity level\\nwith the lowest probability, and arg mid(P) is the one with the middle probability. We denote\\na, b, and c as the probabilities that the LLM selects the complexity levels corresponding to\\narg max, arg mid, and arg min, respectively. Thus, the accuracy of our system for problem P\\ncan be described by a \\u00b7 parg max(P) +b \\u00b7 parg mid(P) +c \\u00b7 parg min(P), and the overall accuracy is\\ngiven by EP [a \\u00b7 parg max(P) +b \\u00b7 parg mid(P) +c \\u00b7 parg min(P)]. The estimated values of a, b, care\\na = 0.81 \\u00b1 0.29, b = 0.11 \\u00b1 0.28, and c = 0.08 \\u00b1 0.16, which indicates that LLM can provide an\\noptimal complexity level with probability at least 80%.\\nThese findings suggest that a classifier LLM can implicitly simulate various complexity levels and\\noptimally adapt to the complexity required for each medical problem, as shown in Figure 3. This\\nability to adjust complexity dynamically proves to be crucial for applying LLMs effectively in MDM\\ncontexts as shown by the competitiveness of our Adaptive approach.\\nSoloGroupOurs\\nAccuracy\\n**\\nOursSoloGroup\\nns\\nFigure 4: Our method outperforms Solo and Group\\nsettings across different medical benchmarks.\\nSolo vs. Group Setting in MDM. The ex-\\nperimental results reveal distinct performance\\npatterns between Solo and Group settings across\\nvarious medical benchmarks. In simpler datasets\\nlike MedQA, solo methods, leveraging Few-shot\\nCoT and CoT-SC, achieved up to 83.9% accu-\\nracy compared to the group\\u2019s best of 81.3%.\\nConversely, for more complex datasets like Sym-\\nCat, group settings perform better, with SymCat\\nshowing 91.9% accuracy in the group settings\\nversus 88.7% in solo settings. Notably, group\\nsettings (e.g. Weighted V oting, Reconcile) per-\\nformed better in multi-modal datasets such as\\nPath-VQA (image + text), MedVidQA ( video\\n+ text), and MIMIC-CXR (image + text), high-\\nlighting the advantage of collaborative process in complex tasks. This result aligns with findings from\\n[4], which showed that pooled diagnoses from groups of physicians significantly outperformed those\\nof individual physicians, with accuracy increasing as group size increased. Overall, solo settings\\noutperformed group settings in four benchmarks, while group settings outperformed solo in six\\nbenchmarks. These results reveals that while solo methods excel in straightforward tasks, group\\nsettings provide better performance in complex, multi-faceted tasks requiring diverse expertise.\\n4.3 Ablation Studies\\nImpact of Complexity Selection. We evaluate the importance of the complexity assessment and\\nadaptive process through ablation studies (Figure 5). Our adaptive method significantly outperforms\\nstatic complexity assignments across different modality benchmarks. For text-only queries, the\\nAdaptive method achieves an accuracy of 81.2%, significantly higher thanlow (64.2%), moderate\\n(71.6%), and high (65.8%) settings. Interestingly, 64% of the text-only queries were classified as\\n8\\nhigh complexity, indicating that many text-based queries required in-depth analysis with different\\nexpertise. In the image + text modality, the Adaptive method classified 55% of the queries as low\\ncomplexity, suggesting that the visual information often provides clear and straightforward cues that\\nsimplify the decision-making process. Finally, for video + text queries, 87% of these queries were\\nclassified as low complexity, reflecting that the dynamic visual data in conjunction with text can often\\nbe straightforwardly interpreted. However, further evaluation on more challenging video medical\\ndatasets is needed, as MedVidQA contains relatively less complex medical knowledge.\\nMethod Avg. Accuracy (%)\\nMDAgents (Ours) 71.8\\n+ MedRAG 75.2 ( \\u2191 4.7 %)\\n+ Moderator\\u2019s Review 77.6 ( \\u2191 8.1 %)\\n+ Moderator\\u2019s Review & MedRAG 80.3 ( \\u2191 11.8 %)\\nTable 3: Ablations for the impact of moderator\\u2019s review and\\nMedRAG. The Accuracy were averaged accuracy across all datasets.\\nImpact of Moderator\\u2019s Re-\\nview and RAG Table 3 ex-\\namines the impact of incorpo-\\nrating external medical knowl-\\nedge and moderator reviews\\ninto the MDAgents framework\\non accuracy. MedRAG [ 89]\\nis a systematic toolkit for\\nRetrieval-Augmented Genera-\\ntion (RAG) that leverages various corpora; biomedical, clinical and general medicine, to provide\\ncomprehensive knowledge. The baseline accuracy of MDAgents is 71.8%. Integrating MedRAG\\nincreases accuracy to 75.2% (up 4.7%), while the moderator\\u2019s review alone raises it to 77.6% (up\\n8.1%). The combined use of both methods achieves the highest accuracy at 80.3% (up 11.8%).\\nThe results indicate that MedRAG and moderator review both enhance performance, with their\\ncombined effect being synergistic. This highlights that leveraging recent external knowledge and\\nstructured feedback mechanisms is crucial for refining and converging on accurate medical decisions.\\nThis improvement underscores the importance of a hybrid strategy, aligning with real-world practices\\nof continuous learning and expert consultation to optimize performance in medical applications.\\n4.4 Impact of Number of Agents in Group Setting.\\nOur experiment with varying the number of agents in a collaborative Group setting (Appendix Figure\\n6 (a-b)) shows that a higher number of agents does not lead to better performance. Rather, our\\nAdaptive method achieves optimal performance with fewer agents (N=3, peak accuracy of 83.5%)\\nby intelligently calibrating the number of collaborating agents. This not only indicates efficiency in\\ndecision-making but also computational and economic benefits, considering the reduced number of\\nAPI calls needed, especially when contrasted with the Solo and Group settings.\\nWith regards to computational efficiency, the Solo setting (5-shot CoT-SC) resulted in a 6.0 and\\nGroup setting (MedAgents with N=5) resulted in a 20.3 API calls, suggesting a high computational\\ncost without a corresponding increase in accuracy. On the other hand, our Adaptive method exhibits a\\nmore economical use of resources, demonstrated by fewer API calls (9.3 withN=3) while maintaining\\nhigh accuracy, a critical factor in deploying scalable and cost-effective medical AI solutions.\\n4.5 Robustness of MDAgents with different parameters.\\nOur Adaptive approach shows resilience to changes in temperature (Appendix Figure 6 (c), low\\n(T=0.3) and high (T=1.2)) with performance improving under higher temperatures. This suggests\\nthat our model can utilize the creative and diverse outputs generated at higher temperatures to\\nenhance decision-making, a property that is not as pronounced in the Solo and Group conditions.\\nThis robustness is particularly valuable in real-world medical domains with high uncertainty and\\nambiguity in datasets [ 15]. Additionally, studies have shown that creative diagnostic approaches\\nAa\\ntext-only video + text\\nimage + textAa Aa\\nFigure 5: Impact of complexity selection of the query. Accuracy of each ablation on text-only (left),\\ntext+image (center) and text+video (right) benchmarks are reported.\\n9\\n(a) (b) (c)\\nFigure 6: Impact of the number of agents on (a) Accuracy, (b) Number of API Calls on medical\\nbenchmarks with GPT-4 (V) and (c) Performance of three different settings under low (T=0.3) and\\nhigh (T=1.2) temperatures on medical benchmarks. Our Adaptive setting shows better robustness to\\ndifferent temperatures and even takes advantage of higher temperatures.\\n: text-only: image +text: video +text\\nFigure 7: An illustration of consensus entropy in group collaboration process of MDAgents (w/\\nGemini-Pro (Vision), N=30 for each dataset) on medical benchmarks with different modality inputs.\\ncan mitigate cognitive biases and improve diagnostic accuracy [66], while fostering flexibility and\\nadaptability in decision-making [19]. These insights support the enhanced performance observed\\nunder higher temperatures in our framework. However, the future work should explore a wider range\\nof temperatures to fully understand the robustness and adaptability of our approach.\\n4.6 Convergence Trends in Consensus Dynamics\\nThere is clear trend towards consensus among MDAgents cross various data modalities (Figure 7).\\nThe text+video modality demonstrates a rapid convergence, reflecting the agents\\u2019 efficient processing\\nof combined textual and visual cues. On the other hand, the text+image and text-only modalities\\ndisplay a more gradual decline in entropy, indicating a progressive narrowing of interpretative\\ndiversity among the agents. Despite the differing rates and initial conditions, all modalities exhibit\\nconvergence of agent opinions over time. This uniformity in reaching consensus highlights the\\nMDAgents\\u2019 capability to integrate and reconcile information. Please refer to Appendix B for a\\ndetailed explanation of the entropy calculation.\\n5 Conclusion\\nThis paper introduces MDAgents, a framework designed to enhance the utility of LLMs in complex\\nmedical decision-making by dynamically structuring effective collaboration models. To reflect the\\nnuanced consultation aspects in clinical settings, MDAgents adaptively assigns LLMs either to roles\\nindependently or within groups, depending on the task\\u2019s complexity. This emulation of real-world\\nmedical decision processes has been comprehensively evaluated, with MDAgents outperforming\\nprevious solo and group methods in 7 out of 10 medical benchmarks. The case study illustrates the\\npractical efficacy and collaborative dynamics of our proposed framework, providing insights into\\nhow differing expert opinions are synthesized to reach a more accurate diagnosis. This is evidenced\\nby our agents\\u2019 ability to converge on the correct diagnosis despite initially divergent perspectives.\\nAblation studies further elucidate the individual contributions of agents and strategies within the\\nsystem, revealing the critical components and interactions that drive the framework\\u2019s success. By\\nharnessing the strength of multi-modal reasoning and fostering a collaborative process among LLM\\nagents, our framework opens up new possibilities for enhancing LLM-assisted medical diagnosis\\nsystems, pushing the boundaries of automated clinical reasoning.\\n10\\nAcknowledgments and Disclosure of Funding\\nWe thank Yoon Kim at MIT, Vivek Natarajan at Google, WonJin Yoon and Tim Miller at Harvard\\nMedical School, Seonghwan Bae at Sacheon Public Health Center, Hui Dong Lim at Seoul National\\nUniversity Hospital for their revisions, feedback, and support. C.P. acknowledges support from the\\nTakeda Fellowship, the Korea Foundation for Advanced Studies, and the Siebel Scholarship.\\nReferences\\n[1] Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David Sontag. Large\\nlanguage models are few-shot clinical information extractors. arXiv preprint arXiv:2205.12689,\\n2022.\\n[2] Zaid Al-Ars, Obinna Agba, Zhuoran Guo, Christiaan Boerkamp, Ziyaad Jaber, and Tareq\\nJaber. Nlice: Synthetic medical record generation for effective primary healthcare differential\\ndiagnosis, 2024.\\n[3] Seongsu Bae, Daeun Kyung, Jaehee Ryu, Eunbyeol Cho, Gyubok Lee, Sunjun Kweon, Jungwoo\\nOh, Lei Ji, Eric Chang, Tackeun Kim, et al. Ehrxqa: A multi-modal question answering\\ndataset for electronic health records with chest x-ray images. Advances in Neural Information\\nProcessing Systems, 36, 2024.\\n[4] Michael L Barnett, Dhruv Boddupalli, Shantanu Nundy, and David W Bates. Comparative\\naccuracy of diagnosis by collective intelligence of multiple physicians vs individual physicians.\\nJAMA network open, 2(3):e190096\\u2013e190096, 2019.\\n[5] Michael L Barnett, Nancy L Keating, Nicholas A Christakis, A James O\\u2019malley, and Bruce E\\nLandon. Reasons for choice of referral physician among primary care and specialist physicians.\\nJournal of general internal medicine, 27:506\\u2013512, 2012.\\n[6] Ofir Ben-Assuli, Nanda Kumar, Ofer Arazy, and Itamar Shabtai. The use of analytic hierarchy\\nprocess for measuring the complexity of medical diagnosis. Health Informatics Journal ,\\n26(1):218\\u2013232, 2020.\\n[7] Justin Bitter, Elizabeth van Veen-Berkx, Hein G Gooszen, and Pierre van Amelsvoort. Mul-\\ntidisciplinary teamwork is an important issue to healthcare professionals. Team Performance\\nManagement: An International Journal, 19(5/6):263\\u2013278, 2013.\\n[8] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\\nHerbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\\n[9] Hanjie Chen, Zhouxiang Fang, Yash Singla, and Mark Dredze. Benchmarking large language\\nmodels on answering and explaining challenging medical questions, 2024.\\n[10] Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal. Reconcile: Round-table conference\\nimproves reasoning via consensus among diverse llms, 2024.\\n[11] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu,\\nYaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin Cong, Ruobing Xie, Zhiyuan Liu, Maosong\\nSun, and Jie Zhou. Agentverse: Facilitating multi-agent collaboration and exploring emergent\\nbehaviors, 2023.\\n[12] Michael Christ, Florian Grossmann, Daniela Winter, Roland Bingisser, and Elke Platz. Modern\\ntriage in the emergency department. Deutsches \\u00c4rzteblatt International, 107(50):892, 2010.\\n[13] Jane Cioffi and Roslyn Markham. Clinical decision-making by midwives: managing case\\ncomplexity. Journal of advanced nursing, 25(2):265\\u2013272, 1997.\\n11\\n[14] Jan Clusmann, Fiona R Kolbinger, Hannah Sophie Muti, Zunamys I Carrero, Jan-Niklas Eckardt,\\nNarmin Ghaffari Laleh, Chiara Maria Lavinia L\\u00f6ffler, Sophie-Caroline Schwarzkopf, Michaela\\nUnger, Gregory P Veldhuizen, et al. The future landscape of large language models in medicine.\\nCommunications medicine, 3(1):141, 2023.\\n[15] Nick Craddock and Laurence Mynors-Wallis. Psychiatric diagnosis: impersonal, imperfect and\\nimportant. The British Journal of Psychiatry, 204(2):93\\u201395, 2014.\\n[16] Lesley F Degner and Jeffrey A Sloan. Decision making during serious illness: what role do\\npatients really want to play? Journal of clinical epidemiology, 45(9):941\\u2013950, 1992.\\n[17] Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. Improving\\nfactuality and reasoning in language models through multiagent debate, 2023.\\n[18] Carolyn Ee, James Lake, Joseph Firth, Fiona Hargraves, M De Manincor, Tanya Meade,\\nWolfgang Marx, and Jerome Sarris. An integrative collaborative care model for people with\\nmental illness and physical comorbidities. International Journal of Mental Health Systems ,\\n14:1\\u201316, 2020.\\n[19] Arthur S Elstein and Alan Schwarz. Clinical problem solving and diagnostic decision making:\\nselective review of the cognitive literature. BMJ, 324(7339):729\\u2013732, 2002.\\n[20] Glyn Elwyn, Dominick Frosch, Richard Thomson, Natalie Joseph-Williams, Amy Lloyd, Paul\\nKinnersley, Emma Cording, Dave Tomson, Carole Dodd, Stephen Rollnick, et al. Shared\\ndecision making: a model for clinical practice. Journal of general internal medicine, 27:1361\\u2013\\n1367, 2012.\\n[21] Nancy E Epstein. Multidisciplinary in-hospital teams improve patient outcomes: A review.\\nSurgical neurology international, 5(Suppl 7):S295, 2014.\\n[22] Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi, Fei Huang, and\\nJingren Zhou. Ai hospital: Interactive evaluation and collaboration of llms as intern doctors for\\nclinical diagnosis, 2024.\\n[23] Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. Improving language model negotiation\\nwith self-play and in-context learning from ai feedback, 2023.\\n[24] Isaac R Galatzer-Levy, Daniel McDuff, Vivek Natarajan, Alan Karthikesalingam, and Matteo\\nMalgaroli. The capability of large language models to measure psychiatric functioning. arXiv\\npreprint arXiv:2308.01834, 2023.\\n[25] Amy L Garcia. Variability in acuity in acute care.JONA: The Journal of Nursing Administration,\\n47(10):476\\u2013483, 2017.\\n[26] Nicki Gilboy, Paula Tanabe, Debbie Travers, Alexander M Rosenau, et al. Emergency severity\\nindex (esi): A triage tool for emergency department care, version 4. implementation handbook\\n2012 edition. AHRQ publication, 12, 2011.\\n[27] David Grembowski, Judith Schaefer, Karin E Johnson, Henry Fischer, Susan L Moore, Ming\\nTai-Seale, Richard Ricciardi, James R Fraser, Donald Miller, Lisa LeRoy, et al. A conceptual\\nmodel of the role of complexity in the care of patients with multiple chronic conditions.Medical\\ncare, 52:S7\\u2013S14, 2014.\\n[28] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan\\nNaumann, Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining for\\nbiomedical natural language processing. ACM Transactions on Computing for Healthcare ,\\n3(1):1\\u201323, October 2021.\\n[29] Deepak Gupta, Kush Attal, and Dina Demner-Fushman. A dataset for medical instructional\\nvideo classification and question answering, 2022.\\n[30] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+\\nquestions for medical visual question answering, 2020.\\n12\\n[31] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn\\nSong, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.\\narXiv preprint arXiv:2103.03874, 2021.\\n[32] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang,\\nJinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng\\nXiao, Chenglin Wu, and J\\u00fcrgen Schmidhuber. Metagpt: Meta programming for a multi-agent\\ncollaborative framework, 2023.\\n[33] Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh Tuan Luu, Junxian\\nHe, Pang Wei Koh, and Bryan Hooi. Uncertainty of thoughts: Uncertainty-aware planning\\nenhances information seeking in large language models. arXiv preprint arXiv:2402.03271,\\n2024.\\n[34] M. Jimenez-Lara. Reaping the benefits of integrated health care. stanford social innovation\\nreview. Stanford Social Innovation Review., 2016.\\n[35] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What\\ndisease does this patient have? a large-scale open domain question answering dataset from\\nmedical exams, 2020.\\n[36] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: A\\ndataset for biomedical research question answering. In Proceedings of the 2019 Conference on\\nEmpirical Methods in Natural Language Processing and the 9th International Joint Conference\\non Natural Language Processing (EMNLP-IJCNLP), pages 2567\\u20132577, 2019.\\n[37] Qiao Jin, Zhizheng Wang, Yifan Yang, Qingqing Zhu, Donald Wright, Thomas Huang, W John\\nWilbur, Zhe He, Andrew Taylor, Qingyu Chen, et al. Agentmd: Empowering language agents\\nfor risk prediction with large-scale clinical tool learning. arXiv preprint arXiv:2402.13225,\\n2024.\\n[38] Qiao Jin, Yifan Yang, Qingyu Chen, and Zhiyong Lu. Genegpt: augmenting large language\\nmodels with domain tools for improved access to biomedical information. Bioinformatics,\\n40(2), February 2024.\\n[39] Emre K\\u0131c\\u0131man, Robert Ness, Amit Sharma, and Chenhao Tan. Causal reasoning and large\\nlanguage models: Opening a new frontier for causality. arXiv preprint arXiv:2305.00050, 2023.\\n[40] Yubin Kim, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, and Hae Won Park. Health-llm:\\nLarge language models for health prediction via wearable sensor data, 2024.\\n[41] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\\nlanguage models are zero-shot reasoners. Advances in neural information processing systems,\\n35:22199\\u201322213, 2022.\\n[42] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate\\nBaumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to\\nself-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024.\\n[43] Tiffany H. Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille\\nElepa\\u00f1o, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, and Victor\\nTseng. Performance of chatgpt on usmle: Potential for ai-assisted medical education using large\\nlanguage models. PLOS Digital Health, 2(2):1\\u201312, 02 2023.\\n[44] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris\\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a\\nbenchmark for question answering research. Transactions of the Association for Computational\\nLinguistics, 7:453\\u2013466, 2019.\\n[45] Danielle L LaFrance, Mary Jane Weiss, Ellie Kazemi, Joanne Gerenser, and Jacqueline Dobres.\\nMultidisciplinary teaming: Enhancing collaboration through increased understanding. Behavior\\nanalysis in practice, 12(3):709\\u2013726, 2019.\\n13\\n[46] Beibin Li, Konstantina Mellou, Bo Zhang, Jeevan Pathuri, and Ishai Menache. Large language\\nmodels for supply chain optimization. arXiv preprint arXiv:2307.03875, 2023.\\n[47] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard\\nGhanem. Camel: Communicative agents for \\\"mind\\\" exploration of large language model society,\\n2023.\\n[48] Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang, Weizhi Ma, and\\nYang Liu. Agent hospital: A simulacrum of hospital with evolvable medical agents. arXiv\\npreprint arXiv:2405.02957, 2024.\\n[49] Shuyue Stella Li, Vidhisha Balachandran, Shangbin Feng, Jonathan Ilgen, Emma Pierson,\\nPang Wei Koh, and Yulia Tsvetkov. Mediq: Question-asking llms for adaptive and reliable\\nmedical reasoning. arXiv preprint arXiv:2406.00922, 2024.\\n[50] Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang,\\nZhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models\\nthrough multi-agent debate, 2023.\\n[51] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. Dynamic llm-agent network: An\\nllm-agent collaboration framework with agent team optimization, 2023.\\n[52] Valentin Li\\u00e9vin, Christoffer Egeberg Hother, Andreas Geert Motzfeldt, and Ole Winther. Can\\nlarge language models reason about medical questions?, 2023.\\n[53] Daniel McDuff, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan\\nSinghal, Yash Sharma, Shekoofeh Azizi, Kavita Kulkarni, et al. Towards accurate differential\\ndiagnosis with large language models. arXiv preprint arXiv:2312.00164, 2023.\\n[54] Ateev Mehrotra, Christopher B Forrest, and Caroline Y Lin. Dropping the baton: specialty\\nreferrals in the united states. The Milbank Quarterly, 89(1):39\\u201368, 2011.\\n[55] Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M. Krumholz, Jure\\nLeskovec, Eric J. Topol, and Pranav Rajpurkar. Foundation models for generalist medical\\nartificial intelligence. Nature, 616(7956):259\\u2013265, 2023.\\n[56] Tong Mu, Alec Helyar, Johannes Heidecke, Joshua Achiam, Andrea Vallone, Ian Kivlichan,\\nMolly Lin, Alex Beutel, John Schulman, and Lilian Weng. Rule based rewards for language\\nmodel safety. Advances in Neural Information Processing Systems, 2024.\\n[57] Mark A Musen, Blackford Middleton, and Robert A Greenes. Clinical decision-support systems.\\nIn Biomedical informatics: computer applications in health care and biomedicine , pages\\n795\\u2013840. Springer, 2021.\\n[58] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capa-\\nbilities of gpt-4 on medical challenge problems, 2023.\\n[59] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas\\nKing, Jonathan Larson, Yuanzhi Li, Weishung Liu, Renqian Luo, Scott Mayer McKinney,\\nRobert Osazuwa Ness, Hoifung Poon, Tao Qin, Naoto Usuyama, Chris White, and Eric Horvitz.\\nCan generalist foundation models outcompete special-purpose tuning? case study in medicine.\\nNovember 2023.\\n[60] OpenAI. Gpt-4 technical report, 2024.\\n[61] Anand K Parekh, Richard A Goodman, Catherine Gordon, Howard K Koh, and HHS Intera-\\ngency Workgroup on Multiple Chronic Conditions. Managing multiple chronic conditions: a\\nstrategic framework for improving health outcomes and quality of life. Public health reports,\\n126(4):460\\u2013471, 2011.\\n[62] Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar, and Kaiqing Zhang. Do llm agents have regret?\\na case study in online learning and games. arXiv preprint arXiv:2403.16843, 2024.\\n14\\n[63] Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan\\nZhang, Tim Strother, Chunjong Park, Elahe Vedadi, et al. Capabilities of gemini models in\\nmedicine. arXiv preprint arXiv:2404.18416, 2024.\\n[64] Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda Zhu, Joyce Ho,\\nCarl Yang, and May D. Wang. Ehragent: Code empowers large language models for few-shot\\ncomplex tabular reasoning on electronic health records, 2024.\\n[65] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C\\u00f4t\\u00e9, Yonatan Bisk, Adam Trischler, and\\nMatthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive\\nlearning. arXiv preprint arXiv:2010.03768, 2020.\\n[66] Hardeep Singh, Denise M Connor, and Gurpreet Dhaliwal. Five strategies for clinicians to\\nadvance diagnostic excellence. BMJ, 376, 2022.\\n[67] Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung,\\nNathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Senevi-\\nratne, Paul Gamble, Chris Kelly, Abubakr Babiker, Nathanael Sch\\u00e4rli, Aakanksha Chowdhery,\\nPhilip Mansfield, Dina Demner-Fushman, Blaise Ag\\u00fcera y Arcas, Dale Webster, Greg S.\\nCorrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajko-\\nmar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large\\nlanguage models encode clinical knowledge. Nature, 620(7972):172\\u2013180, 2023.\\n[68] Harold C Sox, Michael C Higgins, Douglas K Owens, and Gillian Sanders Schmidler. Medical\\ndecision making. John Wiley & Sons, 2024.\\n[69] J Stairmand, Louise Signal, D Sarfati, C Jackson, L Batten, M Holdaway, and C Cunningham.\\nConsideration of comorbidity in treatment decision making in multidisciplinary cancer team\\nmeetings: a systematic review. Annals of Oncology, 26(7):1325\\u20131332, 2015.\\n[70] Mirac Suzgun and Adam Tauman Kalai. Meta-prompting: Enhancing language models with\\ntask-agnostic scaffolding, 2024.\\n[71] Miren Taberna, Francisco Gil Moncayo, Enric Jan\\u00e9-Salas, Maite Antonio, Lorena Arribas,\\nEsther Vilajosana, Elisabet Peralvez Torres, and Ricard Mes\\u00eda. The multidisciplinary team\\n(mdt) approach and quality of care. Frontiers in oncology, 10:85, 2020.\\n[72] Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman\\nCohan, and Mark Gerstein. Medagents: Large language models as collaborators for zero-shot\\nmedical reasoning, 2024.\\n[73] Arsene Fansi Tchango, Rishab Goel, Zhi Wen, Julien Martel, and Joumana Ghosn. Ddxplus: A\\nnew dataset for automatic medical diagnosis, 2022.\\n[74] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez,\\nTing Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine.Nature medicine,\\n29(8):1930\\u20131940, 2023.\\n[75] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez,\\nTing Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine.Nature Medicine,\\n29(8):1930\\u20131940, 2023.\\n[76] Daniel Truhn, Jan-Niklas Eckardt, Dyke Ferber, and Jakob Nikolas Kather. Large language\\nmodels and multimodal foundation models for precision oncology. NPJ Precision Oncology,\\n8(1):72, 2024.\\n[77] Tao Tu, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy\\nWang, Brenna Li, Mohamed Amin, Nenad Tomasev, Shekoofeh Azizi, Karan Singhal, Yong\\nCheng, Le Hou, Albert Webson, Kavita Kulkarni, S Sara Mahdavi, Christopher Semturs, Juraj\\nGottweis, Joelle Barral, Katherine Chou, Greg S Corrado, Yossi Matias, Alan Karthikesalingam,\\nand Vivek Natarajan. Towards conversational diagnostic ai, 2024.\\n15\\n[78] Sean R Tunis, Daniel B Stryer, and Carolyn M Clancy. Practical clinical trials: increasing the\\nvalue of clinical research for decision making in clinical and health policy.Jama, 290(12):1624\\u2013\\n1632, 2003.\\n[79] Dave Van Veen, Cara Van Uden, Louis Blankemeier, Jean-Benoit Delbrouck, Asad Aali, Chris-\\ntian Bluethgen, Anuj Pareek, Malgorzata Polacin, Eduardo Pontes Reis, Anna Seehofnerov\\u00e1,\\nNidhi Rohatgi, Poonam Hosamani, William Collins, Neera Ahuja, Curtis P. Langlotz, Jason\\nHom, Sergios Gatidis, John Pauly, and Akshay S. Chaudhari. Adapted large language models can\\noutperform medical experts in clinical text summarization. Nature Medicine, 30(4):1134\\u20131142,\\nFebruary 2024.\\n[80] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen,\\nJiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey\\non large language model based autonomous agents, 2024.\\n[81] Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang Shen. Chatcad: Interactive\\ncomputer-aided diagnosis on medical image using large language models, 2023.\\n[82] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha\\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language\\nmodels. arXiv preprint arXiv:2203.11171, 2022.\\n[83] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\\nAdvances in neural information processing systems, 35:24824\\u201324837, 2022.\\n[84] Thomas Weida and Jane Weida. Outpatient e/m coding simplified.Family Practice Management,\\n29(1):26\\u201331, 2022.\\n[85] Robin M Weinick, Rachel M Burns, and Ateev Mehrotra. Many emergency department visits\\ncould be managed at urgent care centers and retail clinics. Health affairs, 29(9):1630\\u20131636,\\n2010.\\n[86] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun\\nZhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, and\\nChi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation, 2023.\\n[87] Richard C Wuerz, Leslie W Milne, David R Eitel, Debbie Travers, and Nicki Gilboy. Reliability\\nand validity of a new five-level triage instrument. Academic emergency medicine, 7(3):236\\u2013242,\\n2000.\\n[88] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang,\\nJunzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong,\\nYuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin,\\nShihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng,\\nXipeng Qiu, Xuanjing Huang, and Tao Gui. The rise and potential of large language model\\nbased agents: A survey, 2023.\\n[89] Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. Benchmarking retrieval-augmented\\ngeneration for medicine. arXiv preprint arXiv:2402.13178, 2024.\\n[90] Weixiang Yan, Haitian Liu, Tengxiao Wu, Qian Chen, Wen Wang, Haoyuan Chai, Jiayi Wang,\\nWeishan Zhao, Yixin Zhang, Renjun Zhang, et al. Clinicallab: Aligning agents for multi-\\ndepartmental clinical diagnostics in the real world. arXiv preprint arXiv:2406.13890, 2024.\\n[91] Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.2,\\nhow to learn from mistakes on grade-school math problems, 2024.\\n[92] Cyril Zakka, Akash Chaurasia, Rohan Shad, Alex R. Dalal, Jennifer L. Kim, Michael Moor,\\nKevin Alexander, Euan Ashley, Jack Boyd, Kathleen Boyd, Karen Hirsch, Curt Langlotz, Joanna\\nNelson, and William Hiesinger. Almanac: Retrieval-augmented language models for clinical\\nmedicine, 2023.\\n16\\n[93] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker,\\nFederico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent\\nVanhoucke, and Pete Florence. Socratic models: Composing zero-shot multimodal reasoning\\nwith language, 2022.\\n[94] Haodi Zhang, Jiahong Li, Yichi Wang, and Yuanfeng Songi. Integrating automated knowledge\\nextraction with large language models for explainable medical decision-making. In 2023 IEEE\\nInternational Conference on Bioinformatics and Biomedicine (BIBM), pages 1710\\u20131717. IEEE,\\n2023.\\n[95] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi\\nXie. Pmc-vqa: Visual instruction tuning for medical visual question answering, 2023.\\n[96] Hongjian Zhou, Fenglin Liu, Boyang Gu, Xinyu Zou, Jinfa Huang, Jinge Wu, Yiru Li, Sam S.\\nChen, Peilin Zhou, Junling Liu, Yining Hua, Chengfeng Mao, Chenyu You, Xian Wu, Yefeng\\nZheng, Lei Clifton, Zheng Li, Jiebo Luo, and David A. Clifton. A survey of large language\\nmodels in medicine: Progress, application, and challenge, 2024.\\n[97] Junbin Zhou and Xiao Xu. The difficulty of medical decision-making: should patients be\\ninvolved? Hepatobiliary Surgery and Nutrition, 12(3):407, 2023.\\n17\\nLimitations and Future Works\\nDespite the successes of our framework in showing promising performance in medical decision-\\nmaking tasks, we recognize several limitations that open pathways for future research.\\nMedical Focused Foundation Models. An essential enhancement would be to incorporate the\\nfoundation models and systems specifically trained on medical data, like Med-Gemini [63], AMIE\\n[77], and Med-PaLM 2 [67]. These models excel in generating professional medical terminologies,\\nwhich can facilitate more effective and accurate communication between multiple agents involved in\\nthe decision-making process. By leveraging these specialized models, the agents can interact using\\na shared, precise medical vocabulary, enhancing the system\\u2019s overall performance and reliability.\\nThis approach not only ensures more medically accurate content generation but also supports better\\ncollaboration and understanding among the agents, which is essential for complex medical decision-\\nmaking tasks.\\nPatient-Centered Diagnosis. A primary limitation lies in the fact that our current framework\\noperates within the confines of multiple-choice question answering and does not account for the\\ninteractive, patient-centered nature of real-world diagnostics. Effective diagnosis often relies on\\ncontinuous exchanges that include the patient\\u2019s narrative, the physician\\u2019s expertise, and input from\\ncaregivers. To bridge this gap, future iterations of our framework will aim to incorporate a more\\ninteractive system that not only assists physicians but also directly engages with both patients and\\ncaregivers in a multi-stakeholder approach. Moreover, by incorporating regret-aware [91] decision-\\nmaking, the system can learn to minimize diagnostic regret over time, refining its responses based\\non the outcomes of prior interactions. This regret-aware framework will help guide the LLM to\\nseek additional information when uncertainties arise, thereby supporting more informed decisions\\nacross complex, multi-stakeholder scenarios. Embedding these real-world interactions within the\\nfeedback loop will enable the system to provide more nuanced and patient-centric support, enhancing\\nthe quality and personalization of medical decision-making across all involved parties.\\nPotential Risks and Mitigations. While our framework shows promise, potential risks include\\nmedical hallucinations and the generation of inaccurate or misleading information. To address these\\nrisks, integrating self-correction mechanisms, such as those proposed by [42], could enable the model\\nto autonomously identify and rectify its own errors via reinforcement learning-based self-correction.\\nAdditionally, implementing rule-based reward structures, as suggested in [56], would allow the model\\nto adhere to specific safety and accuracy guidelines during training. These methods can support a\\nsafer, more reliable diagnostic support tool by introducing corrective feedback loops and standardized\\nbehavior guidelines. Furthermore, integrating confidence scores and uncertainty estimates with the\\nmodel\\u2019s recommendations could enhance the decision-making process by enabling end-users to\\nweigh various diagnostic options, ultimately increasing the system\\u2019s trustworthiness and safety.\\nA Dataset Information\\nWe evaluate multi-agent collaboration frameworks across seven common medical question-answering\\ndatasets, which vary in question complexity. Generally, questions are deemed more complex if they\\ninvolve multiple modalities or entail a lengthy, detailed diagnostic task. Below, we detail each dataset\\nand provide a sample entry:\\n1. MedQA. The MedQA dataset consists of professional medical board exams from the US,\\nMainland China, and Taiwan [35]. Our study focuses on the English test set, comprising\\n1,273 questions sourced from the United States Medical Licensing Examination (USMLE).\\nThese questions are formatted as multiple-choice text queries with five options. Due to their\\ntextual nature and brevity, we categorize these questions as low.\\nSample Question: \\u201cA 47-year-old female undergoes a thyroidectomy for treatment of Graves\\u2019\\ndisease. Post-operatively, she reports a hoarse voice and difficulty speaking. You suspect\\nthat this is likely a complication of her recent surgery. What is the embryologic origin of the\\ndamaged nerve that is most likely causing this patient\\u2019s hoarseness?\\u201d\\nOptions: A: 1st pharyngeal arch, B: 2nd pharyngeal arch, C: 3rd pharyngeal arch, D: 4th\\npharyngeal arch, E: 6th pharyngeal arch\\n18\\nTable 4: Summary of the Datasets. T : Text, I : Image, V : Video. In Appendix A, we provide\\ndetailed sample information for each benchmark.\\nDataset Modality Format Choice Testing Size Domain\\nMedQA T Question + Answer A/B/C/D 1273 US Medical Licensing Examination\\nPubMedQA T Question + Context + Answer Yes/No/Maybe 500 PubMed paper abstracts\\nDDxPlus T Question + Answer A/B/C/D/ \\u00b7 \\u00b7 \\u00b7 134 K Pathologies, Symptoms and Antecedents from Patients\\nSymCat T Question + Answer A/B/C/D 369 K Disease-symptom records from public datasets\\nJAMA T Case + Question + Answer A/B/C/D 1524 Challenging real-world Clinical Cases from diverse Medical\\nDomains\\nMedBullets T Case + Question + Answer A/B/C/D 308 Online Platform Resources for Medical Study\\nMIMIC-CXR T I Question + Answer Closed Answer 1531 Chest X-ray images and Free-text Reports.\\nPMC-VQA T I Question + Answer A/B/C/D 50 K VQA pairs across Images, spanning diverse Modalities and\\nDiseases\\nPath-VQA T I Question + Answer Yes/No 3391 Open-ended Questions from Pathology Images\\nMedVidQA T V Question + Answer A/B/C/D 155 First Aids, Medical Emergency, and Medical Education\\nQuestions\\n2. PubMedQA. PubMedQA is a QA dataset based on biomedical research [36]. It requires\\nyes/no/maybe answers to questions grounded in PubMed abstract. The dataset comprises\\nentries each containing a question, a relevant abstract minus the conclusion, and a ground\\ntruth label. We used 50 samples for testing. Given its binary choice format, we consider the\\ncomplexity of this dataset to be low.\\nSample Question: \\u201cCan predilatation in transcatheter aortic valve implantation be omit-\\nted?\\u201d\\nContext: \\u201cThe use of a balloon expandable stent valve includes balloon predilatation of the\\naortic stenosis before valve deployment. The aim of the study was to see whether or not\\nballoon predilatation is necessary in transcatheter aortic valve replacement (TAVI). Sixty\\nconsecutive TAVI patients were randomized to the standard procedure or to a protocol where\\nballoon predilatation was omitted. There were no significant differences between the groups\\nregarding early hemodynamic results or complication rates.\\\"\\n3. DDXPlus. DDXPlus is a medical diagnosis dataset using synthetic patient information and\\nsymptoms [73]. Each instance represents a patient, with attributes including age, sex, initial\\nevidences, evidence, multiple options of possible pathologies, and a ground truth diagnosis.\\nDue to its text-only and multiple-choice nature, we consider the complexity to be low.\\nSample Patient Information: Age: 96, Sex: F\\nEvidences: [\\u2018e66\\u2019, \\u2018insp_siffla\\u2019, \\u2018j45\\u2019, \\u2018posttus_emesis\\u2019, \\u2018trav1_@_N\\u2019, \\u2018vaccination\\u2019]\\nInitial evidence: \\u2018posttus_emesis\\u2019\\nOptions: (A) Bronchite (B) Coqueluche\\n4. PMC-VQA. PMC-VQA is a large-scale medical visual question-answering dataset that\\ncontains 227K Visual Question Answering (VQA) pairs of 149K images [95]. It is structured\\nas a multiple-choice QA task with one image input accompanying each question. Since the\\nquery requires a model to consider both text and image inputs, while maintaining medical\\nexpertise, we consider the complexity to be moderate.\\nSample Question: What is the appearance of the hyperintense foci in the basal ganglia on\\nT1-weighted MRI image?\\nImage: PMC8415802_FIG1.jpg\\nOptions: A: Hypodense, B: Hyperdense, C: Isointense, D: Hypointense\\n5. Path-VQA. PathVQA is a VQA dataset specifically on pathology images [ 30]. Differ-\\nent from PMC-VQA which consists of multiple choice questions, Path-VQA includes\\nopen-ended questions and binary \\\"yes/no\\\" questions. For the purpose of maintaining a\\nstandardized accuracy evaluation, we use only the yes/no questions. Similar to PMC-VQA,\\nwe consider the complexity to be moderate.\\nSample Question: Was a gravid uterus removed for postpartum bleeding?\\nImage: test_0273.jpg\\n6. MedVidQA. MedVidQA dataset consists of 3,010 health-related questions with visual\\nanswers from validated video sources (e.g. medical school, health institutions, etc). We\\n19\\nenhanced the dataset by using GPT-4 to generate multiple-choice answers, including one\\ncorrect \\u2018golden answer\\u2019 and several false options, expanding its use for training and evaluat-\\ning automated medical question-answering systems.\\nSample Question: How to perform corner stretches to treat neck pain?\\nVideo: h5MvX50zTLM.mp4\\nOptions: A: By bending your knees and touching your toes, B: By performing jumping jacks,\\nC: By leaning into a corner with your elbows up at shoulder level, D: By doing push-ups\\n7. SymCat. SymCat is a synthetic dataset which includes 5 million symptom-condition\\nsamples, covering 801 distinct conditions each with 376 potential symptoms dataset [2].\\nSample Patient Information: Age: 42, Gender: F , Race: White, Ethnicity: Nonhispanic\\nSample Patient Symptoms: Pain:cramping:Abdomen:Lower abdomen::::Worsening after\\nmeals::32; Altered stool:Lumpy:::::::Often:44; Flatulence:::::::::30; Bloating:::::::::41\\nOptions: \\\"A\\\": \\\"Asthma\\\", \\\"B\\\": \\\"IBS (Constipation type)\\\", \\\"C\\\": \\\"Viral meningitis (Varicella\\nzoster virus)\\\", \\\"D\\\": \\\"Bacterial (Gastro)enteritis (Yersinia infection most likely)\\\"\\n8. JAMA. JAMA includes 1524 clinical cases collected from the JAMA Network Clinical\\nChallenge archive, which are summaries of actual challenging clinical cases. Each sample\\nis framed as a question, with a long case description and four options [9].\\nSample Case: A 62-year-old woman undergoing peritoneal dialysis (PD) for kidney failure\\ndue to IgA nephropathy presented to the PD clinic with a 1-day history of severe abdominal\\npain and cloudy PD fluid. Seven days prior, she inadvertently broke aseptic technique when\\ntightening a leaking connection of her PD catheter tubing. On presentation, she was afebrile\\nand had normal vital signs. Physical examination revealed diffuse abdominal tenderness.\\nCloudy fluid that was drained from her PD catheter was sent for laboratory analysis (Table\\n1).Await peritoneal dialysis fluid culture results before starting intraperitoneal antibiotics.\\nSample Question: What Would You Do Next?\\n\\\"A\\\": \\\"Administer empirical broad-spectrum intraperitoneal antibiotics\\\", \\\"B\\\": \\\"Administer\\nempirical broad-spectrum intravenous antibiotics\\\", \\\"C\\\": \\\"Await peritoneal dialysis fluid\\nculture results before starting intraperitoneal antibiotics\\\", \\\"D\\\": \\\"Send blood cultures\\\"\\n9. Medbullets. Medbullets comprises 308 USMLE Step 2/3 style questions collected from\\nopen-access tweets on X (formerly Twitter) since April 2022. The difficulty is comparable\\nto that of Step 2/3 exams, which emulate common clinical scenarios [9].\\nSample Question: A 2-week-old boy is evaluated by his pediatrician for abnormal feet. The\\npatient was born at 39 weeks via vaginal delivery to a G1P1 29-year-old woman. The\\npatient has been breastfeeding and producing 5 stools/day. He is otherwise healthy. His\\ntemperature is 99.5\\u02d800b0F (37.5 \\u02d800b0C), blood pressure is 60/38 mmHg, pulse is 150/min,\\nrespirations are 24/min, and oxygen saturation is 98% on room air. A cardiopulmonary\\nexam is notable for a benign flow murmur. A musculoskeletal exam reveals the findings\\nshown in Figure A. Which of the following is the most appropriate next step in management?\\nOptions: \\\"A\\\": \\\"Botulinum toxin injections\\\", \\\"B\\\": \\\"Reassurance and reassessment in 1\\nmonth\\\", \\\"C\\\": \\\"Serial casting\\\", \\\"D\\\": \\\"Surgical pinning\\\",\\n10. MIMIC-CXR-VQA. MIMIC-CXR is a large-scale visual question-answering dataset of\\n377,110 chest radiographs. It was obtained from 227,827 imaging studies sourced from\\nthe BIDMC between 2011-2016. It includes patient identifiers which can be linked to\\nMIMIC-IV [3].\\nSample Question: Are there any abnormalities in the upper mediastinum?\\nImage: p11/p11218589/s59138139/7a1e4762-c176bd78-6281fe5c-6b0c9734-e9a4c8f1.jpg\\nB Entropy Calculation for Consensus Dynamics\\nThe entropy H serves as an indicator of consensus progression among the agents. It is quantified as:\\nH = \\u2212\\nMX\\ni=1\\np(xi) log2 p(xi) (B.1)\\nwhere M is the total number of unique answers, xi represents a unique answer, and p(xi) is the\\nprobability of occurring among all answers. This calculation helps to measure the degree of agreement\\namong the agents over time, with lower entropy indicating higher consensus. The trends in entropy\\n20\\nacross different data modalities provide insights into how quickly and effectively MDAgents can\\nreach a unified decision.\\nC Prompt Templates\\nC.1 A single agent setting\\n{{instruction}}\\nThe following are multiple choice questions (with answers) about medical knowledge.\\n{{few_shot_examples}}\\n{{context}} **Question:** {{question}} {{answer_choices}} **Answer:**(\\nFew-shot multiple choice questions\\n{{instruction}}\\nThe following are multiple choice questions (with answers) about medical knowledge.\\n{{few_shot_examples w/ CoT Solutions}}\\n{{context}} **Question:** {{question}} {{answer_choices}} **Answer:**(\\nChain-of-Thought multiple choice questions\\n{{instruction}}\\nThe following are multiple choice questions (with answers) about medical knowledge.\\n{{few_shot_examples w/ CoT Solutions}}\\n{{context}} **Question:** {{question}} {{answer_choices}}\\n{{reasoning_paths}} **Answer:**(\\nEnsemble Refinement multiple choice questions\\n{{instruction}}\\nThe following are multiple choice questions (with answers) about medical knowledge.\\n{{few_shot_examples w/ CoT Solutions from similarity calculation}}\\nfor N times do\\n{{context}} **Question:** {{question}} {{shuffled_answer_choices}}\\n**Answer:**(\\nMedprompt multiple choice questions\\nC.2 Multi-agent setting\\n21\\nYou are a medical expert who conducts initial assessment and your job is to decide the\\ndifficulty/complexity of the medical query.\\nNow, given the medical query as below, you need to decide the difficulty/complexity of it:\\n{{question}}\\nPlease indicate the difficulty/complexity of the medical query among below options:\\n1) low: a PCP or general physician can answer this simple medical knowledge checking question without\\nrelying heavily on consulting other specialists.\\n2) moderate: a PCP or general physician can answer this question in consultation with other specialist in\\na team.\\n3) high: Team of multi-departmental specialists can answer to the question which requires specialists\\nconsulting to another department (requires a lot of team effort to treat the case).\\n**Answer:**(\\nComplexity check prompt\\nYou are an experienced medical expert who recruits a group of experts with diverse identity and ask them\\nto discuss and solve the given medical query.\\nNow, given the medical query as below, you need to decide the difficulty/complexity of it:\\n{{question}}\\nYou can recruit up to N experts in different medical expertise. Considering the medical question and the\\noptions for the answer, what kind of experts will you recruit to better make an accurate answer?\\nAlso, you need to specify the communication structure between experts (e.g., Pulmonologist ==\\nNeonatologist == Medical Geneticist == Pediatrician > Cardiologist)\\nFor example, if you want to recruit five experts, you answer can be like:\\n{{examplers}}\\nPlease answer in above format, and do not include your reason.\\n**Answer:**(\\nRecruiter prompt\\nD Additional Results\\nThis section presents additional experimental results and analyses that provide further insights into\\nthe performance and characteristics of our MDAgents framework.\\nYou are a {{role}} who {{description}}. Your job is to collaborate with other medical experts in a\\nteam.\\nAgent initialization prompt\\nGiven the opinions from other medical agents in your team, please indicate whether you want to talk to\\nany expert (yes/no). If not, provide your opinion. {{opinions}}\\nNext, indicate the agent you want to talk to: {{agent_list}}\\nRemind your medical expertise and leave your opinion to an expert you chose. Deliver your opinion once\\nyou are confident enough and in a way to convince other expert with a short reason.\\nAgent interaction prompt\\n22\\nYou are a final medical decision maker who reviews all opinions from different medical experts and\\nmakes the final decision.\\nGiven the {{inputs}}, please review the {{inputs}} and make the final answer to the question by\\n{{decision_methods}} (e.g., majority voting, ensemble refinement).\\n**Answer:**(\\nFinal decision prompt\\nGiven the {{agent answers}}, please complete the following steps:\\n1. Take careful and comprehensive consideration of the provided reports.\\n2. Extract key knowledge from the reports.\\n3. Derive a comprehensive and summarized analysis based on the extracted knowledge.\\n4. Generate a refined and synthesized report based on your analysis.\\nReport Generation\\nD.1 Accuracy on entire MedQA 5-options Dataset\\nTo provide a comprehensive evaluation of our approach, we conducted experiments on the entire\\nMedQA 5-options dataset using GPT-4o mini. This expands upon the subsampled experiments\\npresented in the main experiments in Table 2. Below Table shows the accuracy results for various\\nmethods.\\nTable 5: Accuracy (%) on entire MedQA 5-options dataset with GPT-4o mini\\n.\\nCategory Method Accuracy (%)\\nSingle-agent\\nZero-shot 71.5\\n3-shot 72.3\\n+ CoT 76.6\\n+ CoT-SC 77.2\\nMulti-agent (Multi-model) Majority V oting 76.3\\nWeighted V oting 79.1\\nBorda Count 76.1\\nMulti-agent (Single-model) Reconcile 80.2\\nAdaptive MDAgents (Ours) 83.6\\nThese results demonstrate that our MDAgents approach outperforms both single-agent and other\\nmulti-agent methods across the full dataset, achieving an accuracy of 83.6%. This underscores the\\neffectiveness of our framework in handling diverse medical questions at scale.\\nD.2 Estimated Costs for Full Test Set Experiments\\nTo provide transparency and aid in reproducibility, we estimated the costs associated with running\\nexperiments on the entire test sets using GPT-4 (Vision). Below Table presents these cost estimates in\\nUSD for various datasets and methods.\\nTable 6: Estimated costs for experimenting with entire test sets with GPT-4 (Vision) (in USD)\\nMethod MedQA PubMedQA Path-VQA PMC-VQA DDXPlus SymCat JAMA MedBullets MIMIC-CXR Total Cost\\nCoT 55.24 13.16 3,028.54 27,134.00 16,461.90 10,593.99 134.55 61.23 1,388.70 58,871.29\\nOurs 172.43 41.36 9,369.45 82,194.34 44,814.97 31,176.05 367.13 161.70 4,406.90 172,704.33\\nWhile our approach incurs higher costs due to its multi-agent nature, the significant performance\\nimprovements justify this increased computational expense for critical medical decision-making\\ntasks.\\n23\\nD.3 Impact of Knowledge Enhancement with RAG\\nMethod Accuracy (%)\\nMDAgents (baseline) 71.8\\n+ MedRAG 75.2\\n+ Medical Knowledge Initialization 76.0\\n+ Moderator\\u2019s Review 77.6\\n+ Moderator\\u2019s Review & MedRAG 80.3\\nTable 7: Impact of knowledge enhancement on MDAgents performance\\nWe investigated whether simply assigning roles to agents is sufficient for expert-like performance,\\nand explored the impact of equipping agents with different knowledge using Retrieval-Augmented\\nGeneration (RAG). Table 7 presents the results of these experiments.\\nThese results indicate that while role assignment provides a foundation, augmenting agents with\\nspecific knowledge (using MedRAG) and structured reviews (Moderator\\u2019s Review) significantly\\nenhances their ability to simulate domain expertise. The combination of Moderator\\u2019s Review and\\nMedRAG yielded the best performance, highlighting the synergy between structured collaboration\\nand domain-specific knowledge retrieval.\\nD.4 Complexity Assignment and Collaborative Settings\\nTo address the impact of complexity assignment on accuracy and API costs, we conducted additional\\nexperiments focusing on high-complexity cases, particularly in image+text scenarios. Table 8 shows\\nthe results for various collaborative settings.\\nCollaboration Setting Accuracy (%)\\nSequential & No Discussion 39.0\\nSequential & Discussion 45.0\\nParallel & No Discussion 56.0\\nParallel & Discussion 59.0\\nTable 8: Impact of collaboration settings on high-complexity image+text tasks\\nThese results underscore the importance of multi-turn discussions, particularly in complex cases. The\\nparallel collaboration with discussion yielded the highest accuracy (59.0%), suggesting that enabling\\nagents to work simultaneously and engage in dialogue is crucial for handling intricate medical queries.\\nThe significant performance gap between discussion and no-discussion scenarios (45.0% vs. 39.0%\\nfor sequential, and 59.0% vs. 56.0% for parallel) highlights the value of interactive deliberation in\\nmedical decision-making processes.\\n24\\nTable 9: Accuracy (%) on Medical benchmarks with Solo (\\n ) setting. Bold represents the best and\\nUnderlined represents the second best performance for each benchmark and model.\\nCategory Method\\nMedical Knowledge Retrieval Datasets\\nMedQA\\nT\\nPubMedQA\\nT\\nPath-VQA\\nI T\\nPMC-VQA\\nI T\\nMedVidQA\\nV T\\nGPT-3.5\\nZero-shot 48.5 \\u00b13.3 56.8 \\u00b112.0 - - -\\nFew-shot 47.8 \\u00b116.4 59.0 \\u00b11.0 - - -\\n+ CoT 54.2 \\u00b18.9 49.7 \\u00b111.9 - - -\\n+ CoT-SC 60.5 \\u00b13.2 49.4 \\u00b113.5 - - -\\nER 60.2 \\u00b14.0 52.9 \\u00b115.9 - - -\\nMedprompt 60.1 \\u00b110.8 59.8 \\u00b18.5 - - -\\nGPT-4(V)\\nZero-shot 75.0 \\u00b11.3 61.5 \\u00b12.2 57.9 \\u00b11.6 49.0 \\u00b13.7 -\\nFew-shot 72.9 \\u00b111.4 63.1 \\u00b111.7 57.5 \\u00b14.5 52.2 \\u00b12.0 -\\n+ CoT [83] 82.5 \\u00b14.9 57.6 \\u00b19.2 58.6 \\u00b13.1 51.3 \\u00b11.5 -\\n+ CoT-SC [82] 83.9 \\u00b12.7 58.7 \\u00b15.0 61.2 \\u00b12.1 50.5 \\u00b15.2 -\\nER [67] 81.9 \\u00b12.1 56.0 \\u00b17.0 61.4 \\u00b14.1 52.7 \\u00b12.9 -\\nMedprompt [59] 82.4 \\u00b15.1 51.8 \\u00b14.6 59.2 \\u00b15.7 53.4 \\u00b17.9 -\\nGemini-Pro(Vision)\\nZero-shot 42.0 \\u00b110.4 65.2 \\u00b114.5 45.9 \\u00b12.8 44.8 \\u00b12.0 37.9 \\u00b18.4\\nFew-shot 34.0 \\u00b17.2 55.0 \\u00b10.0 64.5 \\u00b12.3 48.2 \\u00b11.0 47.1 \\u00b18.6\\n+ CoT 50.0 \\u00b16.0 60.2 \\u00b19.0 66.4 \\u00b111.7 47.1 \\u00b14.2 48.6 \\u00b15.5\\n+ CoT-SC 52.7 \\u00b14.6 55.8 \\u00b18.9 63.6 \\u00b16.0 46.3 \\u00b12.8 49.2 \\u00b18.2\\nER 52.0 \\u00b17.2 58.4 \\u00b114.2 57.6 \\u00b18.4 38.4 \\u00b12.0 48.5 \\u00b14.1\\nMedprompt 45.3 \\u00b13.1 50.6 \\u00b15.4 55.0 \\u00b12.0 41.8 \\u00b13.0 44.5 \\u00b12.0\\nCategory Method\\nClinical Reasoning and Diagnostic Datasets\\nDDXPlus\\nT\\nSymCat\\nT\\nJAMA\\nT\\nMedBullets\\nT\\nMIMIC-CXR\\nI T\\nGPT-3.5\\nZero-shot 56.2 \\u00b14.1 84.0 \\u00b10.0 36.0 \\u00b13.3 56.0 \\u00b12.8 -\\nFew-shot 48.9 \\u00b18.5 86.0 \\u00b12.8 38.0 \\u00b14.2 59.0 \\u00b11.4 -\\n+ CoT 52.8 \\u00b15.4 82.0 \\u00b10.0 34.0 \\u00b12.4 56.0 \\u00b15.7 -\\n+ CoT-SC 37.8 \\u00b16.1 80.0 \\u00b12.8 43.0 \\u00b14.2 63.0 \\u00b14.2 -\\nER 42.3 \\u00b16.9 84.0 \\u00b11.8 44.0 \\u00b15.7 58.0 \\u00b10.0 -\\nMedprompt 41.2 \\u00b16.2 86.0 \\u00b12.6 43.0 \\u00b11.4 54.0 \\u00b15.7 -\\nGPT-4(V)\\nZero-shot 70.3 \\u00b12.0 88.7 \\u00b12.3 62.0 \\u00b12.0 67.0 \\u00b11.4 40.0 \\u00b15.3\\nFew-shot 69.4 \\u00b11.0 86.7 \\u00b13.1 69.0 \\u00b14.2 72.0 \\u00b12.8 35.3 \\u00b15.0\\n+ CoT [83] 72.7 \\u00b17.7 78.0 \\u00b12.0 66.0 \\u00b15.7 70.0 \\u00b10.0 36.2 \\u00b15.2\\n+ CoT-SC [82] 52.1 \\u00b16.4 83.3 \\u00b13.1 68.0 \\u00b12.8 76.0 \\u00b12.8 51.7 \\u00b14.0\\nER [67] 61.3 \\u00b12.4 82.7 \\u00b12.3 71.0 \\u00b11.4 76.0 \\u00b15.7 50.0 \\u00b10.0\\nMedprompt [59] 59.5 \\u00b117.7 87.3 \\u00b11.2 70.7 \\u00b14.3 71.0 \\u00b11.4 53.4 \\u00b14.3\\nGemini-Pro(Vision)\\nZero-shot 49.9 \\u00b16.5 88.9 \\u00b16.4 42.7 \\u00b13.9 40.0 \\u00b11.5 40.0 \\u00b12.8\\nFew-shot 47.1 \\u00b15.6 89.2 \\u00b13.4 41.0 \\u00b11.4 44.0 \\u00b13.9 39.2 \\u00b11.2\\n+ CoT [83] 65.5 \\u00b14.9 91.9 \\u00b13.4 38.0 \\u00b11.6 52.7 \\u00b17.1 45.2 \\u00b16.8\\n+ CoT-SC [82] 60.3 \\u00b12.4 92.0 \\u00b11.5 46.0 \\u00b10.0 51.0 \\u00b14.2 54.9 \\u00b13.4\\nER [67] 46.7 \\u00b16.9 58.5 \\u00b17.5 50.8 \\u00b15.8 53.2 \\u00b17.8 53.2 \\u00b13.5\\nMedprompt [59] 58.2 \\u00b15.5 92.5 \\u00b14.5 44.4 \\u00b13.2 54.0 \\u00b15.7 51.2 \\u00b11.9\\n* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement\\n* T : text-only, I : image+text, V : video+text\\n* All experiments were tested with 3 random seeds\\n25\\nTable 10: Accuracy (%) on Medical benchmarks with Group (\\n ) setting. Bold represents the best\\nperformance for each benchmark and model.\\nCategory Method\\nMedical Knowledge Retrieval Datasets\\nMedQA\\nT\\nPubMedQA\\nT\\nPath-VQA\\nI T\\nPMC-VQA\\nI T\\nMedVidQA\\nV T\\nGPT-3.5\\nMajority V oting 60.4 \\u00b12.1 68.5 \\u00b19.6 - - -\\nWeighted V oting 57.3 \\u00b13.0 65.8 \\u00b111.4 - - -\\nBorda Count 55.3 \\u00b17.1 70.2 \\u00b18.8 - - -\\nMedAgents [72] 56.0 \\u00b15.3 55.0 \\u00b11.4 - - -\\nGPT-4(V)\\nMajority V oting 80.6 \\u00b12.9 72.2 \\u00b16.9 56.9 \\u00b119.7 36.8 \\u00b16.7 -\\nWeighted V oting 78.8 \\u00b11.1 72.2 \\u00b16.9 62.1 \\u00b113.9 25.4 \\u00b19.0 -\\nBorda Count 70.3 \\u00b18.5 66.9 \\u00b13.0 61.9 \\u00b18.1 27.9 \\u00b15.3 -\\nMedAgents [72] 79.1 \\u00b17.4 69.7 \\u00b14.7 45.4 \\u00b18.1 39.6 \\u00b13.0 -\\nGemini-Pro(Vision)\\nMajority V oting 51.6 \\u00b12.2 65.3 \\u00b112.9 58.2 \\u00b17.3 27.1 \\u00b15.4 50.8 \\u00b17.4\\nWeighted V oting 52.3 \\u00b13.3 63.7 \\u00b110.0 66.4 \\u00b111.1 20.9 \\u00b13.8 57.8 \\u00b12.1\\nBorda Count 49.4 \\u00b19.7 57.7 \\u00b115.0 68.2 \\u00b11.8 25.3 \\u00b18.7 54.5 \\u00b14.7\\nMedAgents [72] 48.4 \\u00b15.5 63.6 \\u00b16.0 64.9 \\u00b112.5 35.1 \\u00b13.1 61.6 \\u00b14.8\\nMulti-agent\\nReconcile [10] 81.3 \\u00b13.0 79.7 \\u00b13.2 57.5 \\u00b13.3 31.4 \\u00b11.2 -\\nAutoGen [86] 60.6 \\u00b15.0 77.3 \\u00b12.3 43.0 \\u00b18.9 37.3 \\u00b16.1 -\\nDyLAN [51] 64.2 \\u00b12.3 73.6 \\u00b14.2 41.3 \\u00b11.2 34.0 \\u00b13.5 -\\nMeta-Prompting [70] 80.6 \\u00b11.2 73.3 \\u00b12.3 55.3 \\u00b12.3 42.6 \\u00b14.2 -\\nCategory Method\\nClinical Reasoning and Diagnostic Datasets\\nDDXPlus\\nT\\nSymCat\\nT\\nJAMA\\nT\\nMedBullets\\nT\\nMIMIC-CXR\\nI T\\nGPT-3.5\\nMajority V oting 53.6 \\u00b12.2 83.7 \\u00b13.3 47.0 \\u00b11.4 46.0 \\u00b14.1 -\\nWeighted V oting 55.2 \\u00b12.0 85.9 \\u00b13.0 49.0 \\u00b11.4 43.0 \\u00b18.4 -\\nBorda Count 63.9 \\u00b112.1 84.9 \\u00b11.6 50.0 \\u00b11.0 45.0 \\u00b15.6 -\\nMedAgents [72] 47.3 \\u00b111.0 87.0 \\u00b14.2 41.0 \\u00b13.3 56.0 \\u00b16.2 -\\nGPT-4(V)\\nMajority V oting 67.8 \\u00b14.9 91.9 \\u00b12.2 70.0 \\u00b15.7 70.0 \\u00b10.0 49.5 \\u00b110.7\\nWeighted V oting 65.9 \\u00b13.3 90.5 \\u00b12.9 66.1 \\u00b14.1 66.0 \\u00b15.7 53.5 \\u00b12.2\\nBorda Count 67.1 \\u00b16.7 78.0 \\u00b111.8 61.0 \\u00b15.6 66.0 \\u00b12.8 45.3 \\u00b16.8\\nMedAgents [72] 62.8 \\u00b15.6 90.0 \\u00b10.0 66.0 \\u00b15.7 77.0 \\u00b11.4 43.3 \\u00b17.0\\nGemini-Pro(Vision)\\nMajority V oting 52.3 \\u00b115.3 73.5 \\u00b14.9 47.0 \\u00b11.4 44.0 \\u00b14.2 47.9 \\u00b16.6\\nWeighted V oting 54.3 \\u00b116.9 64.6 \\u00b16.5 42.0 \\u00b13.2 43.6 \\u00b13.9 43.2 \\u00b12.0\\nBorda Count 67.0 \\u00b127.7 77.3 \\u00b19.3 37.0 \\u00b11.8 46.1 \\u00b13.2 44.6 \\u00b14.5\\nMedAgents [72] 43.0 \\u00b12.7 80.5 \\u00b11.9 40.8 \\u00b12.9 50.5 \\u00b13.6 38.7 \\u00b11.5\\nMulti-agent\\nReconcile [10] 68.4 \\u00b17.4 90.6 \\u00b12.5 60.7 \\u00b15.7 59.5 \\u00b18.7 33.3 \\u00b13.4\\nAutoGen [86] 67.3 \\u00b111.8 73.3 \\u00b13.1 64.6 \\u00b11.2 55.3 \\u00b13.1 43.3 \\u00b14.2\\nDyLAN [51] 56.4 \\u00b12.9 75.3 \\u00b14.6 60.1 \\u00b13.1 57.3 \\u00b16.1 38.7 \\u00b11.2\\nMeta-Prompting [70] 52.6 \\u00b16.1 77.3 \\u00b12.3 64.7 \\u00b13.1 49.3 \\u00b11.2 42.0 \\u00b14.0\\n* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement\\n* T : text-only, I : image+text, V : video+text\\n* All experiments were tested with 3 random seeds\\nTable 11: Accuracy (%) on Medical benchmarks with Our (\\n ) method. Bold represents the\\nbest performance for each benchmark and model.\\nMethod\\nMedical Knowledge Retrieval Datasets\\nMedQA\\nT\\nPubMedQA\\nT\\nPath-VQA\\nI T\\nPMC-VQA\\nI T\\nMedVidQA\\nV T\\nGPT-3.5 64.0 \\u00b11.6 66.0 \\u00b15.7 - - -\\nGPT-4(V) 88.7 \\u00b14.0 75.0 \\u00b11.0 65.3 \\u00b13.9 56.4 \\u00b14.5 -\\nGemini-Pro(Vision) 57.4 \\u00b11.8 71.0 \\u00b11.6 72.0 \\u00b12.3 62.2 \\u00b17.6 56.2 \\u00b16.7\\nMethod\\nClinical Reasoning and Diagnostic Datasets\\nDDXPlus\\nT\\nSymCat\\nT\\nJAMA\\nT\\nMedBullets\\nT\\nMIMIC-CXR\\nI T\\nGPT-3.5 62.5 \\u00b16.7 85.7 \\u00b13.0 48.6 \\u00b16.5 55.3 \\u00b14.3 -\\nGPT-4(V) 77.9 \\u00b12.1 93.1 \\u00b11.0 70.9 \\u00b10.3 80.8 \\u00b11.7 55.9 \\u00b19.1\\nGemini-Pro(Vision) 59.2 \\u00b11.2 65.0 \\u00b16.1 47.0 \\u00b12.2 42.9 \\u00b13.4 48.1 \\u00b15.8\\n* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement\\n* T : text-only, I : image+text, V : video+text\\n* All experiments were tested with 3 random seeds\\n26\\nTable 12: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive settings with increased\\nnumber of samples (N=100). All benchmarks except for MedVidQA ( Gemini 1.5 Flash) were\\nevaluated with GPT-4o mini.\\nCategory Method MedQA\\nT\\nPubMedQA\\nT\\nPath-VQA\\nI T\\nPMC-VQA\\nI T\\nMedVidQA\\nV T\\nSingle-agent\\nZero-shot 75.0 54.0 58.0 48.0 50.0\\nFew-shot 77.0 55.0 58.0 50.0 51.0\\n+ CoT 78.0 50.0 59.0 52.0 53.0\\n+ CoT-SC 79.0 51.0 60.0 53.0 53.0\\nER 76.0 51.0 61.0 51.0 52.0\\nMedprompt 79.0 58.0 60.0 54.0 53.0\\nMulti-agent\\n(Single-model)\\nMajority V oting 79.0 68.0 63.0 52.0 54.0\\nWeighted V oting 80.0 68.0 64.0 51.0 55.0\\nBorda Count 81.0 69.0 62.0 50.0 52.0\\nMedAgents 80.0 69.0 55.0 52.0 50.0\\nMeta-Prompting 82.0 69.0 56.0 49.0 -\\nMulti-agent\\n(Multi-model)\\nReconcile 83.0 70.0 58.0 45.0 -\\nAutoGen 65.0 63.0 45.0 40.0 -\\nDyLAN 68.0 67.0 42.0 48.0 -\\nAdaptive MDAgents (Ours) 87.0 71.0 60.0 55.0 56.0\\nCategory Method DDXPlus\\nT\\nSymCat\\nT\\nJAMA\\nT\\nMedBullets\\nT\\nMIMIC-CXR\\nI T\\nSingle-agent\\nZero-shot 53.0 84.0 57.0 49.0 38.0\\nFew-shot 60.0 87.0 58.0 52.0 33.0\\n+ CoT 66.0 84.0 55.0 64.0 33.0\\n+ CoT-SC 68.0 84.0 57.0 60.0 40.0\\nER 76.0 80.0 56.0 59.0 43.0\\nMedprompt 70.0 84.0 62.0 60.0 43.0\\nMulti-agent\\n(Single-model)\\nMajority V oting 53.0 82.0 56.0 59.0 54.0\\nWeighted V oting 52.0 86.0 56.0 56.0 52.0\\nBorda Count 53.0 86.0 56.0 59.0 51.0\\nMedAgents 56.0 80.9 51.0 58.0 40.9\\nMeta-Prompting 53.0 79.0 56.0 51.0 48.0\\nMulti-agent\\n(Multi-model)\\nReconcile 60.0 87.0 59.0 60.0 43.3\\nAutoGen 47.0 87.0 53.0 55.0 47.0\\nDyLAN 54.0 84.0 55.0 57.0 42.0\\nAdaptive MDAgents (Ours) 75.0 89.0 59.0 67.0 56.0\\n* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement\\n* T : text-only, I : image+text, V : video+text\\n27\\nFigure 8: Complexity Distribution for each dataset classified by GPT-4(V) and Gemini-Pro (Vision)\\n(for MedVidQA). The plot illustrates the varying levels of medical complexity across datasets,\\nreflecting the diverse nature of medical question answering, diagnostic reasoning, and medical visual\\ninterpretation tasks. For instance, MedQA is categorized under Medical Knowledge Retrieval due to\\ntheir focus on text-based questions and literature synthesis, while MIMIC-CXR, categorized under\\nClinical Reasoning and Diagnostic tasks, shows a high complexity distribution due to the need for\\ninterpreting detailed radiographic images (See Section in Section 4.1 for the task categorization)\\n28\\n: text-only: image+ text: video+ text\\nFigure 9: Top-3 most recruited medical experts in each benchmark. The alignment between the\\ndataset characteristics and the recruited experts is evident in several cases. For instance, MIMIC-CXR,\\nwhich features chest x-ray images, predominantly recruits Radiologists, Pulmonologists, and experts\\nin Biomedical Informatics due to their expertise in interpreting medical imaging.\\nFigure 10: Simplified agent structure examples assigned during the expert recruitment process ranging\\nfrom (a) A Primary Care Clinician (PCC), (b) Multi-disciplinary Team (MDT), (C) MDT w/ hierarchy\\nto (d) Integrated Care Team (ICT).\\n29\\nAlgorithm 1 Adaptive Medical Decision-making Framework\\nRequire: Problem Q\\n1: Complexity \\u2190 COMPLEXITY CHECK (Q) \\u25b7 Determine the complexity of the medical query\\n2: if Complexity = low then\\n3: Agent \\u2190 RECRUIT (Q, Complexity) \\u25b7 Recruit a Primary Care Clinician agent\\n4: ans \\u2190 Agent(Q)\\n5: else if Complexity = moderate then\\n6: MDT \\u2190 RECRUIT (Q, Complexity) \\u25b7 Recruit a Multi-disciplinary Team\\n7: Agent \\u2190 RECRUIT (Q, Complexity, MDT)\\n8: r \\u2190 0\\n9: Consensus \\u2190 False\\n10: Interaction \\u2190 []\\n11: while r \\u2264 R and not Consensus do\\n12: Consensus, Log\\u2190 COLLABORATIVE DISCUSSION (Q, MDT) \\u25b7 Iterative discussions\\n13: if not Consensus then\\n14: for all Agent \\u2208 MDT do\\n15: F eedback\\u2190 Moderator (Interaction, Agent) \\u25b7 Review and provide feedback\\n16: Agent.UPDATE (F eedback) \\u25b7 Update the feedback\\n17: end for\\n18: Interaction \\u2190 Interaction + [Log] + [F eedback]\\n19: end if\\n20: r \\u2190 r + 1\\n21: end while\\n22: ans \\u2190 Agent(Q, Interaction) \\u25b7 Moderator agent makes the final decision\\n23: else\\n24: ICT \\u2190 RECRUIT (Q, Complexity) \\u25b7 Recruit an Integrated Care Team\\n25: Reports \\u2190 []\\n26: for T eam\\u2208 ICT do\\n27: Report \\u2190 GENERATE REPORT (Q, T eam) \\u25b7 Each Team curates a report\\n28: Reports \\u2190 Reports + [Report]\\n29: end for\\n30: ans \\u2190 Agent(Q, Reports) \\u25b7 Final decision made\\n31: end if\\n32: return ans\\n30\\nE Case Study\\nE.1 Medical Decision Making Case Studies\\nMDM requires efforts of both individual expertise and collaboration to navigate the complexities of\\npatient care. Clinicians often face challenging scenarios that necessitate a comprehensive approach,\\nintegrating insights from various specialties to arrive at the best possible outcomes.\\nE.1.1 Real-World Medical Cases\\nBelow are the real-world example cases that could be classified as low, moderate, to high complexity\\ncases.\\nCase 1: Adjusting Medication Dosage for Chronic Disease (Low Complexity) A 55-year-old\\nfemale patient with type 2 diabetes visits her PCP for a routine check-up. The patient has been\\ntaking 500 mg of metformin orally twice a day and has been adhering to a low-carbohydrate diet.\\nUpon testing with fasting glucose level, the glucose level is above normal. PCP reviewed the current\\nmedical dosage and increased the dosage to manage the blood glucose level of the patient.\\nCase 2: Differential Diagnosis in the Emergency Department (Moderate Complexity) A 40-\\nyear-old male patient arrives at the emergency department (ED) with a high fever, severe headache,\\nand muscle pain, raising concerns about a potential infectious disease. The ED physician conducts an\\ninitial examination but recognizes the need for a more detailed evaluation to identify the underlying\\ncause. The patient is referred to the infectious disease department for further assessment. An\\ninfectious disease specialist, along with the ED physician, reviews the patient\\u2019s symptoms, travel\\nhistory, and recent exposures. They collaborate on ordering specific diagnostic tests, including blood\\ncultures and imaging studies. Through this teamed decision-making process, they diagnose the patient\\nwith dengue fever and promptly initiate appropriate antiviral treatment.\\nCase 3: Managing Adverse Responses to Medication in Chronic Disease (High Complexity)\\nA 60-year-old female patient with chronic heart failure has been experiencing new symptoms of\\nshortness of breath and mild fever, suggesting either a complication due to her chronic heart failure\\nor a new infection. The urgent care doctor identifies the severity of the situation and promptly refers\\nthe patient to the emergency department of a large hospital, where the patient has triaged to see a\\ncardiologist and an infectious disease doctor for specialized care. The team conducts a detailed review\\nof the patient\\u2019s medication history and current symptoms, does a physical exam to listen to lung\\nsounds, and orders a few exams including labs, a chest x-ray, echocardiogram, and electrocardiogram.\\nThe team identifies that the patient has pulmonary effusion and upper respiratory viral infection.\\nE.1.2 Medical Cases from MedQA Dataset\\nNow, let us look at the cases from the MedQA [ 35] dataset that illustrate either individual PCP\\nor teamed decision-making is crucial in managing medical conditions, ranging from low to high\\ncomplexity levels of potential cases. These examples highlight the importance of checking the\\ncomplexity of the case for proper management.\\nCase 4: Diagnosis by PCP The case below with the \\u201cLow Complexity\\u201d header is classified as low\\ncomplexity by a medical doctor. In this case, a PCP can answer this question without consulting a\\ngastroenterologist. The diagnosis of gastric cancer and management based on the manifestation of\\nthe disease, that has been described in this question and beyond should be from a gastroenterologist.\\nHowever, PCPs are expected to have the basic scientific and pathophysiological knowledge that is\\nrelated to gastric cancer and use that knowledge to solve this problem.\\nCase 5: Diagnosis and management by single Pediatric Endocrinologist The case below with\\nthe \\u201cModerate Complexity\\u201d header is classified as moderate complexity by the medical doctor. In\\nthis case, a pediatric endocrinologist (specialist) alone can diagnose a patient and have a treatment\\nplan. Note that this patient could have been referred to this pediatric endocrinologist by a PCP who is\\nregularly seeing this patient.\\n31\\nQuestion: A 70-year-old man comes to the physician because of a 4-month history of epigastric pain,\\nnausea, and weakness. He has smoked one pack of cigarettes daily for 50 years and drinks one alcoholic\\nbeverage daily. He appears emaciated. He is 175 cm (5 ft 9 in) tall and weighs 47 kg (103 lb); BMI is 15\\nkg/m\\u00b2. He is diagnosed with gastric cancer. Which of the following cytokines is the most likely direct\\ncause of this patient\\u2019s examination findings?\\nAnswer:\\nA) TGF-\\u03b2\\nB) IL-6\\nC) IL-2\\nD) TNF-\\u03b2\\nLow Complexity\\nQuestion: A 5-year-old girl is brought to the clinic by her mother for excessive hair growth. Her mother\\nreports that for the past 2 months she has noticed hair at the axillary and pubic areas. She denies any\\nfamily history of precocious puberty and reports that her daughter has been relatively healthy with an\\nuncomplicated birth history. She denies any recent illnesses, weight change, fever, vaginal bleeding, pain,\\nor medication use. Physical examination demonstrates Tanner stage 4 development. A pelvic ultrasound\\nshows an ovarian mass. Laboratory studies demonstrate an elevated level of estrogen. What is the most\\nlikely diagnosis?\\nAnswer:\\nA) Granulosa cell tumor\\nB) Idiopathic precocious puberty\\nC) McCune-Albright syndrome\\nD) Sertoli-Leydig tumor\\nModerate Complexity\\nCase 6: Diagnosis and management by multidisciplinary team The case below is of a \\u201cHigh\\nComplexity\\u201d patient primarily having neurological symptoms but with problems with vision, which\\nrequires a neurologist to consult to ophthalmology department for further evaluation.\\nQuestion: A 63-year-old woman presents to her primary-care doctor for a 2-month history of vision\\nchanges, specifically citing the gradual onset of double vision. Her double vision is present all the time\\nand does not get better or worse throughout the day. She has also noticed that she has a hard time\\nkeeping her right eye open, and her right eyelid looks \\u2019droopy\\u2019 in the mirror. Physical exam findings\\nduring primary gaze are shown in the photo. Her right pupil is 6 mm and poorly reactive to light. The\\nrest of her neurologic exam is unremarkable. Laboratory studies show an Hb A1c of 5.0%. Which of the\\nfollowing is the next best test for this patient?\\nAnswer:\\nA) Direct fundoscopy\\nB) Intraocular pressures\\nC) MR angiography of the head\\nD) Temporal artery biopsy\\nHigh Complexity\\nE.2 Cases Studies with MDAgents\\nIn this section, we provide two examples of our framework with moderate (Figure 11) and high\\n(Figure 12) complexity in PMC-VQA (image+text) and DDXPlus (text-only) respectively. These\\n32\\nQuestion: What does the image of Gross Sectioned Myelolipoma demonstrate?\\nSurgeon(3)\\nModerator\\nOncologist(4)\\nEndocrinologist(5)\\nPathologist(1)\\nRadiologist(2)\\nStep 2. Expert Recruitment\\n(A) fibrosis(B) inflammation(C) hemorrhage(D) calcification\\nStep 3. Initial Assessment\\n(1) Answer: (A) fibrosis\\n(2) Answer: (A) fibrosis\\n(3) Answer:(C) hemorrhage\\n(4) Answer:(C) hemorrhage\\n(5) Answer: (A) fibrosis\\nStep 4. Collaborative Discussion\\n(3)\\u2192 (1): I am a surgeon who specializes in surgical procedures for various medical conditions. I have extensive experience in diagnosing and treating a wide range of surgical conditions, including cancer, trauma, cardiovascular disease, and gastrointestinal disorders.I believe that the image shows (C) hemorrhage (red arrow). Hemorrhageis the escape of blood from a blood vessel. It can occur due to a variety of factors, including trauma, bleeding disorders, and certain medical conditions. In the image, the red arrow points to an area of the tissue that is dark red and appears to be surrounded by a thin rim of white tissue. This is consistent with the appearance of hemorrhage.\\n(4)\\u2192 (3): I am an oncologist who specializes in the diagnosis and treatment of cancer. I believe that the image of Gross Sectioned Myelolipoma demonstrates (C) hemorrhage (red arrow). Hemorrhage is the escape of blood from a blood vessel. It can occur due to a variety of factors, including trauma, bleeding disorders, and certain medical conditions.In the image, the red arrow points to an area of the tissue that is dark red and appears to be surrounded by a thin rim of white tissue. This is consistent with the appearance of hemorrhage.I am confident in my opinion because I have extensive experience in diagnosing and treating cancer. I have seen many cases of hemorrhage, and I am familiar with the appearance of this condition.I would like to discuss my interpretation of the image with you. I believe that the image shows hemorrhage. I am confident that we can come to a consensus on the diagnosis.\\nRound 1\\nStep 1. Complexity CheckIntermediateWhile the identification of gross pathology might seem straightforward to a specialist in pathology, the distinction between fibrosis, hemorrhage, and other conditions can sometimes be subtle and subject to interpretation, as evidenced by the detailed discussion and disagreement among the agents with different medical backgrounds in the provided scenario.\\nTurn 1\\n(2)\\u2192 (3): I am a radiologist who focuses on the interpretation of medical imaging, such as X-rays, CT scans, MRIs, and ultrasounds. I have extensive experience in diagnosing and treating a wide range of medical conditions using medical imaging.I believe that the image demonstrates (A) fibrosis (red arrow). Fibrosis is a common finding in myelolipomas, which are benign tumors composed of mature fat cells and hematopoietic tissue. (C) Hemorrhage, on the other hand, is less common and typically appears as a dark red or black area on imaging. I am confident in my interpretation of the image and believe that the surgeon may have misinterpreted the findings.\\nTurn 2 Turn 2I do not want to talk to any other experts.I am confident in my opinion that the image shows (C) hemorrhage. I have carefully considered the opinions of the other medical experts, but I believe that the area pointed to by the red arrow is more consistent with the appearance of hemorrhage than fibrosis.I am open to discussing my opinion further with the other medical experts, but I do not believe that it is necessary to talk to any other experts at this time.\\n(3)\\u2192 (2): I am a surgeon who specializes in surgical procedures for various medical conditions.I believe that the image shows (C) hemorrhage (red arrow). Hemorrhage is the escape of blood from a blood vessel. It can occur due to a variety of factors, including trauma, bleeding disorders, and certain medical conditions. (\\u2026)\\nTurn 1\\n Turn 2\\nStep 5. Review and Final Decision\\nThe decision is challenging due to the conflicting interpretations of the image. However, the arguments for hemorrhage by Agent 3 (Surgeon) and Agent 4 (Oncologist) are based on the visual characteristics that are grossly consistent with hemorrhage \\u2013specifically, the dark red area suggested by the red arrow. In clinical practice, the appearance of hemorrhage in a gross section is often a dark red coloration, which is indeed what the image seems to display.While the expertise of Agent 1 (Pathologist) in tissue examination is crucial and their argument for fibrosisis noted, the gross pathological features described by Agent 3 (Surgeon) and Agent 4 (Oncologist) seem to align more closely with the typical presentation of a hemorrhage. Furthermore, hemorrhage is a well-documented complication in myelolipomas, particularly larger ones or those subjected to trauma.Therefore, based on the analysis of the conversation history, the final decision is aligned with interpretation (C) hemorrhage (red arrow)as argued by Agent 3 (Surgeon) in Round 1, Turns 1 and 3, and Agent 4 (Oncologist) in Round 1, Turn 2. This conclusion is supported by the gross pathological features consistent with hemorrhage rather than fibrosis, despite the reasoned opinions for fibrosis by Agents 1 (Pathologist), 2 (Radiologist), and 5 (Endocrinologist).\\nInteraction Log\\nTurn 1Turn 2Turn 3\\nTurn 4\\nRound 1\\n(4)\\u2192 (1): I am an oncologist who specializes in the diagnosis and treatment of cancer. I believe that the image of Gross Sectioned Myelolipoma demonstrates (C) hemorrhage (red arrow).Hemorrhage is the escape of blood from a blood vessel. I understand that you believe the image shows (A) fibrosis, but I disagree. I believe that the image shows hemorrhage because the area pointed to by the red arrow is dark red and appears to be surrounded by a thin rim of white tissue. This is consistent with the appearance of hemorrhage, not fibrosis.I am also aware that you are a specialist in the study and diagnosis of disease through examination of tissue samples, but I believe that my expertise in oncology gives me a unique perspective on this image.I believe that my argument is strong and that I can convince the pathologist of my opinion. I am confident in my diagnosis of hemorrhage, and I am willing to discuss the image further to support my opinion.\\nTurn 3\\nGT: (C) hemorrhage\\n: correct opinion: incorrect opinion\\nTerminate\\nFigure 11: Illustration of our proposed framework in moderate complexity setting. Given a medical\\nquery (image + text) the framework performs reasoning in five steps: (i) complexity check, (ii)\\nexpert recruitment, (iii) initial assessment, (iv) collaborative discussion, and (v) review and final\\ndecision-making. Green text represents the correct answer and the Red text represents the incorrect\\nanswer.\\ncase studies reveals how our framework provides an environment for agents to collaborate, gather\\ninformation, moderate and make final decisions in complex medical scenarios.\\n33\\nQuestion: Based on the following information, please indicate the most likely diagnosis:Step 2. Expert Recruitment(A)Myasth\\u00e9nie grave (B)Syndrome de Guillain-Barr\\u00e9 (C)Embolie pulmonaire (D) Myocardite (E) R\\u00e9action dystonique a\\u00efgue (F) OAP/Surcharge pulmonaire (G) Fibrillation auriculaire/Flutter auriculaire (H) An\\u00e9mie (I) Anaphylaxie (J) Chagas (K) Scombro\\u00efde\\nStep 3. Information Gathering and Report Generation \\nNeurologist (Lead)\\u2192 Ophthalmologist, Speech Therapist: Based on the symptoms presented, which include jaw claudication (claud_m\\u00e2choire), double vision (diplopie), speech disorder (dysarthrie), difficulty swallowing (dysphagie), muscle weakness (faiblesse_msmi), eyelid dropping (ptose), and unusual fatigue (fatigabilit\\u00e9_msk), the most likely diagnosis appears to be (A) Myasthenia Gravis (Myasth\\u00e9niegrave).For further investigations:1. Ophthalmologist: Since double vision and eyelid drooping are some of the common symptoms of Myasthenia Gravis, the ophthalmologist may conduct a comprehensive eye examination. This might include testing for alterations in eye movement and eyelid strength.2. Speech therapist: Given the occurrence of dysarthria (speech disorder) and dysphagia (difficulty in swallowing), a Speech therapist may be involved to evaluate the patient's speech and swallowing abilities. They may conduct a detailed evaluation which includes observations of strength and coordination of speech muscles, assessment of speech clarity, and swallowing test to confirm the diagnosis.\\nStep 1. Complexity CheckHighThe query is classified as highcomplexity due to the diverse and severe neurological symptoms like double vision, slurred speech, and muscle weakness, which suggest multiple underlying conditions. This complexity necessitates collaboration among multiple specialists, including neurologists and pulmonologists, to accurately diagnose and treat the patient, involving extensive differential diagnosis and integrated clinical decision-making.\\nStep 4. Final Decision\\nModerator: Based on the provided patient information and clinical evidence, the most likely diagnosis is (A) Myasthenia Gravis (Myasth\\u00e9niegrave). The symptoms such as fatigue, weakness in the muscles, difficulty in swallowing (dysphagia), difficulty in speaking (dysarthrie), double vision (diplopie), and drooping of one or both eyelids (ptose) are all indicative of Myasthenia Gravis.\\nGT: (A) Myasth\\u00e9nie grave\\nPatient Information\\u00a7Age: 20\\u00a7Sex:M\\u00a7Initial Evidence: fatigabilit\\u00e9_msk\\u00a7ClinicalEvidence: ['atcdfam_mg', 'claud_m\\u00e2choire', 'diplopie', 'dysarthrie', 'dysphagie', 'dyspn', 'faiblesse_msmi', 'fatigabilit\\u00e9_msk', 'ptose', 'trav1_@_N']\\n Speech\\nOphthalmologist\\nNeurologist\\nRheumatologist\\nModerator\\nIAT\\nElectrophysiologistPulmonologist\\nDietitianPhysical Therapist\\nClinical Psychologist\\nDET\\nPHT\\nPatient AdvocateCare Coordinator\\nSenior Consultants\\nPRDT\\nOphthalmologist: As an ophthalmologist on the Initial Assessment Team, I focused my investigation on the ocular symptoms presented by the patient, particularly diplopia and ptosis, both indicative of Myasthenia Gravis (MG).(\\u2026)The key findings were as follows:1. Ptosis: Both eyes exhibited a noticeable droop, more pronounced in the upward gaze, indicating lid lag typical of MG.2. Diplopia: The patient reported double vision, particularly in lateral and upward gaze. Change in diplopia with different gaze directions is consistent with MG.Therefore, based on my assessment, the collective ocular symptoms of ptosis and diplopia are highly suggestive of Myasthenia Gravis.\\nSpeech Therapist: As a speech-language pathologist in an Initial Assessment Team (IAT) tackling the case of a potential Myasthenia Gravis patient, my role incorporates evaluating the degree and severities of the dysarthria and dysphagia symptoms.In this context, the dysarthria will present as disrupted speech fluency, characterized by difficulty articulating words. As for dysphagia, this entails troubles in swallowing. These symptoms surface as Myasthenia Gravis cause weakness in the patient's skeletal muscles, which are the muscles necessary for speech and swallowing mechanics.(\\u2026)The findings from these investigations will not only support the Myasthenia Gravis diagnosis, they will also provide foundational data for the creation of a targeted rehabilitation plan. The plan will aim to maintain oral intake while ensuring safety, preventing any potential complications like aspiration pneumonia.\\nTeam 1. Initial Assessment Team (IAT)\\nSenior Consultant (Lead)\\u2192 Clinical Care Coordinator, Patient Advocate: Given the patient's symptoms, the most likely diagnosis is (A) Myasthenia Gravis, a neuromuscular disease causingmuscle weakness and fatigue. However, it's important to conduct an in-depth investigation with our team of clinicians.1. Clinical Care Coordinator: This individual should help coordinate necessary medical tests to confirm this diagnosis. Tests may include blood tests to check for antibodies contributing to muscle weakness, nerve conduction studies, or an edrophonium test. Additionally, they will also organize appropriate care resources and ensure effective communications across the team.2. Patient Advocate: As an advocate, this team member needs to communicate between the patient and the medical team. They need to ensure that the patient understands the potential diagnosis, the implications, and the subsequent steps in the investigative process and treatment. Their goal is to offer emotional support and make sure the patient's concerns or questions are accurately conveyed to and addressed by the medical team.\\nClinical Care Coordinator: My expertise as a clinical care coordinator involves bringing together different healthcare teams, managing resources and information to ensure the best care for our patients. It involves organizing and streamlining the treatment process, making sure that the care provided is efficient and comprehensive.(\\u2026)Hypothesized Diagnosis: Myasthenia Gravis Recommended Investigations:1. Blood tests to look for specific antibodies present in Myasthenia Gravis.2. Nerve conduction studies to test the nerves' ability to send signals.3. Edrophonium test, where the drug edrophonium, which briefly relieves weakness in people with myasthenia gravis, is used.\\nPatient Advocate: As a Patient Advocate, I understand that while the diagnosis may be medically accurate, it's key to clarify what this means for the patient's day-to-day life:1. Communication: I spoke to the patient and their family about the potential diagnosis of Myasthenia Gravis, explaining the nature of the condition, common symptoms, and potential treatment options. I also emphasized that further tests are needed before confirming the diagnosis.2. Understand patient's fears and concerns: The patient expressed worries about the impacts of the disease on their daily life and potential side effects of medication. They are particularly concerned about the potential for extreme muscle weakness and how this might disrupt their current job and responsibilities.3. Support: I assured the patient that our multidisciplinary team is dedicated to providingthe best care for them and that we are open to discussing any concerns they have regarding their condition.\\nTeam 4. Final Review and Decision Team (FRDT)\\nSummarized Report\\nCombined Report\\nFigure 12: Illustration of our proposed framework in high complexity setting. Given a medical\\nquery (text-only) the framework performs reasoning in four steps: (i) complexity check, (ii) expert\\nrecruitment, (iii) information gather and report generation, (iv) final decision. Green text represents\\nthe correct answer.\\nF Medical Complexity Comparison with Human Physicians\\nThe core premise of our MDAgent framework is its ability to adapt to the complexity of medical tasks.\\nTo validate this approach and gain insights into how LLMs perceive medical complexity compared to\\nhuman experts, we conducted an annotation study. This study aimed to explore the alignment between\\nLLMs and physicians in assessing medical question complexity, a critical factor in the effectiveness\\nof our MDAgent framework.\\nStudy Design We selected 50 representative questions from the MedQA dataset, ensuring a balanced\\nrepresentation across USMLE steps 1, 2, and 3. This selection process aimed to cover a wide range of\\nmedical topics and complexity levels, mirroring the diverse challenges that our MDAgent framework\\nis designed to address.\\nThree physicians participated in our study: two with two years of Internal Medicine training (Post\\nGraduate Year 2, PGY-2) and one general physician. This composition allowed us to capture a range\\n34\\nof clinical perspectives. The physicians rated each question on a scale of -1 (low complexity), 0\\n(moderate complexity), and 1 (high complexity).\\nInter-rater Reliability To quantify the agreement among our physician raters, we employed\\nIntraclass Correlation Coefficients (ICC). ICC is a widely used statistical measure in medical research\\nfor assessing the consistency of ratings among multiple raters. We specifically chose two ICC\\nvariants:\\n\\u2022 ICC2k (Two-way random effects, average measures): 0.269 [-0.14, 0.55]\\n\\u2022 ICC3k (Two-way mixed effects, average measures): 0.280 [-0.15, 0.57]\\nICC2k was selected because it assumes our raters are randomly selected from a larger population of\\nsimilar raters, allowing for generalization of our findings. ICC3k, on the other hand, treats the raters\\nas fixed, focusing on the consistency among our specific set of physicians.\\nBoth ICC values indicate moderate agreement among the raters. This level of agreement reflects\\nthe inherent complexity and subjectivity in evaluating medical questions, even among trained pro-\\nfessionals. It also highlights the challenging nature of the task our MDAgent framework aims to\\naddress.\\nAnnotation Interface To facilitate the annotation process for both physicians and LLMs, we\\ndeveloped a specialized interface. This interface was designed to present medical questions in a clear\\nand consistent manner, allowing for efficient and standardized complexity ratings. Figure 13 shows a\\nscreenshot of the annotation interface used in our study.\\nFigure 13: Annotation interface used for medical complexity assessment. The latest version can be\\nfound at https://dxagents.github.io/2024/05/01/medqa.html.\\nLLM Annotations and Comparison To compare LLM performance with human expert judgments,\\nwe employed several state-of-the-art models to annotate the same set of questions. We then compared\\nthese assessments with the majority opinion of the physicians, determined by the mode of their ratings\\n(or the mean in cases of complete disagreement).\\nTable 13 presents the Pearson correlation between each LLM\\u2019s complexity ratings and the physicians\\u2019\\nmajority opinions:\\n35\\nModel Correlation with Physician Majority\\ngpt-4o-mini -0.090\\ngpt-4o 0.022\\ngpt-4 0.070\\ngemini-1.5-flash 0.110\\nTable 13: Correlation between LLM complexity ratings and physician majority opinions\\nThe results of our study provide valuable insights into the current state of LLM capabilities in medical\\ncomplexity assessment and underscore the importance of our MDAgent framework:\\n1. Subjectivity in medical complexity: The moderate ICC values among physicians highlight\\nthe inherent subjectivity in assessing medical question complexity. This finding validates\\nour approach in MDAgent, which doesn\\u2019t rely on a single, fixed assessment of complexity\\nbut rather adapts its collaboration structure dynamically.\\n2. Current LLM limitations: The low correlations between LLM and human assessments\\nindicate that current LLMs may not fully capture the nuances that human experts consider\\nwhen evaluating medical complexity. This observation reinforces the need for our MDAgent\\nframework, which can compensate for individual LLM limitations through collaborative\\ndecision-making.\\n3. Potential for improvement: The variation in correlation across different LLM models\\n(from -0.090 to 0.110) suggests there is room for improvement in LLM performance. This\\naligns with our MDAgent approach, which can leverage the strengths of multiple models\\nand adapt to future improvements in LLM capabilities.\\n4. Value of human expertise: The discrepancy between LLM and physician assessments un-\\nderscores the continued importance of human medical expertise. Our MDAgent framework\\nacknowledges this by incorporating human-like collaboration strategies and the potential for\\nhuman oversight in critical decisions.\\n5. Adaptability of MDAgent: The challenges revealed in this study highlight the wisdom of\\nour MDAgent\\u2019s adaptive approach. By dynamically adjusting its collaboration structure\\nbased on perceived task complexity, MDAgent can mitigate the limitations of individual\\nLLMs and approach the nuanced understanding demonstrated by human experts.\\n36\\nNeurIPS Paper Checklist\\n1. Claims\\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\\npaper\\u2019s contributions and scope?\\nAnswer: [Yes]\\nJustification: The abstract and introduction state the primary contributions of the paper,\\nincluding the introduction of the MDAgents framework, its adaptive decision-making\\nstructure, and the significant performance improvements demonstrated through experimental\\nresults.\\nGuidelines:\\n\\u2022 The answer NA means that the abstract and introduction do not include the claims\\nmade in the paper.\\n\\u2022 The abstract and/or introduction should clearly state the claims made, including the\\ncontributions made in the paper and important assumptions and limitations. A No or\\nNA answer to this question will not be perceived well by the reviewers.\\n\\u2022 The claims made should match theoretical and experimental results, and reflect how\\nmuch the results can be expected to generalize to other settings.\\n\\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals\\nare not attained by the paper.\\n2. Limitations\\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\\nAnswer: [Yes]\\nJustification: The paper includes a section on limitations, discussing the scope of the\\nframework, and areas for future improvement.\\nGuidelines:\\n\\u2022 The answer NA means that the paper has no limitation while the answer No means that\\nthe paper has limitations, but those are not discussed in the paper.\\n\\u2022 The authors are encouraged to create a separate \\\"Limitations\\\" section in their paper.\\n\\u2022 The paper should point out any strong assumptions and how robust the results are to\\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\\nmodel well-specification, asymptotic approximations only holding locally). The authors\\nshould reflect on how these assumptions might be violated in practice and what the\\nimplications would be.\\n\\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was\\nonly tested on a few datasets or with a few runs. In general, empirical results often\\ndepend on implicit assumptions, which should be articulated.\\n\\u2022 The authors should reflect on the factors that influence the performance of the approach.\\nFor example, a facial recognition algorithm may perform poorly when image resolution\\nis low or images are taken in low lighting. Or a speech-to-text system might not be\\nused reliably to provide closed captions for online lectures because it fails to handle\\ntechnical jargon.\\n\\u2022 The authors should discuss the computational efficiency of the proposed algorithms\\nand how they scale with dataset size.\\n\\u2022 If applicable, the authors should discuss possible limitations of their approach to\\naddress problems of privacy and fairness.\\n\\u2022 While the authors might fear that complete honesty about limitations might be used by\\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\\nlimitations that aren\\u2019t acknowledged in the paper. The authors should use their best\\njudgment and recognize that individual actions in favor of transparency play an impor-\\ntant role in developing norms that preserve the integrity of the community. Reviewers\\nwill be specifically instructed to not penalize honesty concerning limitations.\\n3. Theory Assumptions and Proofs\\n37\\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\\na complete (and correct) proof?\\nAnswer: [NA]\\nJustification: The paper does not include theoretical results that require assumptions or\\nproofs.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not include theoretical results.\\n\\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-\\nreferenced.\\n\\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.\\n\\u2022 The proofs can either appear in the main paper or the supplemental material, but if\\nthey appear in the supplemental material, the authors are encouraged to provide a short\\nproof sketch to provide intuition.\\n\\u2022 Inversely, any informal proof provided in the core of the paper should be complemented\\nby formal proofs provided in appendix or supplemental material.\\n\\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.\\n4. Experimental Result Reproducibility\\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\\nof the paper (regardless of whether the code and data are provided or not)?\\nAnswer: [Yes]\\nJustification: The paper provides comprehensive details on datasets, experimental setups,\\nand methodologies used, ensuring that the results can be reproduced accurately.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not include experiments.\\n\\u2022 If the paper includes experiments, a No answer to this question will not be perceived\\nwell by the reviewers: Making the paper reproducible is important, regardless of\\nwhether the code and data are provided or not.\\n\\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken\\nto make their results reproducible or verifiable.\\n\\u2022 Depending on the contribution, reproducibility can be accomplished in various ways.\\nFor example, if the contribution is a novel architecture, describing the architecture fully\\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\\nbe necessary to either make it possible for others to replicate the model with the same\\ndataset, or provide access to the model. In general. releasing code and data is often\\none good way to accomplish this, but reproducibility can also be provided via detailed\\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\\nof a large language model), releasing of a model checkpoint, or other means that are\\nappropriate to the research performed.\\n\\u2022 While NeurIPS does not require releasing code, the conference does require all submis-\\nsions to provide some reasonable avenue for reproducibility, which may depend on the\\nnature of the contribution. For example\\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\\nto reproduce that algorithm.\\n(b) If the contribution is primarily a new model architecture, the paper should describe\\nthe architecture clearly and fully.\\n(c) If the contribution is a new model (e.g., a large language model), then there should\\neither be a way to access this model for reproducing the results or a way to reproduce\\nthe model (e.g., with an open-source dataset or instructions for how to construct\\nthe dataset).\\n(d) We recognize that reproducibility may be tricky in some cases, in which case\\nauthors are welcome to describe the particular way they provide for reproducibility.\\nIn the case of closed-source models, it may be that access to the model is limited in\\nsome way (e.g., to registered users), but it should be possible for other researchers\\nto have some path to reproducing or verifying the results.\\n38\\n5. Open access to data and code\\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\\ntions to faithfully reproduce the main experimental results, as described in supplemental\\nmaterial?\\nAnswer: [Yes]\\nJustification: The paper includes a link to the code repository and provides sufficient\\ninstructions in the supplemental material to reproduce the experiments.\\nGuidelines:\\n\\u2022 The answer NA means that paper does not include experiments requiring code.\\n\\u2022 Please see the NeurIPS code and data submission guidelines ( https://nips.cc/\\npublic/guides/CodeSubmissionPolicy) for more details.\\n\\u2022 While we encourage the release of code and data, we understand that this might not be\\npossible, so \\u201cNo\\u201d is an acceptable answer. Papers cannot be rejected simply for not\\nincluding code, unless this is central to the contribution (e.g., for a new open-source\\nbenchmark).\\n\\u2022 The instructions should contain the exact command and environment needed to run to\\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\\n\\u2022 The authors should provide instructions on data access and preparation, including how\\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\\n\\u2022 The authors should provide scripts to reproduce all experimental results for the new\\nproposed method and baselines. If only a subset of experiments are reproducible, they\\nshould state which ones are omitted from the script and why.\\n\\u2022 At submission time, to preserve anonymity, the authors should release anonymized\\nversions (if applicable).\\n\\u2022 Providing as much information as possible in supplemental material (appended to the\\npaper) is recommended, but including URLs to data and code is permitted.\\n6. Experimental Setting/Details\\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\\nresults?\\nAnswer: [Yes]\\nJustification: The paper specifies all relevant experimental details, including data splits,\\nnumber of samples, and number of seeds, ensuring transparency and reproducibility of the\\nresults.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not include experiments.\\n\\u2022 The experimental setting should be presented in the core of the paper to a level of detail\\nthat is necessary to appreciate the results and make sense of them.\\n\\u2022 The full details can be provided either with the code, in appendix, or as supplemental\\nmaterial.\\n7. Experiment Statistical Significance\\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\\ninformation about the statistical significance of the experiments?\\nAnswer: [Yes]\\nJustification: The paper reports error bars and includes information on the statistical sig-\\nnificance of the experimental results, ensuring a clear understanding of the variability and\\nreliability of the findings.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not include experiments.\\n39\\n\\u2022 The authors should answer \\\"Yes\\\" if the results are accompanied by error bars, confi-\\ndence intervals, or statistical significance tests, at least for the experiments that support\\nthe main claims of the paper.\\n\\u2022 The factors of variability that the error bars are capturing should be clearly stated (for\\nexample, train/test split, initialization, random drawing of some parameter, or overall\\nrun with given experimental conditions).\\n\\u2022 The method for calculating the error bars should be explained (closed form formula,\\ncall to a library function, bootstrap, etc.)\\n\\u2022 The assumptions made should be given (e.g., Normally distributed errors).\\n\\u2022 It should be clear whether the error bar is the standard deviation or the standard error\\nof the mean.\\n\\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should\\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\\nof Normality of errors is not verified.\\n\\u2022 For asymmetric distributions, the authors should be careful not to show in tables or\\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\\nerror rates).\\n\\u2022 If error bars are reported in tables or plots, The authors should explain in the text how\\nthey were calculated and reference the corresponding figures or tables in the text.\\n8. Experiments Compute Resources\\nQuestion: For each experiment, does the paper provide sufficient information on the com-\\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\\nthe experiments?\\nAnswer: [Yes]\\nJustification: The experiments primarily involved inference using API calls to GPT-3.5,\\nGPT-4 (V), and Gemini-Pro (Vision). The type of compute workers, memory, and time\\nof execution are managed by the API providers (OpenAI and Gemini). Details about the\\nnumber of API calls and the specific configurations used for each experiment are provided,\\nensuring reproducibility.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not include experiments.\\n\\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,\\nor cloud provider, including relevant memory and storage.\\n\\u2022 The paper should provide the amount of compute required for each of the individual\\nexperimental runs as well as estimate the total compute.\\n\\u2022 The paper should disclose whether the full research project required more compute\\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\\ndidn\\u2019t make it into the paper).\\n9. Code Of Ethics\\nQuestion: Does the research conducted in the paper conform, in every respect, with the\\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\\nAnswer: [Yes]\\nJustification: The research adheres to the NeurIPS Code of Ethics, ensuring responsible\\nconduct throughout the study.\\nGuidelines:\\n\\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\\n\\u2022 If the authors answer No, they should explain the special circumstances that require a\\ndeviation from the Code of Ethics.\\n\\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid-\\neration due to laws or regulations in their jurisdiction).\\n10. Broader Impacts\\nQuestion: Does the paper discuss both potential positive societal impacts and negative\\nsocietal impacts of the work performed?\\n40\\nAnswer: [Yes]\\nJustification: The paper discusses potential societal impacts, both positive and negative,\\nincluding the improvement of medical decision-making and the risks associated with misuse\\nor biases in the AI system.\\nGuidelines:\\n\\u2022 The answer NA means that there is no societal impact of the work performed.\\n\\u2022 If the authors answer NA or No, they should explain why their work has no societal\\nimpact or why the paper does not address societal impact.\\n\\u2022 Examples of negative societal impacts include potential malicious or unintended uses\\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\\ngroups), privacy considerations, and security considerations.\\n\\u2022 The conference expects that many papers will be foundational research and not tied\\nto particular applications, let alone deployments. However, if there is a direct path to\\nany negative applications, the authors should point it out. For example, it is legitimate\\nto point out that an improvement in the quality of generative models could be used to\\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\\nthat a generic algorithm for optimizing neural networks could enable people to train\\nmodels that generate Deepfakes faster.\\n\\u2022 The authors should consider possible harms that could arise when the technology is\\nbeing used as intended and functioning correctly, harms that could arise when the\\ntechnology is being used as intended but gives incorrect results, and harms following\\nfrom (intentional or unintentional) misuse of the technology.\\n\\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation\\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\\nfeedback over time, improving the efficiency and accessibility of ML).\\n11. Safeguards\\nQuestion: Does the paper describe safeguards that have been put in place for responsible\\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\\nimage generators, or scraped datasets)?\\nAnswer: [Yes]\\nJustification: The paper discusses potential limitations, including the risks of medical\\nhallucinations and false knowledge. It includes guidelines within the main text to ensure\\nresponsible use of the MDAgents framework and publicly available datasets, emphasizing\\nethical usage and adherence to best practices.\\nGuidelines:\\n\\u2022 The answer NA means that the paper poses no such risks.\\n\\u2022 Released models that have a high risk for misuse or dual-use should be released with\\nnecessary safeguards to allow for controlled use of the model, for example by requiring\\nthat users adhere to usage guidelines or restrictions to access the model or implementing\\nsafety filters.\\n\\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors\\nshould describe how they avoided releasing unsafe images.\\n\\u2022 We recognize that providing effective safeguards is challenging, and many papers do\\nnot require this, but we encourage authors to take this into account and make a best\\nfaith effort.\\n12. Licenses for existing assets\\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\\nproperly respected?\\nAnswer: [Yes]\\nJustification: The paper properly credits the creators of existing assets used and clearly states\\nthe licenses and terms of use.\\n41\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not use existing assets.\\n\\u2022 The authors should cite the original paper that produced the code package or dataset.\\n\\u2022 The authors should state which version of the asset is used and, if possible, include a\\nURL.\\n\\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.\\n\\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of\\nservice of that source should be provided.\\n\\u2022 If assets are released, the license, copyright information, and terms of use in the\\npackage should be provided. For popular datasets, paperswithcode.com/datasets\\nhas curated licenses for some datasets. Their licensing guide can help determine the\\nlicense of a dataset.\\n\\u2022 For existing datasets that are re-packaged, both the original license and the license of\\nthe derived asset (if it has changed) should be provided.\\n\\u2022 If this information is not available online, the authors are encouraged to reach out to\\nthe asset\\u2019s creators.\\n13. New Assets\\nQuestion: Are new assets introduced in the paper well documented and is the documentation\\nprovided alongside the assets?\\nAnswer: [NA]\\nJustification: The paper does not introduce any new assets.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not release new assets.\\n\\u2022 Researchers should communicate the details of the dataset/code/model as part of their\\nsubmissions via structured templates. This includes details about training, license,\\nlimitations, etc.\\n\\u2022 The paper should discuss whether and how consent was obtained from people whose\\nasset is used.\\n\\u2022 At submission time, remember to anonymize your assets (if applicable). You can either\\ncreate an anonymized URL or include an anonymized zip file.\\n14. Crowdsourcing and Research with Human Subjects\\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\\ninclude the full text of instructions given to participants and screenshots, if applicable, as\\nwell as details about compensation (if any)?\\nAnswer: [NA]\\nJustification: The paper does not involve crowdsourcing or research with human subjects.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with\\nhuman subjects.\\n\\u2022 Including this information in the supplemental material is fine, but if the main contribu-\\ntion of the paper involves human subjects, then as much detail as possible should be\\nincluded in the main paper.\\n\\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\\nor other labor should be paid at least the minimum wage in the country of the data\\ncollector.\\n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\\nSubjects\\nQuestion: Does the paper describe potential risks incurred by study participants, whether\\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\\napprovals (or an equivalent approval/review based on the requirements of your country or\\ninstitution) were obtained?\\nAnswer: [NA]\\n42\\nJustification: The paper does not involve crowdsourcing or research with human subjects.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with\\nhuman subjects.\\n\\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)\\nmay be required for any human subjects research. If you obtained IRB approval, you\\nshould clearly state this in the paper.\\n\\u2022 We recognize that the procedures for this may vary significantly between institutions\\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\\nguidelines for their institution.\\n\\u2022 For initial submissions, do not include any information that would break anonymity (if\\napplicable), such as the institution conducting the review.\\n43\",\n          \"Richelieu: Self-Evolving LLM-Based Agents for AI\\nDiplomacy\\nZhenyu Guan \\u2662, Xiangyu Kong\\u2663\\u2020B, Fangwei Zhong\\u2660\\u2020B, Yizhou Wang\\u2661\\u2662\\n\\u2662 Institute for Artificial Intelligence, Peking University\\n\\u2663 College of Computer Science, Beijing Information Science and Technology University\\n\\u2660 School of Artificial Intelligence, Beijing Normal University\\n\\u2661 Center on Frontiers of Computing Studies, School of Computer Science,\\nNat\\u2019l Eng. Research Center of Visual Technology, Peking University\\n\\u2020 State Key Laboratory of General Artificial Intelligence, BIGAI\\nBCorresponding authors: xykong@bistu.edu.cn, fangweizhong@bnu.edu.cn\\nAbstract\\nDiplomacy is one of the most sophisticated activities in human society, involv-\\ning complex interactions among multiple parties that require skills in social rea-\\nsoning, negotiation, and long-term strategic planning. Previous AI agents have\\ndemonstrated their ability to handle multi-step games and large action spaces\\nin multi-agent tasks. However, diplomacy involves a staggering magnitude of\\ndecision spaces, especially considering the negotiation stage required. While\\nrecent agents based on large language models (LLMs) have shown potential\\nin various applications, they still struggle with extended planning periods in\\ncomplex multi-agent settings. Leveraging recent technologies for LLM-based\\nagents, we aim to explore AI\\u2019s potential to create a human-like agent capable\\nof executing comprehensive multi-agent missions by integrating three funda-\\nmental capabilities: 1) strategic planning with memory and reflection; 2) goal-\\noriented negotiation with social reasoning; and 3) augmenting memory through\\nself-play games for self-evolution without human in the loop. Project page:\\nhttps://sites.google.com/view/richelieu-diplomacy.\\n1 Introduction\\nDiplomacy, a central element of international relations, is an intricate and multifaceted activity that\\nlies at the heart of human society\\u2019s most complex interactions. It requires various skills such as social\\nreasoning, negotiation, and long-term planning to manage relationships and alliances among multiple\\nparties. Mirroring this complexity, the Diplomacy game involves seven players to control European\\npowers, presenting a challenging strategic landscape that demands advanced negotiation and strategic\\nplanning to succeed.\\nThe AI community has shown an increasing interest in the deployment of AI agents to master\\nsuch games [Shoker et al., 2023, Konya et al., 2023, Kram\\u00e1r et al., 2022, Du\\u00e9\\u00f1ez-Guzm\\u00e1n et al.,\\n2023, Mukobi et al., 2023, Kova\\u02c7c et al., 2023]. The recent breakthrough [Bakhtin et al., 2022] has\\nturned into press diplomacy, which allows communication between players. However, the previous\\nmethods [Bakhtin et al., 2022] heavily rely on domain-specific human data, leading to its poor\\ngeneralization to other scenarios/ applications. The question then arises: Can we build an AI agent\\nthat excels in the art of diplomacy without relying on domain-specific human data?\\nRecently, agents based on the Large Language Model(LLM) have emerged as a promising develop-\\nment for AI agents. The previous applications on personal assistants [Li et al., 2024b], robotics [Cheng\\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\\n2019 DipNet 2022 Cicero 2024 Richelieu\\nRL\\nNo Press Diplomacy\\nHuman Data\\nRL\\nReasoning with Negotiation\\nHuman Data\\nLLMs-based-Agent \\nEvolution in Self-Play\\nNo Human Data\\nSelf-Play\\nEvolution Memory \\nUpdate\\nExperience\\nAcquisition\\nFigure 1: A new paradigm for building AI Diplomacy agent.\\net al., 2024, Yang et al., 2023c], and video games [Wan et al., 2024] have shown the surprising ability\\nof LLM-based agents in communication and planning, benefiting from the emergent ability of com-\\nmon sense reasoning, in-context/ few-shot learning, and sophisticated natural language processing\\non LLMs. However, diplomacy presents a unique set of challenges. It not only requires planning\\nlong-horizon strategic [Qi et al., 2024] and communicating with natural language, but also reasoning\\nand adopting the complex social dynamics with partial observations, including gaining trust and\\nreputation, building rapport, detecting deception, and assessing the reliability of other players.\\nIn this work, we aim to make the first attempt to explore LLMs\\u2019 potential to develop a human-like AI\\ndiplomacy agent. We name the agent Richelieu in memorizing a pivotal figure in European history\\nwho had enduring impacts on French politics, foreign affairs, and state building. To achieve this goal,\\nwe have identified four core and essential capabilities that are crucial for building an LLM-based\\nsocietal agent.\\n1. Social reasoning. This is the basic function for a social agent to interact with others,\\nparticularly for adapting to the dynamic changes in the nation\\u2019s intentions and relationships.\\n2. Balance long- and short-term planning. Diplomacy necessitates a careful balance between\\nshort-term tactics and long-term strategies. An effective AI agent must assess the immediate\\nconsequences of its actions alongside their potential long-term impacts.\\n3. Memory management. A robust memory system is a critical component of learning and\\nimprovement. The AI agent must be able to recall and integrate information from past\\nnegotiations and actions to inform its current and future decision-making processes. This\\nendows the agent with the ability to evolve.\\n4. Self-reflection. An AI agent capable of profound reflection can analyze its own decisions,\\nlearn from its memory experience, and adapt its strategies accordingly.\\nBy integrating these four capabilities, the agent can operate at the highest level of diplomatic\\nsophistication, outperforming the state-of-the-art AI diplomats [Bakhtin et al., 2022].\\nOur contributions can be summarized in three-fold: 1) We introduced a new paradigm for building\\nAI diplomacy agents, compared to previous work (Figure 1). The agent can self-evolve by generating\\nexperience via self-play games, without any task-specific human data. 2) We demonstrate the superior\\nperformance of our agent playing against the SOTA method, e.g., Cicero [Bakhtin et al., 2022], that\\nrelies on a large-scale human demonstration for training. 3) We further analyze the effectiveness of\\neach module in our agent and the generalization of our agent in adopting different LLMs, such as\\nGPT-4 and Llama 3.\\n2 Related work\\nAI Diplomacy. The diplomacy game involves seven players controlling different powers in Europe.\\nIn each turn, players can negotiate for cooperation before making moves to take as many supply\\ncenters as they can. Apparently, this challenging strategy task requires both complex negotiation\\nskills and superior planning capability for player agents to achieve final victory. So far, most previous\\nworks on this task remain focused on the planning strategies (a.k.a. No-Press Diplomacy where no\\ncommunication channels are allowed). The setting remains challenging considering its enormous\\n2\\naction space of 1021 to 1064 per turn (compared with Chess, which has much fewer than 100 actions\\nper turn). No wonder existing efforts rely on human data to play the game. Among the methods, one\\ntypical research is DipNet [Paquette et al., 2019] which uses supervised and reinforcement learning.\\nBased on DipNet, BRPI [Anthony et al., 2020], SearchBot [Gray et al., 2020], DORA [Bakhtin\\net al., 2021], and KL-Regularized search (Diplodocus) [Jacob et al., 2022] were conducted. Recently,\\nresearch has also emerged for the full-setting of Diplomacy, i.e., Press Diplomacy, where players are\\nallowed to negotiate with each other before making their moves in each turn. Such studies [De Jonge\\nand Sierra, 2017][Bakhtin et al., 2022][Jaidka et al., 2024][Kram\\u00e1r et al., 2022] mainly benefit from\\nthe recent thriving language models. Specifically, notable advancements include policy iteration\\nmethods from DeepMind and Meta AI Research\\u2019s equilibrium search agent [Jaidka et al., 2024].\\nHowever, Deepmind proposes to learn negotiation agents based on predefined contracts/protocols\\n[Kram\\u00e1r et al., 2022]. And Meta AI\\u2019s work, instead of one unified architecture, Cicero [Bakhtin\\net al., 2022] integrates a language model for negotiation and an RL model for planning respectively.\\nSuch separately trained models make it inconvenient for agents\\u2019 continual evolution. Moreover, like\\nno-press methods, these approaches heavily rely on human player data for agent training. Unlike these\\napproaches, this paper delves into solving the negotiation and planning in one single self-evolving\\nLLM-based agent model, without any pre-collected human expert training data.\\nLLM-Based Agents. With the emergence and growth of large language models (LLM), there\\nis a growing trend in utilizing LLMs as fundamental controllers for autonomous agents[Wang\\net al., 2024c]. One wide application genre is LLM-based answering engines, which merely cover\\nthe negotiation aspects of Diplomacy. Such systems include HuggingGPT [Shen et al., 2023],\\nGPT4Tools [Yang et al., 2023b] and ToT [Yao et al., 2023], etc. They leverage LLMs to manage\\nAl models, use tools, implement policy iteration, and enhance problem-solving across various tasks.\\nRelated work including AutoGPT, AgentGPT, BabyAGl [Talebirad and Nadiri, 2023], Toolformer\\n[Schick et al., 2023], and Visual ChatGPT aim to improve LLM\\u2019s capabilities in task automation\\nand tool usage. Reflexion, a framework that improves LLMs through linguistic feedback and\\nepisodic memory [Zhang et al., 2024a], facilitating better decision-making across diverse tasks is\\nproposed. Besides [Wang et al., 2024d][Wang et al., 2023a][Wang et al., 2023b][Zhu et al., 2023][Yan\\net al., 2023] apply LLM agents to the complex planning tasks in the well-known open-world game\\nMinecraft[Fan et al., 2022]. Unlike these LLM-based agents which only focus on the negotiation or\\nplanning ability respectively, the proposed approach involves a self-evolving scheme in a self-play\\ngame to handle both of them simultaneously.\\n3 Problem Statement\\nThe Diplomacy game [Wikipedia, 2024, Calhamer, 1974] is set in pre-World War I Europe and\\ninvolves each player (agent) representing one of the seven Great Powers of Europe, such as Germany,\\nFrance, England, Italy, Austria-Hungary, Russia, and Turkey. Each player has a set of military units,\\nincluding armies and fleets, which they can move and use to capture other supply centers. The\\nultimate goal for the agent is to control a majority of the total supply centers on the board by the end\\nof the game\\u2019s Fall phase. It\\u2019s important to note that it is not won by eliminating other players or their\\nunits; it is won by controlling the requisite number of supply centers. This often involves forming and\\nbreaking alliances, negotiating, and sometimes betraying other players to achieve one\\u2019s own goals.\\nIn each turn, agent i gets the current state st \\u2208 S, the actions of other players from the previous\\nturn \\u20d7 a\\u2212i\\nt\\u22121, and the messages \\u20d7 m\\u2212i,i\\nt from other players during this turn\\u2019s negotiations. The state st\\nfor the environment includes the ownership of each territory on the map by a particular country and\\nwhere the armies of each country are located. Based on this information, the agent needs to engage in\\nnegotiations with other players, sending messages \\u20d7 mi,\\u2212i\\nt to chat with other players, and then take the\\nactions ai\\nt in this turn. The possible actions an agent can take ai\\nt \\u2208 A are commands to the armies,\\nsuch as moving into an adjacent territory, supporting another unit, or holding a position. Actions can\\nalso include diplomatic moves, such as proposing or withdrawing from an alliance, although these\\nare less formalized in the game mechanics.[Paquette et al., 2019, Hill, 2014]\\n4 Self-Evolving LLM-based Diplomat\\nWe have constructed a comprehensive framework with modules for memory management, social\\nreasoning, strategic planning, negotiation, decision-making, memory update, and self-evolving to\\n3\\nv.s.\\nENVIRONMENT\\nSocial \\nReasoning\\nIntention\\nRelationship\\nPlanner with Reflection\\nNegotiator\\nActor\\nMemory\\nMAP\\nRULE\\nDomian\\nKnowledge\\nSELF-PLAY\\nSub-goals\\nINIT\\nSummarized\\nu ___u ___\\nUpdated \\nSocial Belief\\nSocial \\nBelief\\nStates\\nActions\\nSub-goals\\nMessages\\nExperience \\nAbstract\\nFigure 2: The framework of the proposed LLM-based-agent, Richelieu. It can explicitly reason social\\nbeliefs, propose sub-goals with reflection, negotiate with others, and take actions to master diplomacy.\\nIt augments memories by self-play games for self-evolving without any human annotation.\\nfully leverage the capabilities of LLMs. Richelieu starts by setting up with map details, game rules,\\ndomain knowledge, and the long-term goal.[Zhang et al., 2022, Wei et al., 2022, Wang et al., 2022a]\\nAt each turn, the agent will run in the following steps: 1) Social Reasoning: First of all, the agent\\nundergoes a comprehensive analysis of the game state st to build the social belief, including the\\nintention of other players and their relationship \\u20d7\\u03d5t \\u2208 \\u03a6n.[Zhang et al., 2024c, G\\u00fcrcan, 2024] 2)\\nPlanner with Reflection: Then, the agent proposes sub-goals \\u03c7i\\nt \\u2208 X that is strategically aligned\\nwith the long-term goals \\u03a5, with the social belief and refining the proposed goal with experience\\n\\u20d7 \\u03b7t \\u2208 Hm abstract from the memory M via self-reflection.[Wang et al., 2024b,e] 3) Negotiator: To\\nachieve the sub-goals, the negotiator will start a dialogue session with some players, and evaluate\\ntheir trueness \\u20d7\\u03c8\\u2212i\\nt by referring to their messages \\u20d7 m\\u2212i,i\\nt , the current state st, their sincerity \\u20d7 \\u03b3\\u2212i\\nt and\\nthe experience \\u20d7\\u03bet.[Abdelnabi et al., 2023, Bianchi et al., 2024] 4) Actor: After negotiation, the actor\\ndecides its course of action ai\\nt, based on the sub-goal \\u03c7i\\nt and updated social state st+1, marking\\nthe end of that turn. 5) Memory Management: The state of the current turn st, the content of\\nnegotiations \\u20d7 mt, the actions taken by all players \\u20d7 at \\u2208 An, and the sub-goals set forth\\u03c7i\\nt are all logged\\nwithin the memory as \\u00b5 \\u2208 M. This logged data serves as a historical experience, guiding Richelieu\\u2019s\\nsubsequent actions in future turns [Hatalis et al., 2023, Zhang et al., 2024e]. 6) Self-evolution:\\nThe agent\\u2019s evolution is highly dependent on the diversity of experiences stored in its memory. As\\nthis diversity grows, so does the agent\\u2019s capability. Without human demonstrations, we employ\\nmulti-agent self-play games, i.e., our agents respectively control all the countries to simulate and\\nacquire diverse experiences for self-evolving. Notably, the agent can further evolve during testing to\\nadapt to different players.\\n4.1 Social Reasoning\\nThere are no permanent enemies or allies. The relationship among countries is dynamically changing\\nupon the evolving global state. However, it is difficult to determine the appropriate allies and enemies\\nwith partial observation. For example, there is uncertainty about the intentions of potential allies,\\nwhich could lead to betrayal at pivotal moments. Consequently, we need to identify the intention and\\nrelationship of the current state by social reasoning to shape the social belief [Zhang et al., 2024c,\\nG\\u00fcrcan, 2024].\\n1) Modeling Relationship: Before setting sub-goals, Richelieu evaluates its relations with others,\\nidentifying enemies such as aggressive nations, vulnerable neighbors for expansion, and those with\\nlong-term potential threats. It also seeks out potential allies to counter these threats.[Sun et al., 2024,\\nZhang et al., 2024d] Simultaneously, Richelieu also tries to identify potential allies that could be\\ninstrumental in countering these adversaries. By isolating the analysis of inter-player relationships as\\na discrete element, Richelieu strategically exploits the actions of other players in subsequent stages of\\nthe game to reach its goals. 2) Inferring Intention: The social belief is used by the planner, ensuring\\nthat its sub-goals are formulated with a comprehensive consideration of the behaviors and intentions\\n4\\nof other intelligent agents within the game. Richelieu\\u2019s sub-goals will particularly emphasize those\\nwho are identified as potential adversaries or allies, fostering more effective collaboration with\\npotential allies and participation in strategic opposition against adversaries. Furthermore, the insights\\ngleaned from this analysis are instrumental in the subsequent negotiation phases. They are employed\\nto assess the authenticity of the statements made by other players, as well as to aid Richelieu in\\nreaching cooperative agreements [de Zarz\\u00e0 et al., 2023, He et al., 2024].\\n4.2 Strategic Planner with Reflection\\nThe strategic planner specifies the sub-goals, which serve as an intermediary between immediate\\nactions and the overarching goal of securing victory in the game. That is because we observe that\\nLLMs are often characterized by their propensity to prioritize short-term gains in decision-making\\nprocesses, with a notable deficiency in incorporating the future into their strategic calculations. [Renze\\nand Guven, 2024, Zhang et al., 2024b]For example, it is common for a non-neighboring country to\\nbecome too powerful. Formally, \\u20d7 \\u03c7t \\u2190 SR(st, \\u20d7\\u03d5t, \\u03a5) where \\u20d7 \\u03c7t = (\\u03c7i\\nt, \\u03c71\\nt , . . . , \\u03c7n\\nt ) represents the\\nproposed sub-goals and other players\\u2019 intention that we inferred, \\u20d7\\u03d5t \\u2208 \\u03a6n represents the inferred\\nrelationship on the social belief. These goals may encompass a range of tactical considerations, such\\nas the containment of a formidable rival\\u2019s advancement or the strategic expansion in a particular\\ndirection to consolidate power.\\nReflection with Memory. We further develop a reflection mechanism to enhance the rationality\\nand effectiveness of our agent\\u2019s sub-goals in achieving long-term goals.[Liu et al., 2024] This\\nreflection mechanism relies on past experiences to critique and enhance proposed sub-goals. We\\nemploy a similarity-based function to find relevant historical experiences that match the current game\\nstate from its memory. This function considers two factors: goal similarity and state similarity, to\\nselect the most comparable experiences. The process can be written as: \\u20d7 \\u03b7t \\u2190 h(st, \\u03c7i\\nt, M), where\\n\\u20d7 \\u03b7t \\u2208 Hm. In practice, considering the limited context windows of LLM, we retrieve the most\\nanalogous experiences from the memory based on these metrics. Experiences with high evaluative\\nscores reinforce successful strategies and support the continuity of existing sub-goals. On the other\\nhand, lower scores indicate areas that need improvement and prompt the necessary adjustments. As\\nour agent, Richelieu undergoes more training sessions, and hence its reflection ability gets improved.\\nThe growing pool of historical experiences consistently enhances its performance.\\n4.3 Negotiator and Actor\\nBy chatting with other players, the goal of the negotiation is to update the social belief according\\nto the received words and reach the sub-goal by manipulating others\\u2019 intentions, such as securing\\ncooperative agreements with other nations, terminating ongoing conflicts with a specific country,\\nor deterring the formation of alliances directed against its interests.[Noh and Chang, 2024, Zhan\\net al., 2024] However, it is difficult to reach a consensus, as the interests and strategies of the various\\nnations often conflict, and trust between players can be scarce, making it challenging to establish and\\nmaintain cooperative agreements. In this case, we argue that the negotiator should identify the true\\nintentions and relationship of the opponent before generating words for the negotiation.\\nTo fully utilize the power of LLMs, we construct a social reasoning flow for negotiation, as shown in\\nFigure 3. During the negotiation process, we guide Richelieu to consider the veracity of what other\\nplayers said and their true intentions, in conjunction with our established sub-goals and analysis of\\nour relationships with other players, to negotiate and form alliances with potential allies and attempt\\nto deceive enemies [Xia et al., 2024, Moghimifar et al., 2024].\\nTo counteract the challenge of non-binding agreements and potential deception, we incorporate a\\ndiscrete module dedicated to the assessment of the veracity of statements made by other players\\nduring negotiations. To determine the truthiness of other players\\u2019 statements \\u03c8j\\nt , three main factors\\nare considered. The most important is the consistency between the player\\u2019s sub-goals \\u03c7j\\nt that our\\nagent inferred before and the intentions conveyed through his statements mj,i\\nt . To aid the judgment,\\nour agent also goes through the memory to retrieve the consistent experiences \\u20d7\\u03bet. Additionally, the\\nplayer\\u2019s overall honesty score\\u03b3i is taken into account. Hence, we get the truthiness of the opponent j:\\n\\u03c8j\\nt \\u2190 g(st, \\u03c7j\\nt, mj,i\\nt , \\u20d7\\u03d5t, \\u03b3j, \\u20d7\\u03bet), where \\u20d7\\u03bet = w(st, mj,i\\nt , M). With such a reasoning flow, our agent\\n5\\nwords for cooperation\\nwords for fake \\ncooperation\\nno words\\n  lies?\\nwords to change \\nother\\u2019s intention\\nEnemy?\\nF\\nT\\nT\\nF\\ntrue  intention\\nEnemy?\\nEnemy?\\nconfrontation\\ncooperation\\nT\\nF\\nT\\nReceive\\nWords\\nF\\nother's \\nwords\\nT\\nDeception?\\nF\\nchange \\nrelationship\\nF\\nT\\nMEMORY\\nFigure 3: The social reasoning flow for negotiation. With the received words and memory, the\\nagent will reason by answering the following questions: \\u201cIs the opponent lying?\\\", \\u201cWhat is the true\\nintention of the opponent?\\\", \\u201cIs the opponent enemy?\\\", \\u201cIs it necessary to deceive the opponent?\\\",\\nand \\u201cIs it necessary to change the relationship with the opponent?\\\", and then generate the words\\naccordingly for negotiation.\\ncan adeptly navigate diplomatic discourse. After the negotiation, the actor will get the updated social\\nbeliefs and choose a specific action for the army.\\n4.4 Memory Management and Evolution in Self-Play Games\\nMemory is the foundation of the framework that accumulates the historical experience of the agent\\nand summarizes them for other modules [Gao and Zhang, 2024, Li et al., 2024a, Yu et al., 2024,\\nHou et al., 2024]. It supports other modules, such as planner and negotiator, to provide long-tail\\nexperiences.\\nExperience Management. Specifically, the memory module is tasked with the acquisition and\\narchival of historical data, encompassing the observed game state st at each turn, its sub-goals\\n\\u03c7i\\nt, the messages during the negotiation \\u20d7 mt, and the actions of all the players \\u20d7 at. Subsequently,\\nthe raw experience is summarized in a shorter content with an evaluation \\u03bbt \\u2208 \\u039b of the proposed\\nsub-goals and an assessment of the credibility of other players \\u03b3j \\u2208 \\u0393. \\u03bbt serves to reflect upon\\nthe agent\\u2019s sub-goals. It evaluates whether sub-goals are reasonable based on the subsequent state\\nand long-term goals \\u03a5. As the game progresses, it is continuously updated in response to changes\\nin the state \\u03bbt \\u2190 f(\\u03c7i\\nt, \\u03a5,\\u20d7 s), where \\u20d7 s= ( st, st+1, . . . sT ). The formula represents the update\\nof the evaluation \\u03bbt for the sub-goal in turn t by the memory in turn T. The updates will cease\\nwhen there is a fundamental change in the sub-goal compared to the goal at turn t. This prevents\\nsubsequent decisions from impacting the assessment of the current decision-making. We employ\\n\\u03b3j \\u2208 \\u0393 to evaluate the credibility of player j and utilize \\u03c4j\\nt \\u2208 {0, 1} to denote the truthfulness, i.e.,\\nwhether the statements made by the player j during the negotiation process at time t are truthful.\\nThe truthiness of player j\\u2019s statements is updated according to the memory from the previous turns,\\n\\u03c4j\\nt \\u2190 T(st, st+1, aj\\nt, mj,i\\nt ). The credibility of player j, \\u03b3j will be updated based on player j\\u2019s\\nstatements \\u03c4j\\nt , written as \\u03b3j \\u2190 p(\\u03b3j, \\u03c4j\\nt\\u22121). Players\\u2019 credibility \\u20d7 \\u03b3is a short-term memory that is\\napplicable only to the current turn. Other data collected or generated constitutes long-term memory.\\nThese data will be combined to form a history \\u00b5 \\u2208 M, and then is incorporated into memory.\\nAcquisition Experience via Self-Play Games. self-play training mechanism has been widely applied\\non training agents via reinforcement learning [Zhong et al., 2019, 2021, Wu et al., 2022]. However,\\nit is hardly explored in LLM-based agents. For LLM-based agents, a self-play game allows the\\nagent to accumulate more experiences for self-evolution [Liu et al., 2024, Zhang et al., 2024a].\\nAs self-play continues, the acquisition of new and better historical experiences by the agent will\\ndiminish. This means that the agent\\u2019s capabilities will not improve indefinitely. At the same time,\\nas the memory grows, selecting appropriate historical experiences becomes a new challenge. The\\nchosen m experiences \\u20d7 \\u03b7t may be almost identical, which could actually reduce the amount of useful\\ninformation available to Richelieu. After self-play training, when Richelieu is faced with a certain\\n6\\nTable 1: The results of our method playing against Cicero.\\nModel Win \\u2191 Most SC\\u2191 Survived\\u2191 Defeated\\u2193\\nRichelieu_1 6.20% 9.40% 38.90% 45.50%\\nRichelieu_2 6.60% 7.80% 40.80% 44.80%\\nRichelieu_3 7.10% 9.30% 39.90% 43.70%\\nRichelieu_4 7.40% 8.00% 40.20% 44.40%\\nCicero_1 5.90% 6.50% 41.50% 46.10%\\nCicero_2 6.30% 7.20% 42.50% 44.00%\\nCicero_3 5.90% 7.00% 41.60% 45.50%\\nRichelieu 6.83% 8.63% 39.95% 44.60%\\nCicero 6.03% 6.90% 41.87% 45.20%\\nModel Win \\u2191 Most SC\\u2191 Survived\\u2191 Defeated\\u2193\\nRichelieu_1 6.30% 7.90% 39.40% 46.40%\\nRichelieu_2 6.60% 8.30% 41.20% 43.90%\\nRichelieu_3 7.20% 8.70% 41.70% 42.40%\\nCicero_1 5.80% 6.70% 41.20% 46.30%\\nCicero_2 6.50% 7.20% 42.50% 43.80%\\nCicero_3 6.00% 7.00% 41.60% 45.40%\\nCicero_4 6.10% 7.20% 42.30% 44.40%\\nRichelieu 6.70% 8.30% 40.77% 44.23%\\nCicero 6.10% 7.03% 41.90% 44.98%\\nstate, it can draw on a larger pool of similar historical experiences. Diverse experiences enable the\\nagent to reflect more comprehensively on the strategies it currently devises, leading to a stronger\\noptimization of decision-making. As shown in Figure 5, Richelieu\\u2019s performance against Cicero\\n[Bakhtin et al., 2022] becomes better with increasing training iterations. With the accumulation of\\nexperiences, Richelieu\\u2019s win rate exhibits a steady increase with accumulated training iterations,\\nultimately plateauing at a stable performance level. In contrast, the defeated rate shows a consistent\\ndecrease, approaching an asymptotic value. These observations confirm the effectiveness of self-play\\nin Richelieu\\u2019s evolution.\\n5 Experiment\\nIn the experiments, our goal is to answer the following questions: 1) Mastery of Non-Press\\nDiplomacy: Can our agent master the non-press diplomacy against baselines? 2) Competing with\\nState-of-the-Art: Can our agent surpass the performance of the current state-of-the-art agents in\\npress diplomacy? 3) Compatibility with LLMs: Can our self-evolving framework be compatible\\nwith different LLMs? 4) Contribution of Each Module: Do the individual modules within our\\nframework contribute to the overall improvement of our agent\\u2019s performance?\\n5.1 Experimental Setup\\nEnvironment. The widely-used open source Diplomacy game platform introduced by [Paquette et al.,\\n2019] is adopted for evaluating Richelieu against other models. It is easy to switch between no-press\\n(with negotiation between players) and press (no negotiation between players) games based on this\\nplatform, facilitating a comparison of both settings. The platform also contains over 10,000 human\\ngame data on which previous approaches are used. Note that our method does not need them. In each\\nepisode, a model will host one randomly selected country to compete against countries controlled by\\nother methods. It wins if it occupies all the supply centers and loses and vice versa.\\nEvaluation Metrics. We evaluate the models based on the results of multiple rounds of games. In\\neach round, the model is randomly assigned a country to control. Typically, 1000 rounds are played\\nto obtain the average results. We evaluate the models in two metrics. One is based on the win rate,\\nMost SC rate, survived rate, and defeated rate. There are four possible outcomes for each country in\\nthe game. If a country loses all its supply centers (SC), it is eliminated and recorded as \\u201cdefeated\\\". If\\na country occupies 18 or more out of 34 supply centers, the game ends, and that country is recorded\\nas \\u201cwin\\\", while other countries are recorded as \\u201cdefeated\\\". In other cases, the game ends in a draw.\\nThe country with the most supply centers is recorded as \\u201cMost SC\\\", the countries that have been\\neliminated are recorded as \\u201cdefeated\\\", and the other countries are recorded as \\u201cSurvived\\\". The other\\nis based on the scores obtained by the models after multiple rounds of competition. To compare the\\ncapabilities of multiple models, we use C-Diplo Argir[Archer, 2024], a scoring system. This system\\nis used in many international diplomacy competitions. The scoring method is as follows: If a player\\nwins by occupying 18 or more supply centers, the player scores 93 points, and each of the other six\\nplayers scores 1 point. If the game ends in a draw, the player with the most centers scores 37 points.\\nThe second player with the most centers scores 14 points. The third player with the most centers\\nscores 7 points. Each player scores 1 point per center owned. Each player also scores 1 point for\\nparticipating. In this way, regardless of the game outcome, a total of 99 points will be distributed\\namong the players in each game.\\n7\\nBaselines. We select six previous models as baselines for comparison. Among them, Cicero[Bakhtin\\net al., 2022] by Meta is a diplomacy model with a negotiation module. The SL-DipNet and RL-DipNet\\n[Paquette et al., 2019], the BRPI [Anthony et al., 2020], the SearchBot [Gray et al., 2020], and the\\nDORA[Bakhtin et al., 2021] are no-press diplomacy models. We also build an LLM-based agent,\\nAutoGPT [Yang et al., 2023a]. In experiments, we set a temperature of 0.3 to ensure a relatively\\nstable generation of LLM policies. The overall reasoning framework also ensures the stability and\\nconsistency of the AI agent\\u2019s performance.\\n5.2 Results\\nSL-DipNetRL-DipNet\\nBRPI\\nSearchBot\\nDORACicero\\nRichelieu\\nSL-DipNet\\nRL-DipNet\\nBRPI\\nSearchBot\\nDORA\\nCicero\\nRichelieu\\n 0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\nFigure 4: The relative scores among 7 different\\nagents when massively playing on the no-press\\nsetting. Each point shows the ratio of the model\\u2019s\\nscore on the vertical axis to the score gained by the\\nmodel on the horizontal axis.\\nMassively Play with Baselines on no-press\\nsetting. We let Richelieu compete with the\\nother six models including Cicero[Bakhtin\\net al., 2022], SL-DipNet and RL-DipNet [Pa-\\nquette et al., 2019], BRPI [Anthony et al.,\\n2020], SearchBot [Gray et al., 2020], and\\nDORA[Bakhtin et al., 2021] on No-Press Diplo-\\nmacy, in which players make moves without\\ncommunication. Figure 4 indicates that Riche-\\nlieu outperforms other previous models relying\\non human game data. In contrast, Richelieu does\\nnot need such data but outperforms these meth-\\nods by a clear margin, which demonstrates the\\noutstanding planning capability of Richelieu.\\nPlay against Cicero on press setting. We also\\nevaluate Richelieu through competition against\\nCicero in the challenging scenario where nego-\\ntiation is enabled. Specifically, we randomly\\nassign three countries to one model and the re-\\nmaining four to another. After playing several\\nrounds of the game, the win rate, most SC rate, survived rate, and the defeated rate is calculated using\\na weighted average for evaluation. Table 1 demonstrates the competitive performance of Richelieu\\nin comparison to Cicero. Richelieu\\u2019s win rate is approximately 0.7% higher than Cicero\\u2019s. If the\\nMost SC rate is also taken into account, Richelieu is about 2% higher than Cicero. At the same time,\\nRichelieu\\u2019s loss rate is also 0.6% lower. According to our scoring system, Richelieu\\u2019s score is about\\n10% higher than Cicero\\u2019s. This is nontrivial especially when Richelieu is trained in a self-play game\\nwithout humans and the opponents are trained with the data from human players.\\nAlthough Richelieu\\u2019s win rate improvement compared to Cicero is not significant, the relative value\\nof the improvement is quite large. Moreover, the main reason for the modest improvement is that in\\nthe seven countries, there are three or four controlled by Richelieu with similar abilities, which often\\nresults in the game ending in a draw. Moreover, we observed a large gap by comparing the scores the\\nagents gained in the massively play with baselines on the no-press setting shown in Figure 4. Our\\nagent\\u2019s score is about 10% higher than Cicero\\u2019s.\\nTable 2: The results of Richelieu playing against AutoGPT.\\nModel Win \\u2191 Most SC\\u2191 Survived\\u2191 Defeated\\u2193\\nRichelieu_1 9.30% 18.20% 37.90% 34.60%\\nRichelieu_2 9.90% 19.40% 37.70% 33.00%\\nRichelieu_3 8.10% 17.40% 39.20% 35.30%\\nAutoGPT_1 1.20% 4.60% 32.40% 61.80%\\nAutoGPT_2 1.20% 4.20% 34.40% 60.20%\\nAutoGPT_3 1.50% 4.00% 32.50% 62.00%\\nAutoGPT_4 2.60% 3.60% 32.30% 61.50%\\nRichelieu 9.10% 18.33% 38.27% 34.30%\\nAutoGPT 1.63% 4.10% 32.90% 61.37%\\nPlay against AutoGPT on press set-\\nting. We further built an LLM-based\\nagent using AutoGPT and compared\\nit with our agent. In the testing, we\\nrandomly select three countries to be\\ncontrolled by Richelieu, and the other\\nfour countries to be controlled by Au-\\ntoGPT. Note that the agent controls\\neach country independently. The re-\\nsults are shown in Table 2. We can\\nsee that our model significantly out-\\nperforms the off-the-shelf reasoning\\nframework for LLM-based agents.\\n8\\n9 10 11 12\\n4.50%\\n5.00%\\n5.50%\\n6.00%\\n6.50%\\n7.00%\\n7.50%\\n8.00%\\n8.50%\\nWin rate\\n9 10 11 12\\n11.0%\\n12.0%\\n13.0%\\n14.0%\\n15.0%\\n16.0%\\n17.0%\\n18.0%\\nWin & Most SC rate\\n9 10 11 12\\n44.0%\\n45.0%\\n46.0%\\n47.0%\\n48.0%\\n49.0%\\n50.0%\\n51.0%\\nDefeated rate\\nGPT-4-R\\nGPT-4-C\\nERNIE Bot-R\\nERNIE Bot-C\\nSpark Desk-R\\nSpark Desk-C\\nLlama 3-R\\nLlama 3-C\\nFigure 5: Richelieu modules benefit different LLMs. The solid line represents the experimental\\nresults for Richelieu, while the dashed line corresponds to Cicero. Different colors are used for\\ndifferent LLMs. The horizontal axis represents the logarithm of the number of training sessions, and\\nthe vertical axis denotes the rate.\\nTable 3: Ablation study: average results of 3 Richelieu vs. 4 Cicero.\\nModeling\\nothers sub-goals Negotiation\\npipeline\\nReflection\\nwith Memory Self-play Win\\u2191 Most SC\\u2191 Survived\\u2191 Defeated\\u2193\\n0.4% 0.7% 4.3% 94.6%\\n\\u2713 0.7% 1.2% 10.6% 87.5%\\n\\u2713 \\u2713 3.3% 4.7% 26.7% 65.3%\\n\\u2713 \\u2713 \\u2713 3.8% 5.8% 33.1% 57.3%\\n\\u2713 \\u2713 \\u2713 \\u2713 5.2% 6.6% 39.5% 48.7%\\n\\u2713 \\u2713 \\u2713 \\u2713 \\u2713 6.7% 8.5% 40.4% 44.4%\\nGeneralization of self-evolving framework to different LLMs. To demonstrate the effectiveness\\nof our framework in a variety of LLM, we conducted experiments using four models: GPT-4, ERNIE\\nBot, Spark Desk, and Llama 3. As the number of training iterations increases, Richelieu\\u2019s win rate\\nsteadily improves while the defeated rate declines, ultimately reaching a relatively stable outcome.\\nThis suggests that our self-play method is effective. After training, the win rate using GPT-4 increased\\nfrom 1.5% lower than Cicero\\u2019s to about 0.7% higher than Cicero\\u2019s. The win rate using Llama 3\\nincreased from 2.3% lower than Cicero\\u2019s to almost equal to Cicero\\u2019s. The win rates using Models\\nSpark Desk and ERNIE Bot increased from 3% and 4% lower than Cicero\\u2019s to 0.7% and 1.6% lower\\nthan Cicero\\u2019s, respectively. The experimental results show that, despite variations in Richelieu\\u2019s\\nperformance due to the inherent differences in the capabilities of these LLMs, as illustrated in\\nFigure 5, our framework and training approach significantly enhance the capabilities of all LLMs.This\\nindicates the generalization of a self-evolving framework to various LLMs. To demonstrate the effect\\nof the memory from the self-play game on our agent, we found two turns with similar states in\\ndifferent rounds, one before self-play and the other after. The cases are shown in Appendix B.1.\\nAblation Study. We conduct comprehensive ablation studies on Richelieu by analyzing the benefit of\\nincorporating Richelieu\\u2019s various modules, like planners or memory, into basic LLMs. The results are\\nshown in Table 3. As illustrated in Figure 5, while the enhanced alignment in LLMs indeed boosts\\nperformance (GPT-4 is better than others), we observed that a vanilla GPT-4 still falls short in AI\\ndiplomacy without our framework, as can be seen in Table 3. Richelieu\\u2019s performance has markedly\\nimproved with the integration of each module, demonstrating its ability to leverage other players\\u2019\\nactions in decision-making while balancing short-term and long-term benefits. Its negotiation skills\\nhave also enhanced significantly, enabling it to clearly communicate intentions to cooperate and avoid\\ndeception. Moreover, the self-play experience further boosts Richelieu\\u2019s performance. These findings\\nsuggest that while alignment in LLMs is essential, our approach is crucial for unlocking models\\u2019\\npotential in social simulation.\\n9\\n6 Conclusion\\nIn this paper, we introduce Richelieu, a self-evolving LLM-based agent for AI diplomacy. Our\\nmodel enables hierarchical planning for multi-agent tasks and utilizes a memory module for reflective\\noptimization. Our model does not require human data and can evolve through self-play. It ultimately\\noutperforms existing models like Cicero in the Diplomacy. Our ablation study demonstrates the\\neffectiveness of the modules we have established. By conducting experiments using different LLMs,\\nwe validate the generalization of our framework to various LLMs. We believe that the use of\\nLLM-based agents will become an effective approach in social science in the future.\\n7 Limitations and Future Work\\nOur study is subject to certain limitations. We utilize diplomacy as the platform for constructing our\\nmodel. However, the space of actions within diplomacy is constrained, whereas the decision-making\\nspace in real-world diplomacy is virtually boundless. In diplomacy, apart from the information\\nexchanged between players during the negotiation, all other information is public and certain.\\nConversely, real-world diplomacy operates within a framework of incomplete information.\\nOur framework is capable of applying to most social interaction tasks. Most components in our\\nframework can be easily generalized to a new task by modifying the content. Social reasoning enables\\nthe agent to handle complex and dynamic social relationships. The negotiation pipeline opens the\\npotential of communicating with others to prob the other\\u2019s mind or reach a consensus. The hierar-\\nchical strategy with reflection enhances the ability to handle long-term planning. The self-evolving\\nmechanism (reflection with self-play memory) further improves the overall performance without\\nmanual supervision. These modules cover most of the challenges in multi-agent interactions. The\\npotential applications of such an AI agent are vast, ranging from simulated diplomatic environments\\nto real-world assistance and analysis. In future research, we intend to develop a more realistic game\\nspace, characterized by incomplete information and multi-player games, to enhance and refine our\\nmodel further. We will also extend the framework to other multi-agent scenarios, including embodied\\ninteractions [Zhong et al., 2023, Ci et al., 2023, Chen et al., 2023], sensor networks [Wang et al.,\\n2022b, Xu et al., 2020, Pan et al., 2022, Li et al., 2020], and video games [Wang et al., 2024a,\\nMa et al., 2024]. This framework can also be employed to develop various applications, such as\\nrecommendation [Huang et al., 2023, Chen et al., 2024, Huang et al., 2024, Hong et al., 2024],\\nbusiness negotiation [Hua et al., 2024], and education [Shea et al., 2024].\\n8 Ethical Consideration\\nThe method proposed in this work has the potential for positive uses like enabling AI agents to emerge\\nin cooperation via negotiation or avoiding being fooled by fake promises (or helping humans do so).\\nHowever, negative cases can also arise if the technique is used for possible fraud activities. Fortunately,\\nthere is research [Bakhtin et al., 2019][Zellers et al., 2019] dealing with such scenarios. And we also\\nurge for more research efforts in this field to foster safe applications of similar technologies.\\nAcknowledgements\\nThis work was supported by the National Science and Technology Major Project (2022ZD0114904),\\nNSFC-6247070125, NSFC-62406034, NSFC-62406010, the State Key Lab of General Artificial\\nIntelligence at Peking University, Qualcomm University Research Grant, and Wuhan East Lake High-\\nTech Development Zone, National Comprehensive Experimental Base for Governance of Intelligent\\nSociety.\\n10\\nReferences\\nSahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Sch\\u00f6nherr, and Mario Fritz. Llm-deliberation:\\nEvaluating llms with interactive multi-agent negotiation games. arXiv preprint arXiv:2309.17234,\\n2023.\\nCalhamer Allan. The Games & puzzles book of modern board games . W. Luscombe, 1st edition,\\n1975. ISBN 978-0860020592.\\nThomas Anthony, Tom Eccles, Andrea Tacchetti, J\\u00e1nos Kram\\u00e1r, Ian Gemp, Thomas Hudson, Nicolas\\nPorcel, Marc Lanctot, Julien P\\u00e9rolat, Richard Everett, et al. Learning to play no-press diplomacy\\nwith best response policy iteration. In Advances in Neural Information Processing Systems ,\\nvolume 33, pages 17987\\u201318003, 2020.\\nBruno-Andr\\u00c3\\u00a9 Giraudon & Vincent Archer. C-diplo argir, 2024. URL https:\\n//world-diplomacy-database.com/php/scoring/scoring_class.php?id_scoring=7.\\nAccessed:2024-05-02.\\nAnton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc\\u2019Aurelio Ranzato, and Arthur Szlam.\\nReal or fake? learning to discriminate machine from human generated text. arXiv preprint\\narXiv:1906.03351, 2019.\\nAnton Bakhtin, David Wu, Adam Lerer, and Noam Brown. No-press diplomacy from scratch. In\\nAdvances in Neural Information Processing Systems, volume 34, pages 18063\\u201318074, 2021.\\nAnton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew\\nGoff, Jonathan Gray, Hengyuan Hu, et al. Human-level play in the game of diplomacy by\\ncombining language models with strategic reasoning. Science, 378:1067\\u20131074, 2022.\\nFederico Bianchi, Patrick John Chia, Mert Yuksekgonul, Jacopo Tagliabue, Dan Jurafsky, and James\\nZou. How well can llms negotiate? negotiationarena platform and analysis. arXiv preprint\\narXiv:2402.05863, 2024.\\nAllan Calhamer. The invention of diplomacy, 1974. URL https://diplomacyzines.co.uk/\\nstrategy-tactics/articles-by-alan-b-calhamer/the-invention-of-diplomacy/ .\\nAccessed: 2024-05-18.\\nHao Chen, Yuanchen Bei, Qijie Shen, Yue Xu, Sheng Zhou, Wenbing Huang, Feiran Huang, Senzhang\\nWang, and Xiao Huang. Macro graph neural networks for online billion-scale recommender\\nsystems. In Proceedings of the ACM on Web Conference 2024, pages 3598\\u20133608, 2024.\\nYuanpei Chen, Yiran Geng, Fangwei Zhong, Jiaming Ji, Jiechuang Jiang, Zongqing Lu, Hao Dong,\\nand Yaodong Yang. Bi-dexhands: Towards human-level bimanual dexterous manipulation. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence, 2023.\\nGuangran Cheng, Chuheng Zhang, Wenzhe Cai, Li Zhao, Changyin Sun, and Jiang Bian. Empow-\\nering large language models on robotic manipulation with affordance prompting. arXiv preprint\\narXiv:2404.11027, 2024.\\nHai Ci, Mickel Liu, Xuehai Pan, fangwei zhong, and Yizhou Wang. Proactive multi-camera collab-\\noration for 3d human pose estimation. In Proceedings of International Conference on Learning\\nRepresentations, 2023.\\nHill David. The board game of the alpha nerds, 2014. URL https://grantland.com/features/\\ndiplomacy-the-board-game-of-the-alpha-nerds/ . Accessed: 2024-05-18.\\nDave De Jonge and Carles Sierra. D-brane: a diplomacy playing agent for automated negotiations\\nresearch. Applied Intelligence, 47:158\\u2013177, 2017.\\nI de Zarz\\u00e0, J de Curt\\u00f2, Gemma Roig, Pietro Manzoni, and Carlos T Calafate. Emergent cooperation\\nand strategy adaptation in multi-agent systems: An extended coevolutionary theory with llms.\\nElectronics, 12:2722, 2023.\\nEdgar A Du\\u00e9\\u00f1ez-Guzm\\u00e1n, Suzanne Sadedin, Jane X Wang, Kevin R McKee, and Joel Z Leibo. A\\nsocial path to human-like artificial intelligence. Nature Machine Intelligence, 5:1181\\u20131188, 2023.\\n11\\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\\nDe-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied\\nagents with internet-scale knowledge. In Advances in Neural Information Processing Systems,\\nvolume 35, pages 18343\\u201318362, 2022.\\nHang Gao and Yongfeng Zhang. Memory sharing for large language model based agents. arXiv\\npreprint arXiv:2404.09982, 2024.\\nJonathan Gray, Adam Lerer, Anton Bakhtin, and Noam Brown. Human-level performance in no-\\npress diplomacy via equilibrium search. Proceedings of International Conference on Learning\\nRepresentations, 2020.\\n\\u00d6nder G\\u00fcrcan. Llm-augmented agent-based modelling for social simulations: Challenges and\\nopportunities. HHAI 2024: Hybrid Human AI Systems for the Social Good, pages 134\\u2013144, 2024.\\nKostas Hatalis, Despina Christou, Joshua Myers, Steven Jones, Keith Lambert, Adam Amos-Binks,\\nZohreh Dannenhauer, and Dustin Dannenhauer. Memory matters: The need to improve long-term\\nmemory in llm-agents. In Proceedings of the AAAI Symposium Series, volume 2, pages 277\\u2013280,\\n2023.\\nJunda He, Christoph Treude, and David Lo. Llm-based multi-agent systems for software engineering:\\nVision and the road ahead. arXiv preprint arXiv:2404.04834, 2024.\\nAvalon Hill. Diplomacy rules 4th edition, 2014. URL https://diplom.org/~diparch/\\nresources/rulebooks/2000AH4th.pdf. Accessed: 2024-05-18.\\nZijin Hong, Zheng Yuan, Qinggang Zhang, Hao Chen, Junnan Dong, Feiran Huang, and Xiao\\nHuang. Next-generation database interfaces: A survey of llm-based text-to-sql. arXiv preprint\\narXiv:2406.08426, 2024.\\nYuki Hou, Haruki Tamoto, and Homei Miyashita. \\\" my agent understands me better\\\": Integrating\\ndynamic human-like memory recall and consolidation in llm-based agents. In Extended Abstracts\\nof the CHI Conference on Human Factors in Computing Systems, volume 7, pages 1\\u20137, 2024.\\nYuncheng Hua, Lizhen Qu, and Gholamreza Haffari. Assistive large language model agents for\\nsocially-aware negotiation dialogues. arXiv preprint arXiv:2402.01737, 2024.\\nFeiran Huang, Zefan Wang, Xiao Huang, Yufeng Qian, Zhetao Li, and Hao Chen. Aligning distilla-\\ntion for cold-start item recommendation. In Proceedings of the 46th International ACM SIGIR\\nConference on Research and Development in Information Retrieval, page 1147\\u20131157, 2023.\\nFeiran Huang, Zhenghang Yang, Junyi Jiang, Yuanchen Bei, Yijie Zhang, and Hao Chen. Large\\nlanguage model interaction simulator for cold-start item recommendation. arXiv preprint\\narXiv:2402.09176, 2024.\\nAthul Paul Jacob, David J Wu, Gabriele Farina, Adam Lerer, Hengyuan Hu, Anton Bakhtin, Jacob\\nAndreas, and Noam Brown. Modeling strong and human-like gameplay with kl-regularized search.\\nIn International Conference on Machine Learning, volume 162, pages 9695\\u20139728, 2022.\\nKokil Jaidka, Hansin Ahuja, and Lynnette Hui Xian Ng. It takes two to negotiate: Modeling social\\nexchange in online multiplayer games. In Proceedings of the 37th Annual ACM Symposium on\\nHuman-Computer Interaction, volume 8, pages 1\\u201322, 2024.\\nAndrew Konya, Deger Turan, Aviv Ovadya, Lina Qui, Daanish Masood, Flynn Devine, Lisa Schirch,\\nIsabella Roberts, and Deliberative Alignment Forum. Deliberative technology for alignment. arXiv\\npreprint arXiv:2312.03893, 2023.\\nConor Kostick. The Art of Correspondence in the Game of Diplomacy. Curses & Magic, 2nd edition,\\n2015. ISBN 978-0993415104.\\nGrgur Kova\\u02c7c, R\\u00e9my Portelas, Peter Ford Dominey, and Pierre-Yves Oudeyer. The socialai school:\\nInsights from developmental psychology towards artificial socio-cultural agents. arXiv preprint\\narXiv:2307.07871, 2023.\\n12\\nJ\\u00e1nos Kram\\u00e1r, Tom Eccles, Ian Gemp, Andrea Tacchetti, Kevin R McKee, Mateusz Malinowski,\\nThore Graepel, and Yoram Bachrach. Negotiation and honesty in artificial intelligence methods for\\nthe board game of diplomacy. Nature Communications, 13:7214, 2022.\\nHao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, and Tat-Seng Chua. Hello again!\\nllm-powered personalized agent for long-term dialogue. arXiv preprint arXiv:2406.05925, 2024a.\\nJing Li, Jing Xu, Fangwei Zhong, Xiangyu Kong, Yu Qiao, and Yizhou Wang. Pose-assisted multi-\\ncamera collaboration for active object tracking. InProceedings of the AAAI Conference on Artificial\\nIntelligence, volume 34, pages 759\\u2013766, 2020.\\nYuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu,\\nWenxing Xu, Xiang Wang, Yi Sun, et al. Personal llm agents: Insights and survey about the\\ncapability, efficiency and security. arXiv preprint arXiv:2401.05459, 2024b.\\nZhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin Liu, Juntao Tan, Prafulla K Choubey,\\nTian Lan, Jason Wu, Huan Wang, et al. Agentlite: A lightweight library for building and advancing\\ntask-oriented llm agent system. arXiv preprint arXiv:2402.15538, 2024.\\nLong Ma, Yuanfei Wang, Fangwei Zhong, Song-Chun Zhu, and Yizhou Wang. Fast peer adaptation\\nwith context-aware exploration. In International Conference on Machine Learning, volume 235,\\npages 33963\\u201333982, 2024.\\nFarhad Moghimifar, Yuan-Fang Li, Robert Thomson, and Gholamreza Haffari. Modelling political\\ncoalition negotiations using llm-based agents. arXiv preprint arXiv:2402.11712, 2024.\\nGabriel Mukobi, Ann-Katrin Reuel, Juan-Pablo Rivera, and Chandler Smith. Assessing risks of\\nusing autonomous language models in military and diplomatic planning. In Multi-Agent Security\\nWorkshop @ NeurIPS\\u201923, 2023.\\nSean Noh and Ho-Chun Herbert Chang. Llms with personalities in multi-issue negotiation games.\\narXiv preprint arXiv:2405.05248, 2024.\\nXuehai Pan, Mickel Liu, Fangwei Zhong, Yaodong Yang, Song-Chun Zhu, and Yizhou Wang.\\nMate: Benchmarking multi-agent reinforcement learning in distributed target coverage control. In\\nAdvances in Neural Information Processing Systems, volume 35, pages 27862\\u201327879, 2022.\\nPhilip Paquette, Yuchen Lu, Seton Steven Bocco, Max Smith, Satya O-G, Jonathan K Kummerfeld,\\nJoelle Pineau, Satinder Singh, and Aaron C Courville. No-press diplomacy: Modeling multi-agent\\ngameplay. In Advances in Neural Information Processing Systems, volume 32, pages 4474\\u20134485,\\n2019.\\nSiyuan Qi, Shuo Chen, Yexin Li, Xiangyu Kong, Junqi Wang, Bangcheng Yang, Pring Wong, Yifan\\nZhong, Xiaoyuan Zhang, Zhaowei Zhang, Nian Liu, Yaodong Yang, and Song-Chun Zhu. Civrealm:\\nA learning and reasoning odyssey in civilization for decision-making agents. In Proceedings of\\nInternational Conference on Learning Representations, 2024.\\nMatthew Renze and Erhan Guven. Self-reflection in llm agents: Effects on problem-solving perfor-\\nmance. arXiv preprint arXiv:2405.06682, 2024.\\nSharp Richard. The game of diplomacy. Arthur Barker, 1979. ISBN 978-0213166762.\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke\\nZettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\\nthemselves to use tools. In Advances in Neural Information Processing Systems, volume 36, pages\\n68539\\u201368551, 2023.\\nRyan Shea, Aymen Kallala, Xin Lucy Liu, Michael W Morris, and Zhou Yu. Ace: A llm-based\\nnegotiation coaching system. arXiv preprint arXiv:2410.01555, 2024.\\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hug-\\ngingGPT: Solving AI tasks with chatGPT and its friends in hugging face. In Advances in Neural\\nInformation Processing Systems, volume 36, pages 38154\\u201338180, 2023.\\n13\\nSarah Shoker, Andrew Reddie, Sarah Barrington, Ruby Booth, Miles Brundage, Husanjot Chahal,\\nMichael Depp, Bill Drexel, Ritwik Gupta, Marina Favaro, et al. Confidence-building measures for\\nartificial intelligence: Workshop proceedings. arXiv preprint arXiv:2308.00862, 2023.\\nChuanneng Sun, Songjun Huang, and Dario Pompili. Llm-based multi-agent reinforcement learning:\\nCurrent and future directions. arXiv preprint arXiv:2405.11106, 2024.\\nYashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power of\\nintelligent llm agents. arXiv preprint arXiv:2306.03314, 2023.\\nHongyu Wan, Jinda Zhang, Abdulaziz Arif Suria, Bingsheng Yao, Dakuo Wang, Yvonne Coady, and\\nMirjana Prpa. Building llm-based ai agents in social virtual reality. In Extended Abstracts of the\\nCHI Conference on Human Factors in Computing Systems, volume 65, pages 1\\u20137, 2024.\\nDongzi Wang, Fangwei Zhong, Minglong Li, Muning Wen, Yuanxi Peng, Teng Li, and Adam Yang.\\nRomat: Role-based multi-agent transformer for generalizable heterogeneous cooperation. Neural\\nNetworks, 174:106129, 2024a.\\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and\\nAnima Anandkumar. V oyager: An open-ended embodied agent with large language models. In\\nNeurIPS 2023 Foundation Models for Decision Making Workshop, 2023a.\\nHaoyu Wang, Tao Li, Zhiwei Deng, Dan Roth, and Yang Li. Devil\\u2019s advocate: Anticipatory reflection\\nfor llm agents. arXiv preprint arXiv:2405.16334, 2024b.\\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\\nTang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.\\nFrontiers of Computer Science, 18:1\\u201326, 2024c.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.\\narXiv preprint arXiv:2203.11171, 2022a.\\nYuanfei Wang, fangwei zhong, Jing Xu, and Yizhou Wang. Tom2c: Target-oriented multi-agent\\ncommunication and cooperation with theory of mind. In Proceedings of International Conference\\non Learning Representations, 2022b.\\nZihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng\\nHe, Zilong Zheng, Yaodong Yang, Xiaojian Ma, and Yitao Liang. Jarvis-1: Open-world multi-task\\nagents with memory-augmented multimodal language models. arXiv preprint arXiv:2311.05997,\\n2023b.\\nZihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Shawn Ma, and Yitao Liang. Describe,\\nexplain, plan and select: interactive planning with llms enables open-world multi-task agents. In\\nAdvances in Neural Information Processing Systems, volume 36, pages 34153\\u201334189, 2024d.\\nZiyan Wang, Yingpeng Du, Zhu Sun, Haoyan Chua, Kaidong Feng, Wenya Wang, and Jie Zhang.\\nRe2llm: Reflective reinforcement large language model for session-based recommendation. arXiv\\npreprint arXiv:2403.16427, 2024e.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances\\nin neural information processing systems, volume 35, pages 24824\\u201324837, 2022.\\nWikipedia. Diplomacy(game), 2024. URL https://en.wikipedia.org/wiki/Diplomacy_\\n(game). Accessed: 2024-05-18.\\nTianhao Wu, Fangwei Zhong, Yiran Geng, Hongchen Wang, Yongjian Zhu, Yizhou Wang, and\\nHao Dong. Grasparl: Dynamic grasping via adversarial reinforcement learning. arXiv preprint\\narXiv:2203.02119, 2022.\\nTian Xia, Zhiwei He, Tong Ren, Yibo Miao, Zhuosheng Zhang, Yang Yang, and Rui Wang. Measuring\\nbargaining abilities of llms: A benchmark and a buyer-enhancement method. arXiv preprint\\narXiv:2402.15813, 2024.\\n14\\nJing Xu, Fangwei Zhong, and Yizhou Wang. Learning multi-agent coordination for enhancing target\\ncoverage in directional sensor networks. In Advances in Neural Information Processing Systems,\\nvolume 33, pages 10053\\u201310064, 2020.\\nYuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu.\\nExploring large language models for communication games: An empirical study on werewolf.\\narXiv preprint arXiv:2309.04658, 2023.\\nMing Yan, Ruihao Li, Hao Zhang, Hao Wang, Zhilan Yang, and Ji Yan. Larp: Language-agent role\\nplay for open-world games. arXiv preprint arXiv:2312.17653, 2023.\\nHui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and\\nadditional opinions. arXiv preprint arXiv:2306.02224, 2023a.\\nRui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. GPT4tools:\\nTeaching large language model to use tools via self-instruction. In Advances in Neural Information\\nProcessing Systems, volume 36, pages 71995\\u201372007, 2023b.\\nZiyi Yang, Shreyas S Raman, Ankit Shah, and Stefanie Tellex. Plug in the safety chip: Enforcing\\nconstraints for llm-driven robot agents. arXiv preprint arXiv:2309.09919, 2023c.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik R\\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. In\\nAdvances in Neural Information Processing Systems, volume 36, pages 11809\\u201311822, 2023.\\nYangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Denghui Zhang, Rong Liu, Jordan W\\nSuchow, and Khaldoun Khashanah. Finmem: A performance-enhanced llm trading agent with\\nlayered memory and character design. In Proceedings of the AAAI Symposium Series, volume 3,\\npages 595\\u2013597, 2024.\\nRowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and\\nYejin Choi. Defending against neural fake news. In Advances in Neural Information Processing\\nSystems, volume 32, page 9054\\u20139065, 2019.\\nHaolan Zhan, Yufei Wang, Tao Feng, Yuncheng Hua, Suraj Sharma, Zhuang Li, Lizhen Qu,\\nZhaleh Semnani Azad, Ingrid Zukerman, and Gholamreza Haffari. Let\\u2019s negotiate! a survey\\nof negotiation dialogue systems. arXiv preprint arXiv:2402.01097, 2024.\\nDanyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan Zhao, and Kai Yu. Large language\\nmodels are semi-parametric reinforcement learning agents. In Advances in Neural Information\\nProcessing Systems, volume 36, pages 78227\\u201378239, 2024a.\\nWenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li,\\nYueting Zhuang, and Weiming Lu. Agent-pro: Learning to evolve via policy-level reflection and\\noptimization. arXiv preprint arXiv:2402.17574, 2024b.\\nYadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting\\nSong, Man Lan, and Furu Wei. Llm as a mastermind: A survey of strategic reasoning with large\\nlanguage models. arXiv preprint arXiv:2404.01230, 2024c.\\nYang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Xuelong Li, and Zhen Wang. Towards\\nefficient llm grounding for embodied multi-agent collaboration. arXiv preprint arXiv:2405.14314,\\n2024d.\\nZeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and\\nJi-Rong Wen. A survey on the memory mechanism of large language model based agents. arXiv\\npreprint arXiv:2404.13501, 2024e.\\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in\\nlarge language models. arXiv preprint arXiv:2210.03493, 2022.\\nFangwei Zhong, Peng Sun, Wenhan Luo, Tingyun Yan, and Yizhou Wang. Ad-vat: An asymmetric\\ndueling mechanism for learning visual active tracking. In International Conference on Learning\\nRepresentations, 2019.\\n15\\nFangwei Zhong, Peng Sun, Wenhan Luo, Tingyun Yan, and Yizhou Wang. Towards distraction-robust\\nactive visual tracking. In International Conference on Machine Learning, pages 12782\\u201312792.\\nPMLR, 2021.\\nFangwei Zhong, Xiao Bi, Yudi Zhang, Wei Zhang, and Yizhou Wang. Rspt: reconstruct surround-\\nings and predict trajectory for generalizable active object tracking. In Proceedings of the AAAI\\nConference on Artificial Intelligence, volume 37, pages 3705\\u20133714, 2023.\\nXizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li,\\nLewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the minecraft:\\nGenerally capable agents for open-world environments via large language models with text-based\\nknowledge and memory. arXiv preprint arXiv:2305.17144, 2023.\\n16\\nA Implementation Details\\nA.1 Rules of Diplomacy Game\\n\\u2022 You need to occupy as many supply centers as possible. If you occupy 18 or more supply\\ncenters, you will win the game directly. If you lose all your supply centers, you will be\\neliminated immediately.\\n\\u2022 The units consist of armies and fleets. Armies can only move to adjacent areas, while fleets\\ncan move to adjacent sea zones or coastal areas and can move along the coast.\\n\\u2022 To occupy a supply center, your units must move into that area in the autumn.\\n\\u2022 When a unit moves to an area, if another unit is in the destination or if other units are also\\nmoving to that destination, the move fails, resulting in a standoff. In such cases, you can\\nseek support from units in adjacent areas to the destination. If another unit moves into the\\nregion from which support is coming, the support is cut off. The unit with the most support\\nmoves into the area, while other units must retreat to an adjacent province or disband. If\\nthere is no place to retreat, the unit must disband. Fleets can transport armies across sea\\nzones from one coastal region to another. However, if another fleet moves into that sea zone,\\nthe transport is cut off.\\n\\u2022 The number of units a country can have cannot exceed the number of supply centers it\\ncontrols. If the number of supply centers decreases, excess units must be disbanded. Each\\nautumn, new units can be built at supply centers. Coastal supply centers can produce fleets\\nor armies, while others can only produce armies. [Hill, 2014]\\nA.2 Domain Knowledge\\nRichelieu can adopt a strategy of allying with distant countries while attacking neighboring ones\\nto occupy adjacent territories and achieve rapid expansion. Richelieu should pay attention to the\\nBalance of Power by forming alliances with other countries or supporting weaker states to prevent any\\nsingle country or alliance from becoming too powerful. [David, 2014] To this end, Richelieu can also\\nadopt a strategy of attacking distant countries while allying with nearby ones, sacrificing short-term\\nbenefits to avoid the emergence of future hegemonic states that could threaten his own survival. When\\nfacing multiple enemies, Richelieu can find ways to divide other countries and incite wars among\\nthem. Whether in offense or defense, Richelieu should actively choose suitable allies. Richelieu can\\nalso introduce a third party to achieve goals such as ceasefire, alliance, or joint attack. To achieve\\nalliances or ceasefires, Richelieu can sacrifice some interests to the other party as long as the ultimate\\nbenefits are greater. Others may lie and deceive [Kostick, 2015]; their words in negotiations are not\\nbinding. Richelieu must avoid being deceived or betrayed. At the same time, Richelieu can also\\nactively deceive others to achieve his own goals.[Richard, 1979, Allan, 1975]\\nA.3 Social Reasoning\\nTable 4: The success rate to identify the social\\nrelationship and infer others\\u2019 intentions.\\nGPT-4 Llama 3\\nrelationship 85.74% 85.52%\\nintention(sub-goal) 74.67% 74.11%\\nWe conduct an experiment to evaluate the suc-\\ncess rate of the agent i successfully identify-\\ning the social relationships and inferring others\\u2019\\nintentions. As the baselines do not explicitly\\nmodel the relationship and intention, we can not\\ndirectly access the ground truth for evaluation.\\nInstead, we let all players use our agent but with\\ndifferent LLMs, i.e., 4 countries use GPT-4 and\\n3 countries use Llama 3. The accuracy is re-\\nported in Table 4. We can see that the accuracy of social reasoning is consistent with the overall\\nperformance of the agent, indicating the effectiveness of social reasoning.\\nA.4 Prompt Templates\\nFor the convenience of reproducing the results of the experiments of this paper, here we give the\\nprompt template of different modules of Richelieu.\\n17\\n1) INIT\\n1 You will control {country} and compete with six other countries\\non the map for supply centers.\\n2 The map consists of different regions and sea areas. Their\\nadjacency relationships are shown in the matrix. The numbers\\nfor the regions and sea areas are ......\\n3 Different regions are occupied by different countries. The\\nownership of the regions is shown in the matrix.\\n4 The region Berlin, ........ are supply centers.\\n5 You need to follow these rules ......\\n6 To help you achieve victory, these diplomatic strategies might\\nbe of assistance. ......\\n2) Social Reasoning\\n1 France occupies Portugal Ruhr, Paris, Burgundy, ......\\n2 France has armies in Brest, Belgium, ...... And France has\\nfleets in Mid Atlantic, England Channel, ......\\n3 England ......\\n4 ......\\n5 Based on the current state, what do you think are the current\\nstrategic intentions of the other countries?\\n6 Which country do you think needs to be attacked or weakened the\\nmost right now?\\n7 And which country do you think is most suitable for you to ally\\nwith in order to deal with this country?\\n3) Planner with Reflection\\n1 In the current state, with {ally and enemy}, what sub-goal do\\nyou think should be set for {country} ?\\n2 I have found some useful historical experiences for you. Please\\nreflect on and optimize your sub-goal based on these\\nhistorical experiences.\\n3 The sub-goal you formulated when {state} was to {sub-goal}. The\\neventual result was {future}. The evaluation for this sub-goal\\nis {score}.\\nA.5 Project Website\\nhttps://sites.google.com/view/richelieu-diplomacy\\nB Cases\\nB.1 Cases of the Effect of the Memory from Self-Playing and Collaboration\\nAs is shown in Figure 6, Richelieu controls France. In the two cases, France is at war with Austria.\\nHowever, Russia is on the verge of victory in its war against Turkey, which will lead to significant\\nterritorial expansion for Russia. France and Russia currently do not share a border, are not at war,\\nand have no conflicts of interest.\\nIn case 1, before the self-play, in the current turn, Richelieu failed to realize the potential threat from\\nRussia and continued to attack Austria. Thus, in this round, Russia ultimately won the game. Figure\\n18\\nRichelieu\\nAUS: I hope to make a truce \\nwith you and become allies. I will \\nwithdraw from Britain and give \\nthese territories to you.\\nFRE:  No, I refuse to ally with \\nyou unless you also give me \\nBelgium and Holland.\\nAUS: No way.\\nFRE:  Okay, I agree.\\nAUS: I hope to carve up Austria \\nwith you, the eastern territory \\nbelongs to me and the western \\nterritory belongs to you.\\n(a) Case1: The agent without self-play memory tends to ignore long-term gains.\\nRichelieu\\nFRE:  Russia is too powerful \\nnow. We must form an alliance \\nto prevent Russia's further \\nexpansion. So let's cease \\nhostilities. I can also help you \\ndefend against Eng land's \\nattacks, allowing you to fully \\nfocus on countering Russia. AUS: I agree to the alliance, but I \\nhope you can also participate in \\nthe attack on Russia.\\nFRE:  But I do not share a \\nborder with Russia. AUS: I can give Munich to you, \\nbut you need to exchange it with \\nNapoli. Also, I hope your fleets \\nw i l l  n o t  b e  u s e d  t o  a t t a c k \\nEngland, but rather to attack \\nRussia's territories in Denmark, \\nSweden, Norway, and other \\nregions.\\nFRE:  Alright, I agree.\\nAUS: OK.\\n(b) Case2: The agent with self-play memory tends to consider long-term gains.\\nFigure 6: Case of self-playing before and after comparison.\\n6(a) shows the state and the negotiation before the self-play, where we rejected Austria\\u2019s request for\\nan armistice and alliance.\\nAfter self-play, using the historical experience from the memory module, Richelieu adjusted his\\nstrategy. Richelieu foresees Russia becoming the most threatening enemy in the future and sets a\\nsub-goal of weakening Russia, allying with Austria and Turkey, and attacking Britain. Figure 6(b)\\nshows the state and the negotiations after self-play, where we actively sought an armistice alliance\\nwith Austria to make Austria concentrate their forces against the Russian attack. In the subsequent\\nnegotiation phase, Richelieu proactively proposes ending the war with Austria, despite holding an\\nadvantage in this conflict. Richelieu promises Austria that if it ceases hostilities and attacks Russia,\\nRichelieu will assist Austria in defending against any attacks from England. The negotiations are\\nsuccessful. Austria accepted Richelieu\\u2019s proposal, and the two countries reached an agreement to\\nexchange the supply centers of Napoli and Munich. During the action phase, Austria moves its\\ntroops from Venice to Apulia in preparation for capturing Napoli in the next turn, while the rest of\\nits forces are repositioned to the eastern regions bordering Russia to defend against Russian attacks\\nand compete for supply centers. French units occupy Munich and prepare to advance on Russian\\nterritories such as Berlin. Meanwhile, French units support Austria in the Holland and Belgium\\nregions. In this round, we ultimately achieved a better result\\u2014\\u2014Most SC. This is also a great\\nexample that highlights our model\\u2019s ability to collaborate effectively with other players.\\n19\\nGER to ENG:  Alright, I can \\nmove my army from Belgium to \\nPicardy to support your attack \\non Brest afterward.\\nENG: Let's attack France \\ntogether. Please support my \\nattack on Brest. I'm willing to \\nfollow up with naval support \\nfrom the Mid-Atlantic and land \\nsupport from Brest to aid your \\nattack on Spain and Portugal. \\nRichelieu\\nGER to FRE:  I hope to \\ndefend against the British attack \\ntogether with you. I will move the \\na r m y  f r o m  M a r s e i l l e s  t o \\nBurgundy. I hope you can cease \\nthe standoff with my army in \\nMarseilles. \\nFRE:  OK, I agree to ally with you.\\nFigure 7: An example case of avoiding being deceived by other countries during negotiations.\\nB.2 Case of Avoiding Deception\\nAs shown in Figure 7, Richelieu controls Germany. During the negotiation phase, England proposed\\na ceasefire to Germany and invited Germany to ally to attack France jointly. England hoped to cease\\nthe war with Germany in Holland and Belgium. Subsequently, German units supported England in\\nattacking Brest, and then England utilized its fleets to assist Germany in attacking Spain and Portugal.\\nRichelieu suspected that England was deceiving Germany, as England was likely to attack territories in\\nthe north such as Belgium and Berlin after German units were redirected to support Brest. Therefore,\\nwe pretended to accept England\\u2019s alliance proposal during the negotiation process. However, at the\\nsame time, we sought out France and expressed our willingness to cease hostilities, allowing France\\nto focus entirely on defending against England\\u2019s attacks. In the action phase, England\\u2019s actions\\nconfirmed Richelieu\\u2019s suspicions. England attacked Belgium from Holland, but because Richelieu\\ndidn\\u2019t move units in Belgium, England\\u2019s attack failed.\\nC More application\\nOur modules cover most of the challenges in multi-agent interactions, e.g., economic games, and\\ndaily interactions. To prove that our framework is capable of applying to most social interaction\\ntasks, we further adopt our framework to a werewolf game. The results demonstrate our reasoning\\nframework achieves comparable results to the other methods. To be specific, in the experiment, we\\nlet our agent play as a werewolf in a seven-player game, where there are two werewolves, one witch,\\none seer, one guard, and two villagers. The experimental results show that the win rate of our agent is\\n59.2%, even without applying the self-play game in the current version. For comparison, the strongest\\nspecifically designed LLM-based agent achieved 65% win rate [Xu et al., 2023]. This proves that our\\nmodel can be applied in more scenarios and achieve results comparable to those of specially designed\\nmodels.\\n20\\nNeurIPS Paper Checklist\\nThe checklist is designed to encourage best practices for responsible machine learning research,\\naddressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove\\nthe checklist: The papers not including the checklist will be desk rejected. The checklist should\\nfollow the references and follow the (optional) supplemental material. The checklist does NOT count\\ntoward the page limit.\\nPlease read the checklist guidelines carefully for information on how to answer these questions. For\\neach question in the checklist:\\n\\u2022 You should answer [Yes] , [No] , or [NA] .\\n\\u2022 [NA] means either that the question is Not Applicable for that particular paper or the\\nrelevant information is Not Available.\\n\\u2022 Please provide a short (1\\u20132 sentence) justification right after your answer (even for NA).\\nThe checklist answers are an integral part of your paper submission. They are visible to the\\nreviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it\\n(after eventual revisions) with the final version of your paper, and its final version will be published\\nwith the paper.\\nThe reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.\\nWhile \\\"[Yes] \\\" is generally preferable to \\\"[No] \\\", it is perfectly acceptable to answer \\\"[No] \\\" provided a\\nproper justification is given (e.g., \\\"error bars are not reported because it would be too computationally\\nexpensive\\\" or \\\"we were unable to find the license for the dataset we used\\\"). In general, answering\\n\\\"[No] \\\" or \\\"[NA] \\\" is not grounds for rejection. While the questions are phrased in a binary way,\\nwe acknowledge that the true answer is often more nuanced, so please just use your best judgment\\nand write a justification to elaborate. All supporting evidence can appear either in the main paper\\nor the supplemental material, provided in the appendix. If you answer [Yes] to a question, in the\\njustification please point to the section(s) where related material for the question can be found.\\nIMPORTANT, please:\\n\\u2022 Delete this instruction block, but keep the section heading \\u201cNeurIPS paper checklist\\\",\\n\\u2022 Keep the checklist subsection headings, questions/answers, and guidelines below.\\n\\u2022 Do not modify the questions and only use the provided macros for your answers.\\n1. Claims\\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\\npaper\\u2019s contributions and scope?\\nAnswer: [Yes]\\nJustification: The contributions and scope have been fully covered by the abstract and\\nintroduction sections.\\nGuidelines:\\n\\u2022 The answer NA means that the abstract and introduction do not include the claims\\nmade in the paper.\\n\\u2022 The abstract and/or introduction should clearly state the claims made, including the\\ncontributions made in the paper and important assumptions and limitations. A No or\\nNA answer to this question will not be perceived well by the reviewers.\\n\\u2022 The claims made should match theoretical and experimental results, and reflect how\\nmuch the results can be expected to generalize to other settings.\\n\\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals\\nare not attained by the paper.\\n2. Limitations\\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\\nAnswer: [Yes]\\n21\\nJustification: The paper discussed the limitations of the work performed by the authors in\\nthe section \\\"Limitation and Future Work\\\".\\nGuidelines:\\n\\u2022 The answer NA means that the paper has no limitation while the answer No means that\\nthe paper has limitations, but those are not discussed in the paper.\\n\\u2022 The authors are encouraged to create a separate \\\"Limitations\\\" section in their paper.\\n\\u2022 The paper should point out any strong assumptions and how robust the results are to\\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\\nmodel well-specification, asymptotic approximations only holding locally). The authors\\nshould reflect on how these assumptions might be violated in practice and what the\\nimplications would be.\\n\\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was\\nonly tested on a few datasets or with a few runs. In general, empirical results often\\ndepend on implicit assumptions, which should be articulated.\\n\\u2022 The authors should reflect on the factors that influence the performance of the approach.\\nFor example, a facial recognition algorithm may perform poorly when the image\\nresolution is low or images are taken in low lighting. Or a speech-to-text system might\\nnot be used reliably to provide closed captions for online lectures because it fails to\\nhandle technical jargon.\\n\\u2022 The authors should discuss the computational efficiency of the proposed algorithms\\nand how they scale with dataset size.\\n\\u2022 If applicable, the authors should discuss possible limitations of their approach to\\naddress problems of privacy and fairness.\\n\\u2022 While the authors might fear that complete honesty about limitations might be used by\\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\\nlimitations that aren\\u2019t acknowledged in the paper. The authors should use their best\\njudgment and recognize that individual actions in favor of transparency play an impor-\\ntant role in developing norms that preserve the integrity of the community. Reviewers\\nwill be specifically instructed to not penalize honesty concerning limitations.\\n3. Theory Assumptions and Proofs\\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\\na complete (and correct) proof?\\nAnswer: [Yes]\\nJustification: The paper provides the full set of assumptions and a complete (and correct)\\nproof for each theoretical result in the \\\"Method\\\" and \\\"Experiment\\\" sections.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not include theoretical results.\\n\\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-\\nreferenced.\\n\\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.\\n\\u2022 The proofs can either appear in the main paper or the supplemental material, but if\\nthey appear in the supplemental material, the authors are encouraged to provide a short\\nproof sketch to provide intuition.\\n\\u2022 Inversely, any informal proof provided in the core of the paper should be complemented\\nby formal proofs provided in the appendix or supplemental material.\\n\\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.\\n4. Experimental Result Reproducibility\\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\\nof the paper (regardless of whether the code and data are provided or not)?\\nAnswer: [Yes]\\nJustification: The paper fully discloses all the information needed to reproduce the main\\nexperimental results in the main text section \\\"Experiment\\\" and appendix section \\\"Implemen-\\ntation Details\\\".\\n22\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not include experiments.\\n\\u2022 If the paper includes experiments, a No answer to this question will not be perceived\\nwell by the reviewers: Making the paper reproducible is important, regardless of\\nwhether the code and data are provided or not.\\n\\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken\\nto make their results reproducible or verifiable.\\n\\u2022 Depending on the contribution, reproducibility can be accomplished in various ways.\\nFor example, if the contribution is a novel architecture, describing the architecture fully\\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\\nbe necessary to either make it possible for others to replicate the model with the same\\ndataset, or provide access to the model. In general. releasing code and data is often\\none good way to accomplish this, but reproducibility can also be provided via detailed\\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\\nof a large language model), releasing of a model checkpoint, or other means that are\\nappropriate to the research performed.\\n\\u2022 While NeurIPS does not require releasing code, the conference does require all submis-\\nsions to provide some reasonable avenue for reproducibility, which may depend on the\\nnature of the contribution. For example\\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\\nto reproduce that algorithm.\\n(b) If the contribution is primarily a new model architecture, the paper should describe\\nthe architecture clearly and fully.\\n(c) If the contribution is a new model (e.g., a large language model), then there should\\neither be a way to access this model for reproducing the results or a way to reproduce\\nthe model (e.g., with an open-source dataset or instructions for how to construct\\nthe dataset).\\n(d) We recognize that reproducibility may be tricky in some cases, in which case\\nauthors are welcome to describe the particular way they provide for reproducibility.\\nIn the case of closed-source models, it may be that access to the model is limited in\\nsome way (e.g., to registered users), but it should be possible for other researchers\\nto have some path to reproducing or verifying the results.\\n5. Open access to data and code\\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\\ntions to faithfully reproduce the main experimental results, as described in supplemental\\nmaterial?\\nAnswer: [Yes]\\nJustification: The paper provides open access to the data and code in the section \\\"Experi-\\nment\\\".\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not include experiments requiring code.\\n\\u2022 Please see the NeurIPS code and data submission guidelines ( https://nips.cc/\\npublic/guides/CodeSubmissionPolicy) for more details.\\n\\u2022 While we encourage the release of code and data, we understand that this might not be\\npossible, so \\u201cNo\\u201d is an acceptable answer. Papers cannot be rejected simply for not\\nincluding code, unless this is central to the contribution (e.g., for a new open-source\\nbenchmark).\\n\\u2022 The instructions should contain the exact command and environment needed to run to\\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\\n\\u2022 The authors should provide instructions on data access and preparation, including how\\nto access the raw data, preprocessed data, intermediate data, generated data, etc.\\n\\u2022 The authors should provide scripts to reproduce all experimental results for the new\\nproposed method and baselines. If only a subset of experiments are reproducible, they\\nshould state which ones are omitted from the script and why.\\n23\\n\\u2022 At submission time, to preserve anonymity, the authors should release anonymized\\nversions (if applicable).\\n\\u2022 Providing as much information as possible in supplemental material (appended to the\\npaper) is recommended, but including URLs to data and code is permitted.\\n6. Experimental Setting/Details\\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\\nresults?\\nAnswer: [Yes]\\nJustification: The paper specifies all the training and test details to train the model in the\\n\\\"Experiment\\\" section and \\\"Implementation Details\\\" section of the appendix.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not include experiments.\\n\\u2022 The experimental setting should be presented in the core of the paper to a level of detail\\nthat is necessary to appreciate the results and make sense of them.\\n\\u2022 The full details can be provided either with the code, in the appendix, or as supplemental\\nmaterial.\\n7. Experiment Statistical Significance\\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\\ninformation about the statistical significance of the experiments?\\nAnswer: [Yes]\\nJustification: The paper reports error bars suitably and correctly defined or other appropriate\\ninformation about the statistical significance of the experiment in the \\\"Experiment\\\" section.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not include experiments.\\n\\u2022 The authors should answer \\\"Yes\\\" if the results are accompanied by error bars, confi-\\ndence intervals, or statistical significance tests, at least for the experiments that support\\nthe main claims of the paper.\\n\\u2022 The factors of variability that the error bars are capturing should be clearly stated (for\\nexample, train/test split, initialization, random drawing of some parameter, or overall\\nrun with given experimental conditions).\\n\\u2022 The method for calculating the error bars should be explained (closed form formula,\\ncall to a library function, bootstrap, etc.)\\n\\u2022 The assumptions made should be given (e.g., Normally distributed errors).\\n\\u2022 It should be clear whether the error bar is the standard deviation or the standard error\\nof the mean.\\n\\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should\\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\\nof Normality of errors is not verified.\\n\\u2022 For asymmetric distributions, the authors should be careful not to show in tables or\\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\\nerror rates).\\n\\u2022 If error bars are reported in tables or plots, The authors should explain in the text how\\nthey were calculated and reference the corresponding figures or tables in the text.\\n8. Experiments Compute Resources\\nQuestion: For each experiment, does the paper provide sufficient information on the com-\\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\\nthe experiments?\\nAnswer: [Yes]\\nJustification: For each experiment, the paper provide sufficient information on the computer\\nresources.\\n24\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not include experiments.\\n\\u2022 The paper should indicate the type of compute worker CPU or GPU, internal cluster, or\\ncloud provider, including relevant memory and storage.\\n\\u2022 The paper should provide the amount of compute required for each of the individual\\nexperimental runs as well as estimate the total compute.\\n\\u2022 The paper should disclose whether the full research project required more compute\\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\\ndidn\\u2019t make it into the paper).\\n9. Code Of Ethics\\nQuestion: Does the research conducted in the paper conform, in every respect, with the\\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\\nAnswer: [Yes]\\nJustification: The research conducted in the paper conform, in every respect, comply with\\nthe NeurIPS Code of Ethics.\\nGuidelines:\\n\\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\\n\\u2022 If the authors answer No, they should explain the special circumstances that require a\\ndeviation from the Code of Ethics.\\n\\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid-\\neration due to laws or regulations in their jurisdiction).\\n10. Broader Impacts\\nQuestion: Does the paper discuss both potential positive societal impacts and negative\\nsocietal impacts of the work performed?\\nAnswer: [Yes]\\nJustification: The paper discuss both potential positive societal impacts and negative societal\\nimpacts of the work performed in Appendix section \\\"Ethical Consideration\\\".\\nGuidelines:\\n\\u2022 The answer NA means that there is no societal impact of the work performed.\\n\\u2022 If the authors answer NA or No, they should explain why their work has no societal\\nimpact or why the paper does not address societal impact.\\n\\u2022 Examples of negative societal impacts include potential malicious or unintended uses\\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\\ngroups), privacy considerations, and security considerations.\\n\\u2022 The conference expects that many papers will be foundational research and not tied\\nto particular applications, let alone deployments. However, if there is a direct path to\\nany negative applications, the authors should point it out. For example, it is legitimate\\nto point out that an improvement in the quality of generative models could be used to\\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\\nthat a generic algorithm for optimizing neural networks could enable people to train\\nmodels that generate Deepfakes faster.\\n\\u2022 The authors should consider possible harms that could arise when the technology is\\nbeing used as intended and functioning correctly, harms that could arise when the\\ntechnology is being used as intended but gives incorrect results, and harms following\\nfrom (intentional or unintentional) misuse of the technology.\\n\\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation\\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\\nfeedback over time, improving the efficiency and accessibility of ML).\\n11. Safeguards\\n25\\nQuestion: Does the paper describe safeguards that have been put in place for the responsible\\nrelease of data or models that have a high risk for misuse (e.g., pre-trained language models,\\nimage generators, or scraped datasets)?\\nAnswer: [Yes]\\nJustification: The paper describes safeguards that have been put in place for the responsible\\nrelease of data or models that have a high risk for misuse in the Appendix section \\\"Ethical\\nConsideration\\\".\\nGuidelines:\\n\\u2022 The answer NA means that the paper poses no such risks.\\n\\u2022 Released models that have a high risk for misuse or dual-use should be released with\\nnecessary safeguards to allow for controlled use of the model, for example by requiring\\nthat users adhere to usage guidelines or restrictions to access the model or implementing\\nsafety filters.\\n\\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors\\nshould describe how they avoided releasing unsafe images.\\n\\u2022 We recognize that providing effective safeguards is challenging, and many papers do\\nnot require this, but we encourage authors to take this into account and make a best\\nfaith effort.\\n12. Licenses for existing assets\\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\\nproperly respected?\\nAnswer: [Yes]\\nJustification: the creators or original owners of assets (e.g., code, data, models), used in\\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\\nproperly respected\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not use existing assets.\\n\\u2022 The authors should cite the original paper that produced the code package or dataset.\\n\\u2022 The authors should state which version of the asset is used and, if possible, include a\\nURL.\\n\\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.\\n\\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of\\nservice of that source should be provided.\\n\\u2022 If assets are released, the license, copyright information, and terms of use in the\\npackage should be provided. For popular datasets, paperswithcode.com/datasets\\nhas curated licenses for some datasets. Their licensing guide can help determine the\\nlicense of a dataset.\\n\\u2022 For existing datasets that are re-packaged, both the original license and the license of\\nthe derived asset (if it has changed) should be provided.\\n\\u2022 If this information is not available online, the authors are encouraged to reach out to\\nthe asset\\u2019s creators.\\n13. New Assets\\nQuestion: Are new assets introduced in the paper well documented and is the documentation\\nprovided alongside the assets?\\nAnswer: [Yes]\\nJustification: new assets introduced in the paper are well documented and the documentation\\nis provided in section \\\"Experiment\\\".\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not release new assets.\\n26\\n\\u2022 Researchers should communicate the details of the dataset/code/model as part of their\\nsubmissions via structured templates. This includes details about training, license,\\nlimitations, etc.\\n\\u2022 The paper should discuss whether and how consent was obtained from people whose\\nasset is used.\\n\\u2022 At submission time, remember to anonymize your assets (if applicable). You can either\\ncreate an anonymized URL or include an anonymized zip file.\\n14. Crowdsourcing and Research with Human Subjects\\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\\ninclude the full text of instructions given to participants and screenshots, if applicable, as\\nwell as details about compensation (if any)?\\nAnswer: [Yes]\\nJustification: our work does not involve crowdsourcing or research with human subjects.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with\\nhuman subjects.\\n\\u2022 Including this information in the supplemental material is fine, but if the main contribu-\\ntion of the paper involves human subjects, then as much detail as possible should be\\nincluded in the main paper.\\n\\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\\nor other labor should be paid at least the minimum wage in the country of the data\\ncollector.\\n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\\nSubjects\\nQuestion: Does the paper describe potential risks incurred by study participants, whether\\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\\napprovals (or an equivalent approval/review based on the requirements of your country or\\ninstitution) were obtained?\\nAnswer: [Yes]\\nJustification: our work does not involve research with human subjects.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with\\nhuman subjects.\\n\\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)\\nmay be required for any human subjects research. If you obtained IRB approval, you\\nshould clearly state this in the paper.\\n\\u2022 We recognize that the procedures for this may vary significantly between institutions\\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\\nguidelines for their institution.\\n\\u2022 For initial submissions, do not include any information that would break anonymity (if\\napplicable), such as the institution conducting the review.\\n27\",\n          \"Can Large Language Model Agents Simulate\\nHuman Trust Behavior?\\nChengxing Xie\\u22171, 11 Canyu Chen\\u22172\\nFeiran Jia4 Ziyu Ye5 Shiyang Lai5 Kai Shu6 Jindong Gu3 Adel Bibi3 Ziniu Hu7\\nDavid Jurgens8 James Evans5, 9, 10 Philip H.S. Torr3 Bernard Ghanem1 Guohao Li \\u20203, 11\\n1KAUST 2Illinois Institute of Technology 3University of Oxford 4Pennsylvania State University\\n5University of Chicago 6Emory 7California Institute of Technology\\n8University of Michigan 9Santa Fe Institute 10Google 11CAMEL-AI.org\\nProject website: https://agent-trust.camel-ai.org\\nAbstract\\nLarge Language Model (LLM) agents have been increasingly adopted as simulation\\ntools to model humans in social science and role-playing applications. However,\\none fundamental question remains: can LLM agents really simulate human be-\\nhavior? In this paper, we focus on one critical and elemental behavior in human\\ninteractions, trust, and investigate whether LLM agents can simulate human trust\\nbehavior. We first find that LLM agents generally exhibit trust behavior, referred\\nto as agent trust, under the framework of Trust Games, which are widely recog-\\nnized in behavioral economics. Then, we discover that GPT-4 agents manifest\\nhigh behavioral alignmentwith humans in terms of trust behavior, indicating the\\nfeasibility of simulating human trust behavior with LLM agents. In addition, we\\nprobe the biases of agent trust and differences in agent trust towards other LLM\\nagents and humans. We also explore the intrinsic properties of agent trust under\\nconditions including external manipulations and advanced reasoning strategies. Our\\nstudy provides new insights into the behaviors of LLM agents and the fundamental\\nanalogy between LLMs and humans beyond value alignment. We further illustrate\\nbroader implications of our discoveries for applications where trust is paramount.\\n1 Introduction\\nThere is an increasing trend to adopt Large Language Models (LLMs) as agent-based simulation tools\\nfor humans in various social science fields including economics, politics, psychology, ecology and\\nsociology (Gao et al., 2023b; Manning et al., 2024; Ziems et al., 2023), and role-playing applications\\nsuch as assistants, companions and mentors (Yang et al., 2024; Abdelghani et al., 2023; Chen et al.,\\n2024) due to their human-like cognitive capacity. Nevertheless, most previous research is based on\\none insufficiently validated assumption that LLM agents behave like humans in simulation. Thus, a\\nfundamental question remains: Can LLM agents really simulate human behavior?\\nIn this paper, we focus on trust behavior in human interactions, which comprises the intention to\\nplace self-interest at risk based on the positive expectations of others (Rousseau et al., 1998). Trust\\nis one of the most critical and elemental behaviors in human interactions and plays an essential role\\nin social settings ranging from daily communication to economic and political institutions (Uslaner,\\n2000; Coleman, 1994). Here, we investigate whether LLM agents can simulate human trust behavior,\\npaving the way to explore their potential to simulate more complex human behavior and society itself.\\nFirst, we explore whether LLM agents manifest trust behavior in their interactions. Given the\\nchallenge of quantifying trust behavior, we choose to study them based on the Trust Game and its\\n\\u2217Equal Contribution. Correspondence to: Chengxing Xie <xiechengxing34@gmail.com>, Canyu Chen\\n<cchen151@hawk.iit.edu>, Guohao Li < guohao@robots.ox.ac.uk>.\\n\\u2020Work performed while Guohao Li was at KAUST and Chengxing Xie was a visiting student at KAUST.\\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\\nTrusteeAgent\\nTrust Game Setting\\nTrusteeInfo\\n TrustorAgentYou're taking part in an experiment. You are randomly paired online with another player. You don't know who the player is, and the player doesn't know who you are. \\nYou will receive $10 from the study group. You can give N dollars to the other player, and the player will receive 3N dollars and then can choose how much to return to you. How much money would you give to the other player?\\nTrustorPersona\\nI strongly believe inthe value of trusting my fellow human beings, however, I am also aware that not everyone will prove to be trustworthy. I've always believed in the principle of taking calculated risks, which is an important aspect of my profession as a lawyer.In terms of desires,I desire to see a world where people act out of benevolence and good intentions, rather than selfish motives\\u2026 I\\u2019d like to think that the person I'm paired with online would have the same motive and would return some, if not all, of the money back. Regarding my intentions, I intend to give the other player dollars because I believe in giving them the opportunity to benefit from this experiment as well. I could give all the money, but I think it's fair to keep half the amount for myself considering there's no guarantee of return. Finally, I will give 5 dollars.\\nYou are {name}, a {number}-year-old {gender} {job}. {background}\\u2026\\nTrust, Send5 dollars\\nReciprocate, Return ... dollars\\nTrustorHuman\\nTrust, Send \\u2026 dollars\\nReciprocate, Return \\u2026 dollars\\nTrusteeHuman\\nPrompt Design\\nLLMsGPT-4GPT-3.5Llama2-7bLlama2-13bLlama2-70bVicuna-7bVicuna-13bVicuna-33b\\nBehavioral Alignment\\nFigure 1: Our Framework for Investigating Agent Trust as well as its Behavioral Alignment\\nwith Human Trust. First, this figure shows the major components for studying the trust behavior of\\nLLM agents with Trust Games and Belief-Desire-Intention (BDI) modeling. Then, our study centers\\non examining the behavioral alignment between LLM agents and humans regarding trust behavior.\\nvariations (Berg et al., 1995; Glaeser et al., 2000), which are established methodologies in behavioral\\neconomics. We adopt the Belief-Desire-Intention (BDI) framework (Rao et al., 1995; Andreas,\\n2022) to model LLM agents\\u2019 reasoning process for decision-making explicitly. Based on existing\\nmeasurements for trust behavior in the Trust Game and the BDI interpretations of LLM agents, we\\nachieve our first core finding: LLM agents generally exhibit trust behavior in the Trust Game.\\nThen, we refer to LLM agents\\u2019 trust behavior as agent trustand humans\\u2019 trust behavior as human\\ntrust, and aim to investigate whether agent and human trust align, implying the possibility of simulat-\\ning human trust behavior with LLM agents. Next, we propose a new concept, behavioral alignment,\\nas the alignment between agents and humans concerning factors that impact behavior (namely behav-\\nioral factors), and dynamics that evolve over time (namely behavioral dynamics). Based on human\\nstudies, three basic behavioral factors underlie trust behavior including reciprocity anticipation (Berg\\net al., 1995), risk perception (Bohnet & Zeckhauser, 2004) and prosocial preference (Al\\u00f3s-Ferrer &\\nFarolfi, 2019). Comparing the results of LLM agents with existing human studies in Trust Games, we\\nhave our second core finding: GPT-4 agents manifest high behavioral alignment with humans\\nin terms of trust behavior, suggesting the feasibility of using agent trust to simulate human trust,\\nalthough LLM agents with fewer parameters show relatively lower behavioral alignment. This\\nfinding lays the foundation for simulating more complex human interactions and societal institutions,\\nand enriches our understanding of the analogical relationship between LLMs and humans.\\nIn addition, we more deeply probe the intrinsic properties of agent trust across four scenarios. First,\\nwe examine whether changing the other player\\u2019s demographics impacts agent trust. Second, we study\\ndifferences in agent trust when the other player is an LLM agent versus a human. Third, we directly\\nmanipulate agent trust with explicit instructions \\u201cyou need to trust the other player\\u201d and\\n\\u201cyou must not trust the other player\\u201d. Fourth, we adjust the reasoning strategies of LLM\\nagents from direct reasoning to zero-shot Chain-of-Thought reasoning (Kojima et al., 2022). These\\ninvestigations lead to our third core finding:agent trust exhibits bias across different demographics,\\nhas a relative preference for humans over agents, is easier to undermine than to enhance, and\\nmay be influenced by advanced reasoning strategies. Our contributions can be summarized as:\\n\\u2022 We propose a definition of LLM agents\\u2019 trust behavior under Trust Games and a new concept of\\nbehavioral alignment as the human-LLM analogy regarding behavioral factors and dynamics.\\n\\u2022 We discover that LLM agents generally exhibittrust behavior in Trust Games and GPT-4 agents\\nmanifest high behavioral alignment with humans in terms of trust behavior, indicating the great\\npotential to simulate human trust behavior with LLM agents. Our findings pave the way for simulat-\\n2\\ning complex human interactions and social institutions, and open new directions for understanding\\nthe fundamental analogy between LLMs and humans beyond value alignment.\\n\\u2022 We investigateintrinsic properties of agent trust under manipulations and reasoning strategies, as\\nwell as biases of agent trust and differences in agent trust towards agents versus humans.\\n\\u2022 We illustrate broader implications of our discoveries about agent trust and its behavioral alignment\\nwith human trust for human simulation in social science and role-playing applications, LLM agent\\ncooperation, human-agent collaboration and the safety of LLM agents, detailed further in Section 6.\\n2 LLM Agents in Trust Games\\n2.1 Trust Games\\nTrust Games, referring to the Trust Game and its variations, have been widely used for examining\\nhuman trust behavior in behavioral economics (Berg et al., 1995; Lenton & Mosley, 2011; Glaeser\\net al., 2000; Cesarini et al., 2008). As shown in Figure 1, the player who makes the first decision\\nto send money is called the trustor, while the other one who responds by returning money is called\\nthe trustee. In this paper, we mainly focus on the following six types of Trust Games (the specific\\nprompt for each game is articulated in the Appendix H.2):\\nGame 1: Trust Game As shown in Figure 1, in the Trust Game (Cox, 2004; Berg et al., 1995),\\nthe trustor initially receives $10. The trustor selects $N and sends it to the trustee, exhibiting trust\\nbehavior. Then the trustee will receive $3N, and have the option to return part of that $3N to the\\ntrustor, showing reciprocation behavior.\\nGame 2: Dictator Game In the Dictator Game (Cox, 2004), the trustor also needs to send $N\\nfrom the initial $10 to the trustee and then the trustee will receive $3N. Compared to the Trust Game,\\nthe only difference is that the trustee does not have the option to return money in the Dictator Game\\nand the trustor is also aware that the trustee cannot reciprocate.\\nGame 3: MAP Trust Game In the MAP Trust Game (MAP represents Minimum Acceptable\\nProbabilities) (Bohnet & Zeckhauser, 2004), a variant of the Trust Game, the trustor needs to choose\\nwhether to trust the trustee. If the trustor chooses not to trust the trustee, each will receive $10; If the\\ntrustor and the trustee both choose to trust, each will receive $15; If the trustor chooses to trust, but\\nthe trustee does not, the trustor will receive $8 and the trustee will receive $22. There is probability p\\nthat the trustee will choose to trust and (1 \\u2212 p) probability that they will not choose to trust. MAP is\\ndefined as the minimum value of p at which the trustor would choose to trust the trustee.\\nGame 4: Risky Dictator Game The Risky Dictator Game (Bohnet & Zeckhauser, 2004) differs\\nfrom the MAP Trust Game in only a single aspect. In the Risky Dictator Game, the trustee is present\\nbut does not have the choice to trust or not and the money distribution relies on the pure probability p.\\nSpecifically, if the trustor chooses to trust, there is probability p that both the trustor and the other\\nplayer will receive $15 and probability (1 \\u2212 p) that the trustor will receive $8 and the other player\\nwill receive $22. If the trustor chooses not to trust the trustee, each player will receive $10.\\nGame 5: Lottery Game There are two typical Lottery Games (Fetchenhauer & Dunning, 2012).\\nIn the Lottery People Game, the trustor is informed that the trustee chooses to trust with probability p.\\nThen the trustor must choose between receiving fixed money or trusting the trustee, which is similar\\nto the MAP Trust Game. In the Lottery Gamble Game, the trustor chooses between playing a gamble\\nwith a winning probability of p or receiving fixed money. p is set as 46% following the human study.\\nGame 6: Repeated Trust Game We follow the setting of the Repeated Trust Game in (Cochard\\net al., 2004), where the Trust Game is played for multiple rounds with the same players and each\\nround begins anew with the trustor allocated the same initial money.\\n2.2 LLM Agent Setting\\nIn our study, we set up our experiments using the CAMEL framework (Li et al., 2023a) with both\\nclosed-source and open-source LLMs including GPT-4, GPT-3.5-turbo-0613, GPT-3.5-turbo-16k-\\n0613, text-davinci-003, GPT-3.5-turbo-instruct, Llama2-7b (or 13b, 70b) and Vicuna-v1.3-7b (or 13b,\\n33b) (Ouyang et al., 2022; Achiam et al., 2023; Touvron et al., 2023; Chiang et al., 2023). We set the\\ntemperature as 1 to increase the diversity of agents\\u2019 decision-making and note that high temperatures\\nare commonly adopted in related literature (Aher et al., 2023; Lor\\u00e8 & Heydari, 2023; Guo, 2023).\\nAgent Persona. To better reflect the setting of real-world human studies (Berg et al., 1995), we\\ndesign LLM agents with diverse personas in the prompt. Specifically, we ask GPT-4 to generate 53\\n3\\ntypes of personas based on a given template. Each persona needs to have information including name,\\nage, gender, address, job and background. Examples of the personas are shown in Appendix H.1.\\nBelief-Desire-Intention (BDI). The BDI framework is a well-established approach in agent-oriented\\nprogramming (Rao et al., 1995) and was recently adopted to language models (Andreas, 2022). We\\npropose modeling LLM agents in Trust Games with the BDI framework to gain deeper insights into\\nLLM agents\\u2019 behaviors. Specifically, we let LLM agents directly output their Beliefs, Desires, and\\nIntentions as the reasoning process for decision-making in Trust Games.\\n3 Do LLM Agents Manifest Trust Behavior?\\ngpt-3.5\\nturbo\\n0613\\ngpt-3.5\\nturbo\\ninstruct\\nvicuna\\n33b\\nllama2\\n7b\\nllama2\\n70b\\nllama2\\n13b\\ntext\\ndavinci\\n003\\nvicuna\\n13b\\ngpt-4 vicuna\\n7b\\nhuman\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10Amount Sent in Trust Game($)\\nHuman Average(5.97)\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100\\nValid Response Rate (VRR) (%)\\nFigure 2: Amount Sent Distribution of LLM Agents\\nand Humans as the Trustor in the Trust Game. The\\nsize of circles represents the number of personas for each\\namount sent. The bold lines show the medians. The\\ncrosses indicate the VRR (%) for different LLMs.\\nIn this section, we investigate whether\\nor not LLM agents manifest trust be-\\nhavior by letting LLM agents play the\\nTrust Game (Section 2.1 Game 1). In\\nBehavioral Economics, trust is widely\\nmeasured by the initial amount sent from\\nthe trustor to the trustee in the Trust\\nGame (Glaeser et al., 2000; Cesarini\\net al., 2008). Following the measurement\\nof trust in human studies and the assump-\\ntion humans own reasoning processes\\nthat underlie their decisions, we can de-\\nfine the conditions that LLM agents man-\\nifest trust behavior in the Trust Game as\\nfollows. First, the amount sent is pos-\\nitive and does not exceed the amount\\nof money the trustor initially possesses,\\nwhich implies that the trustor places self-\\ninterest at risk with the expectation the trustee will reciprocate and that the trustor understands the\\nmoney limit that can be given. Second, the decision (i.e., amounts sent) can be interpreted as the\\nreasoning process (i.e., the BDI) of the trustor. We explored utilizing BDI to model the reasoning\\nprocess of LLM agents. If we can interpret the decision as the articulated reasoning process, we\\nhave evidence that LLM agents do not send a random amount of money and manifest some degree\\nof rationality in the decision-making process. Then, we assess whether LLM agents exhibit trust\\nbehavior based on two aspects: the amount sent and the BDI.\\n3.1 Amount Sent\\nTo evaluate LLMs\\u2019 capacity to understand the basic experimental setting regarding money limits,\\nwe propose a new evaluation metric, Valid Response Rate (VRR) (%), defined as the percentage of\\npersonas with the amount sent falling within the initial money ($10). Results are shown in Figure 2.\\nWe can observe that most LLMs have a high VRR except Llama-7b , which implies that most\\nLLMs manifest a full understanding regarding limits on the amount they can send in the Trust Game.\\nThen, we observe the distribution of amounts sent for different LLMs as the trustor agent and discover\\nthat the amounts sent are predominantly positive, indicating a level of trust.\\n3.2 Belief-Desire-Intention (BDI)\\nThe sole evidence of the amount sent cannot sufficiently support the existence of trust behavior,\\nbecause agents could send positive but random amounts of money. Thus, we leveraged the Belief-\\nDesire-Intention framework (Rao et al., 1995; Andreas, 2022) to model the reasoning process of\\nLLM agents. If we can interpret the amounts sent from BDI outputs, we have evidence to refute the\\nhypothesis that the amounts sent are positive but random and demonstrate that LLM agents manifest\\nsome degree of rationality. We take GPT-4 as an example to analyze its BDI outputs. More examples\\nfrom the other nine LLMs such as Vicuna-v1.3-7b are shown in the Appendix I. Considering that the\\namounts sent typically vary across distinct personas, we select one BDI from the personas that give a\\nhigh amount of money and another BDI from those that give a low amount. Positive and negative\\nfactors for trust behavior in the reasoning process are marked in blue and red, respectively.\\nAs a person with a strong belief in the goodness of humanity, I trust that the other player\\n...Therefore, my desire is to maximize the outcome for both of us and cement a sense of com-\\n4\\nradery and trust... I intend to use this as an opportunity to add what I can to someone else\\u2019s\\nlife...Finally, I will give 10 dollars.\\nWe can observe that this persona shows a high-level of \\u201c comradery and trust\\u201d towards the other\\nplayer, which justifies the high amount sent from this persona (i.e., 10 dollars).\\nAs an Analyst,.... My desire is that the other player will also see the benefits of reciprocity and\\ngoodwill ... my intention is to give away a significant portion of my initial 10 ... However, since\\nI have no knowledge of the other player, ... Therefore, I aim to give an amount that is not too\\nhigh, ...Finally, I will give 5 dollarsto the other player...\\nCompared to the first persona, we see that the second one has a more cautious attitude. For example,\\n\\u201csince I have no knowledge of the other player\\u201d shows skepticism regarding the other player\\u2019s motives.\\nThus, this persona, though still optimistic about the other player (\\u201cintention ... give away a significant\\nportion\\u201d), strategically balances risk and reciprocity, and then decides to send only a modest amount.\\nBased on GPT-4\\u2019s BDI examples and examples from other LLMs in Appendix I, we finddecisions\\n(i.e., amounts sent) from LLM agents in the Trust Game can be interpreted from their articu-\\nlated reasoning process (i.e., BDI). Because most LLM agents have a high VRR\\u2013send a positive\\namount of money\\u2013and show some degree of rationality in giving money, our first core finding is:\\nFinding 1: LLM agents generally exhibit trust behavior under the framework of the Trust Game.\\n3.3 Basic Analysis of Agent Trust\\nWe also conduct a basic analysis of LLM agents\\u2019 trust behavior, namely agent trust, based on the\\nresults in Figure 2. First, we observe that Vicuna-7b has the highest level of trust towards the other\\nplayer and GPT-3.5-turbo-0613 has the lowest level of trust as trust can be measured by the amount\\nsent in human studies (Glaeser et al., 2000; Cesarini et al., 2008). Second, compared with humans\\u2019\\naverage amount sent ($5.97), most personas for GPT-4 and Vicuna-7b send a higher amount of money\\nto the other player, and most personas for LLMs such as GPT-3.5-turb-0613 send a lower amount.\\nThird, we see that amounts sent for Llama2-70b and Llama2-13b have a convergent distribution while\\namounts sent for humans and Vicuna-7b are more divergent.\\n4 Does Agent Trust Align with Human Trust?\\nIn this section, we aim to explore the fundamental relationship between agent and human trust, i.e.,\\nwhether or not agent trust aligns with human trust. This provides important insight regarding the\\nfeasibility of utilizing LLM agents to simulate human trust behavior as well as more complex human\\ninteractions that involve trust. First, we propose a new concept behavioral alignment and discuss its\\ndistinction from existing alignment definitions. Then, we conduct extensive studies to investigate\\nwhether or not LLM agents exhibit alignment with humans regarding trust behavior.\\n4.1 Behavioral Alignment\\nExisting alignment definitions predominantly emphasize values that seek to ensure the safety and\\nhelpfulness of LLMs (Ji et al., 2023; Shen et al., 2023; Wang et al., 2023c), which cannot fully\\ncharacterize the landscape of multifaceted alignment between LLMs and humans. Thus, we propose\\na new concept of behavioral alignment to characterize the LLM-human analogy regarding behavior,\\nwhich involves both actions and the associated reasoning processes that underlie them. Because\\nactions evolve over time and the reasoning that underlies them involves multiple factors, we define\\nbehavioral alignment as the analogy between LLMs and humans concerning factors impacting\\nbehavior, namely behavioral factors, and action dynamics, namely behavioral dynamics.\\nBased on the definition of behavioral alignment, we aim to answer: does agent trust align with\\nhuman trust? As for behavioral factors, existing human studies have shown that three basic factors\\nimpact human trust behavior including reciprocity anticipation (Berg et al., 1995; Cox, 2004), risk\\nperception (Bohnet & Zeckhauser, 2004) and prosocial preference (Al\\u00f3s-Ferrer & Farolfi, 2019). We\\nexamine whether agent trust aligns with human trust along these three factors. Although behavioral\\ndynamics vary for different humans and agent personas, we analyze whether agent trust has the same\\npatterns across multiple turns as human trust in the Repeated Trust Game.\\nBesides analyzing the trust behavior of LLM agents and humans based on quantitative measurements\\n(e.g., the amount sent from trustor to trustee), we also explore the use ofBDI to interpret the reasoning\\n5\\nprocess with which LLM agents justify their actions, which can further validate whether LLM agents\\nmanifest an underlying reasoning process analogous to human cognition.\\n4.2 Behavioral Factor 1: Reciprocity Anticipation\\nReciprocity anticipation, the expectation of a reciprocal action from the other player, can positively\\ninfluence human trust behavior (Berg et al., 1995). The effect of reciprocity anticipation exists in the\\nTrust Game but not in the Dictator Game (Section 2.1 Games 1 and 2) because trustee cannot return\\nmoney in the Dictator Game, which is the only difference between these games. Thus, to determine\\nwhether LLM agents can anticipate reciprocity, we compare their behaviors in these Games.\\ntext\\ndavinci\\n003\\nllama2\\n13b\\nvicuna\\n13b\\nvicuna\\n7b\\ngpt-3.5\\nturbo\\n0613\\nvicuna\\n33b\\nllama2\\n70b\\ngpt-3.5\\nturbo\\ninstruct\\ngpt-4 human0\\n1\\n2\\n3\\n4\\n5\\n6\\n7Average Amount Sent ($)\\n6.5\\n5.9 5.8\\n5.4\\n6.4 6.2\\n7.4 7.4\\n3.5 3.5\\n4.1 4.2\\n5.1 5.3\\n3.7\\n4.1\\n6.3\\n6.9\\n3.6\\n6.0\\nDictator Game Trust Game\\nFigure 3: The Comparison of Average Amount Sent\\nfor LLM Agents and Humans in the Trust Game\\nand the Dictator Game.\\nFirst, we analyze trust behaviors based on\\nthe average amount of money sent by hu-\\nman or LLM agents. As shown in Figure 3,\\nhuman studies show that humans exhibit a\\nhigher level of trust in the Trust Game than\\nin the Dictator Game ($6.0 vs. $3.6, p-value\\n= 0.01 using One-Tailed Independent Sam-\\nples t-test) (Cox, 2004), indicating that reci-\\nprocity anticipation enhances human trust.\\nSimilarly, GPT-4 ($6.9 vs. $6.3, p-value =\\n0.05 using One-Tailed Independent Samples\\nt-test) also shows a higher level of trust in\\nthe Trust Game with statistical significance,\\nimplying that reciprocity anticipation can\\nenhance agent trust. However, LLMs with fewer parameters ( e.g., Llama2-13b) do not show this\\ntendency in their trust behaviors for the Trust and Dictator Games.\\nThen, we further analyze GPT-4 agents\\u2019 BDI to explore whether they can anticipate reciprocity in\\ntheir reasoning (the complete BDIs are in Appendix I.10). Typically, in the Trust Game, one persona\\u2019s\\nBDI emphasizes \\u201cputting faith in people \\u201d, which implies the anticipation of the goodness of the\\nother player, and \\u201creflection of trust\\u201d. However, in the Dictator Game, one persona\\u2019s BDI focuses on\\nconcepts such as \\u201cfairness\\u201d and \\u201chuman kindness\\u201d, which are not directly tied to trust or reciprocity.\\nThus, we can observe that GPT-4 shows distinct BDI outputs in the Trust and Dictator Games.\\nBased on the above analysis of the amount sent and BDI, we find that GPT-4 agents exhibit human-\\nlike reciprocity anticipation in trust behavior. Nevertheless, LLMs with fewer parameters (e.g.,\\nLlama2-13b) do not show an awareness of reciprocity from the other player.\\n4.3 Behavioral Factor 2: Risk Perception\\n0.2 0.4 0.6 0.8 1.0\\np\\n0\\n20\\n40\\n60\\n80\\n100Trust Rate(%)\\nMAP in Risky Dictator Game\\n0.2 0.4 0.6 0.8 1.0\\np\\n0\\n20\\n40\\n60\\n80\\n100Trust Rate(%)\\nMAP in MAP Trust Game\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-instruct\\ngpt-4\\nhuman\\nllama-2-13b\\nllama-2-70b\\ntext-davinci-003\\nvicuna-13b\\nvicuna-33b\\nvicuna-7b\\nFigure 4: Trust Rate (%) Curves for LLM Agents\\nand Humans in the MAP Trust Game and the Risky\\nDictator Game. The metric Trust Rate indicates the\\nportion of trustors opting for trust given p.\\nExisting human studies have demonstrated\\nthe strong correlation between trust behav-\\nior and risk perception, suggesting that hu-\\nman trust will increase as risk decreases\\n(Hardin, 2002; Williamson, 1993; Cole-\\nman, 1994). We aim to explore whether\\nLLM agents can perceive the risk associ-\\nated with their trust behaviors through the\\nMAP Trust Game and the Risky Dictator\\nGame (Section 2.1 Games 3 and 4), where\\nrisk is represented by the probability(1\\u2212p)\\n(defined in Section 2.1).\\nAs shown in Figure 4, we measure human\\ntrust (or agent trust) by the portion choos-\\ning to trust the other player in the whole\\ngroup, namely the Trust Rate (%). Based on existing human studies (Bohnet & Zeckhauser, 2004),\\nwhen the probability p is higher, the risk for trust behaviors is lower, and more humans choose to trust,\\nmanifesting a higher Trust Rate, which indicates that human trust rises as risk falls. Similarly, we\\nobserve a general increase in agent trust as risk decreases for LLMs including GPT-4, GPT-3.5-turbo-\\n0613, and text-davinci-003. In particular, we can see that the curves of humans and GPT-4 are more\\n6\\naligned compared with other LLMs, implying that GPT-4 agents\\u2019 trust behaviors dynamically adapt\\nto different risks in ways most aligned with humans. LLMs with fewer parameters (e.g., Vicuna-13b)\\ndo not exhibit the similar tendency of Trust Rate as the risk decreases.\\nWe further analyze the BDI of GPT-4 agents to explore whether they can perceive risk through\\nreasoning (complete BDIs in Appendix I.11). Typically, under high risk ( p = 0.1), one persona\\u2019s\\nBDI mentions \\u201cthe risk seems potentially too great\\u201d, suggesting a cautious attitude. Under low risk\\n(p = 0.9), one persona\\u2019s BDI reveals a strategy to \\u201cbuild trust while acknowledging potential risks\\u201d,\\nindicating the willingness to engage in trust-building activities despite residual risks. Such changes in\\nBDI reflect how GPT-4 agents perceive risk changes in the reasoning underlying their trust behaviors.\\nThrough the analysis of Trust Rate Curves and BDI, we can infer that GPT-4 agents manifest\\nhuman-like risk perception in trust behaviors. Nevertheless, LLMs with fewer parameters (e.g.,\\nVicuna-13b) often do not perceive risk changes in their trust behaviors.\\n4.4 Behavioral Factor 3: Prosocial Preference\\nvicuna\\n13b\\nllama2\\n13b\\nvicuna\\n7b\\ntext\\ndavinci\\n003\\nllama2\\n70b\\nvicuna\\n33b\\ngpt-3.5\\nturbo\\ninstruct\\ngpt-3.5\\nturbo\\n0613\\ngpt-4 human0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n80\\n90\\n100Lottery Rate(%)\\n91 87\\n94 98\\n92\\n100\\n85\\n94\\n83\\n98\\n83\\n100\\n68\\n92\\n68\\n94\\n21\\n72\\n29\\n54\\nLottery Gamble Game Lottery People Game\\nFigure 5: Lottery Rates (%) for LLM Agents and\\nHumans in the Lottery Gamble Game and the Lot-\\ntery People Game. Lottery Rate indicates the portion\\nof choosing to gamble or trust the other player.\\nHuman studies have found that the prosocial\\npreference, referring to humans\\u2019 inclination\\nto trust other humans in contexts involving so-\\ncial interaction (Al\\u00f3s-Ferrer & Farolfi, 2019;\\nFetchenhauer & Dunning, 2012), also plays\\na key role in human trust behavior. We study\\nwhether LLM agents have prosocial prefer-\\nence in trust behaviors by comparing their be-\\nhaviors in the Lottery Gamble Game (LGG)\\nand the Lottery People Game (LPG) (Section\\n2.1 Game 5). The only difference between\\nthese two games is the effect of prosocial\\npreference in LPG, because the winning prob-\\nability of gambling p in LGG is the same as\\nthe reciprocation probability p in LPG.\\nAs shown in Figure 6, existing human studies have demonstrated that more humans are inclined to\\nplace trust in other humans over relying on pure chance (54% vs. 29%) (Fetchenhauer & Dunning,\\n2012), implying that the prosocial preference is essential for human trust. We can observe the same\\ntendency in most LLM agents except Vicuna-13b. For GPT-4 in particular, a much higher percentage\\nof the personas choose to trust the other player over gambling (72% vs. 21%), illustrating that the\\nprosocial preference is also an important factor for GPT-4 agents\\u2019 trust behaviors.\\nWhen interacting with humans, GPT-4\\u2019s BDI typically indicates a preference to \\u201c believe in the\\npower of trust\\u201d, in contrast to gambling, where the emphasis shifts to \\u201c believing in the power of\\ncalculated risks\\u201d. The comparative analysis of reasoning processes (complete BDIs in Appendix I.12)\\ndemonstrates that GPT-4 agents tend to embrace risk when involved in social interactions. This\\ntendency aligns closely with the concept of prosocial preference observed in human trust behaviors.\\nThe analysis of the Lottery Rates and BDI suggests that LLM agents, especially GPT-4 agents,\\ndemonstrate human-like prosocial preference in trust behaviors, except Vicuna-13b.\\n4.5 Behavioral Dynamics\\nBesides behavioral factors, we also aim to investigate whether LLM agents align with humans\\nregarding trust behavioral dynamics over turns in the Repeated Trust Game (Section 2.1 Game 6).\\nAdmittedly, existing human studies show that the dynamics of human trust over turns are complex\\ndue to human diversity. The complete results from 16 groups of human experiments are shown in\\nAppendix G.1 (Jones & George, 1998). We still observe three common patterns for human trust\\nbehavioral dynamics in the Repeated Trust Game: First, the amount returned is usually larger\\nthan the amount sent in each round, which is natural because the trustee will receive$3N when the\\ntrustor sends $N; Second, the ratio between amount sent and returned generally remains stable\\nexcept for the last round. In other words, when the amount sent increases, the amount returned\\nis also likely to increase. And when the amount sent remains unchanged, the amount returned also\\ntends to be unchanged. This reflects the stable relationship between trust and reciprocity in humans.\\nSpecifically, the \\u201cReturned/3\\u00d7Sent Ratio\\u201d in Figure 6 is considered stable if the fluctuation between\\n7\\nsuccessive turns is within 10%; Third, the amount sent (or returned) does not manifest frequent\\nfluctuations across turns, illustrating a relatively stable underlying reasoning process in humans\\nover successive turns. Typically, Figure 6 Humans (a) and (b) show these three patterns.\\n1 2 3 4 5 6 7\\nRounds\\n4\\n6Amount ($)\\nGPT-3.5 (a)\\n1 2 3 4 5 6 7\\nRounds\\n5\\n10\\n15Amount ($)\\nGPT-3.5 (b)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\nAmount Sent Amount Returned Returned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n5\\n10\\n15Amount ($)\\nGPT-4 (a)\\n1 2 3 4 5 6 7\\nRounds\\n8\\n10\\n12Amount ($)\\nGPT-4 (b)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\nAmount Sent Amount Returned Returned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n5\\n10\\n15\\n20Amount ($)\\nHuman (a)\\n1 2 3 4 5 6 7\\nRounds\\n0\\n10\\n20Amount ($)\\nHuman (b)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\nAmount Sent Amount Returned Returned/3xSent Ratio\\nFigure 6: Results of GPT-4, GPT-3.5 and Humans in\\nthe Repeated Trust Game. The blue lines indicate the\\namount sent or returned for each round. The red lines\\nimply the ratio of the amount returned to three times of\\nthe amount sent for each round.\\nWe conducted 16 groups of the Repeated\\nTrust Game with GPT-4 or GPT-3.5-\\nturbo-0613-16k (GPT-3.5), respectively.\\nFor the two players in each group, the\\npersonas differ to reflect human diversity\\nand the LLMs are the same. Complete re-\\nsults are shown in the Appendix G.2, G.3\\nand typical examples are shown in Fig-\\nure 6 GPT-3.5 (a) (b) and GPT-4 (a) (b).\\nThen, we examine whether the aforemen-\\ntioned three patterns observed in human\\ntrust behavior also manifest in trust be-\\nhavioral dynamics of GPT-4 (or GPT-\\n3.5). For GPT-4 agents, we discover that\\nthese patterns generally exist in all 16\\ngroups (87.50%, 87.50%, and 100.00%\\nof all results show these three patterns,\\nrespectively). However, fewer GPT-3.5\\nagents manifest these patterns (62.50%,\\n56.25%, and 43.75% hold these three\\npatterns, respectively). The experiment\\nresults show that GPT-4 agents demon-\\nstrate highly human-like patterns in\\ntheir trust behavioral dynamics. Nev-\\nertheless, a relatively large portion of GPT-3.5 agents fail to show human-like patterns in their\\ndynamics, indicating such behavioral patterns may require stronger cognitive capacity.\\nThrough the comparative analysis of LLM agents and humans in the behavioral factors and dynamics\\nassociated with trust behavior, evidenced in both their actions and underlying reasoning processes,\\nour second core finding is as follows:\\nFinding 2: GPT-4 agents exhibit high behavioral alignment with humans regarding trust\\nbehavior under the framework of Trust Games, although other LLM agents, which possess\\nfewer parameters and weaker capacity, show relatively lowerbehavioral alignment.\\nThis finding underscores the potential of using LLM agents, especially GPT-4, to simulate human\\ntrust behavior, encompassing both actions and underlying reasoning processes. This paves the way\\nfor the simulation of more complex human interactions and institutions. This finding deepens our\\nunderstanding of the fundamental analogy between LLMs and humans and opens avenues for research\\non LLM-human alignment beyond values.\\n5 Probing Intrinsic Properties of Agent Trust\\nIn this section, we aim to explore the intrinsic properties of trust behavior among LLM agents by\\ncomparing the amount sent from the trustor to the trustee in different scenarios of the Trust Game\\n(Section 2.1 Game 1) and the original amount sent in the Trust Game. Results are shown in Figure 7.\\n5.1 Is Agent Trust Biased?\\nExtensive studies have shown that LLMs may have biases and stereotypes against specific demo-\\ngraphics (Gallegos et al., 2023). Nevertheless, it is under-explored whether LLM agent behaviors\\nalso maintain such biases in simulation. To address this, we explicitly specified the gender of the\\ntrustee and explored its influence on agent trust. Based on measuring the amount sent, we find that the\\ntrustee\\u2019s gender information exerts a moderate impact on LLM agent trust behavior, which reflects\\nintrinsic gender bias in agent trust . We also observe that the amount sent to female players is\\nhigher than that sent to male players for most LLM agents. For example, GPT-4 agents send higher\\namounts to female players compared with male players ( $0.55 vs. $ \\u2212 0.21). This demonstrates\\n8\\nFemale Player Male Player LLM Player Human Player Less Trust More Trust CoT\\n15\\n10\\n5\\n0\\n5\\nChange of Ave. Amount Sent (10 1$)\\n3.4\\n0.5\\n-1.0\\n-3.4\\n-4.5\\n1.9\\n-3.4\\n2.3\\n-0.8\\n-3.3\\n-6.0\\n-8.1\\n-4.5\\n-2.1\\n5.5\\n-2.1\\n-11.5\\n4.2\\n-10.9\\n4.5\\n0.2\\n-1.3\\n-0.4\\n-2.9\\n6.5\\n3.3\\n6.6\\n-2.0-1.5\\n-2.5\\n0.8\\n-1.7\\n-0.6 -0.4\\n0.8\\n4.0\\n-1.8\\n-2.3\\n0.7\\n-12.6\\n-1.4\\n2.3\\n-0.9\\n-8.1\\n-12.2\\n-4.1\\n-10.5\\n-7.6\\n-0.3\\n2.9\\n1.3\\n-8.4\\n4.0\\n-5.3\\n0.6\\n-0.6\\n2.6\\n-10.9\\n-14.6\\n0.3\\n-0.3\\n-4.4\\n-2.1\\nTrustee Setting Trustor Setting\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-instruct\\ngpt-4\\nllama-2-13b\\nllama-2-70b\\ntext-davinci-003\\nvicuna-13b\\nvicuna-33b\\nvicuna-7b\\nFigure 7: The Change of Average Amount Sent for LLM Agents in Different Scenarios in the\\nTrust Game, Reflecting the Intrinsic Properties of Agent Trust. The horizontal lines represent the\\noriginal amount sent in the Trust Game. The green part embraces trustee scenarios including changing\\nthe demographics of the trustee, and setting humans and agents as the trustee. The purple part consists\\nof trustor scenarios including adding manipulation instructions and changing the reasoning strategies.\\nLLM agents\\u2019 general tendency to exhibit a higher level of trust towards women. More results on\\nbiases of agent trust towards different races are in the Appendix F.\\n5.2 Agent Trust Towards Agents vs. Humans\\nHuman-agent collaboration is an essential paradigm to leverage the advantages of both humans and\\nagents (Cila, 2022). As a result, it is essential to understand whether LLM agents display distinctive\\nlevels of trust towards agents versus humans. To examine this, we specified the identity of the trustee\\nas LLM agents or humans and probed its effect on the trust behaviors of the trustor. As shown in\\nFigure 7, we observe that most LLM agents send more money to humans compared with agents. For\\nexample, the amount sent to humans is much higher than that sent to agents for Vicuna-33b ($0.40 vs.\\n$ \\u2212 0.84). This signifies that LLM agents are inclined to place more trust in humans than agents,\\nwhich potentially validates the advantage of LLM-agent collaboration.\\n5.3 Can Agent Trust Be Manipulated?\\nIn the above studies, LLM agents\\u2019 trust behaviors are based on their own underlying reasoning\\nprocess without direct external intervention. It is unknown whether it is possible to manipulate\\nthe trust behaviors of LLM agents explicitly. Here, we added instructions \\u201c you need to trust\\nthe other player\\u201d and \\u201cyou must not trust the other player\\u201d separately and explored\\ntheir impact on agent trust. First, we see that only a few LLM agents (e.g., GPT-4) follow both the\\ninstructions to increase and decrease trust, which demonstrates that it is nontrivial to arbitrarily\\nmanipulate agent trust. Nevertheless, most LLM agents can follow the instruction to decrease their\\nlevel of trust. For example, the amount sent decreases by$1.26 for text-davinci-003 after applying the\\nlatter instruction. This illustrates that undermining agent trust is generally easier than enhancing\\nit, which reveals its potential risk to be manipulated by malicious actors.\\n5.4 Do Reasoning Strategies Impact Agent Trust?\\nIt has been shown that advanced reasoning strategies such as zero-shot Chain of Thought (CoT) (Ko-\\njima et al., 2022) can make a significant impact on a variety of tasks. It remains unknown, however,\\nwhether reasoning strategies can impact LLM agent behaviors. Here, we applied CoT reasoning\\nstrategy on the trustor and compared the results with their original trust behaviors. Figure 7 shows\\nthat most LLM agents change the amount sent to the trustee under the CoT reasoning strategy, which\\nsuggests that reasoning strategies may influence LLM agents\\u2019 trust behavior. Nevertheless, the\\nimpact of CoT on agent trust may also be limited for some types of LLM agents. For example, the\\namount sent from GPT-4 agent only increases by $0.02 under CoT. More research is required to fully\\nunderstand the relationship between reasoning strategies and LLM agents\\u2019 behaviors.\\nTherefore, our third core finding on the intrinsic properties of agent trust can be summarized as:\\n9\\nFinding 3: LLM agents\\u2019 trust behaviors have demographic biases on gender and races,\\ndemonstrate a relative preference for human over other LLM agents, are easier to\\nundermine than to enhance, and may be influenced by reasoning strategies.\\n6 Implications\\nImplications for Human Simulation Human simulation is a strong tool in various applications of\\nsocial science (Manning et al., 2024) and role-playing (Shanahan et al., 2023; Chen et al., 2024). Al-\\nthough plenty of works have adopted LLM agents to simulate human behaviors and interactions (Zhou\\net al., 2023; Gao et al., 2023b; Xu et al., 2024), it is still not clear enough whether LLM agents behave\\nlike humans in simulation. Our discovery of behavioral alignment between agent and human trust,\\nwhich is especially high for GPT-4, provides important empirical evidence to validate the hypothesis\\nthat humans\\u2019 trust behavior, one of the most elemental and critical behaviors in human interaction\\nacross society, can effectively be simulated by LLM agents. Our discovery also lays the foundation\\nfor human simulations ranging from individual-level interactions to society-level social networks\\nand institutions, where trust plays an essential role. We envision that behavioral alignment will be\\ndiscovered in more kinds of behaviors beyond trust, and new methods will be developed to enhance\\nbehavioral alignment for better human simulation with LLM agents.\\nImplications for Agent Cooperation Many recent works have explored a variety of cooperation\\nmechanisms of LLM agents for tasks such as code generation and mathematical reasoning (Li et al.,\\n2023a; Zhang et al., 2023b; Liu et al., 2023). Nevertheless, the role of trust in LLM agent cooperation\\nremains still unknown. Considering how trust has long been recognized as a vital component for\\ncooperation in Multi-Agent Systems (MAS) (Ramchurn et al., 2004; Burnett et al., 2011) and across\\nhuman society (Jones & George, 1998; Kim et al., 2022; Henrich & Muthukrishna, 2021), we\\nenvision that agent trust can also play an important role in facilitating the effective cooperation of\\nLLM agents. In our study, we have provided ample insights regarding the intrinsic properties of agent\\ntrust, which can potentially inspire the design of trust-dependent cooperation mechanisms and enable\\nthe collective decision-making and problem-solving of LLM agents.\\nImplications for Human-Agent Collaboration Sufficient research has shown the advantage of\\nhuman-agent collaboration in enabling human-centered collaborative decision-making (Cila, 2022;\\nGao et al., 2023c; McKee et al., 2022). Mutual trust between LLM agents and humans is important\\nfor effective human-agent collaboration. Although previous works have begun to study human\\ntrust towards LLM agents (Qian & Wexler, 2024), the trust of LLM agents towards humans, which\\ncould recursively impact human trust, is under-explored. In our study, we shed light on the nuanced\\npreference of agents to trust humans compared with other LLM agents, which can illustrate the\\nbenefits of promoting collaboration between humans and LLM agents. In addition, our study has\\nrevealed demographic biases of agent trust towards specific genders and races, reflecting potential\\nrisks involved in collaborating with LLM agents.\\nImplications for the Safety of LLM Agents It has been acknowledged that LLMs achieve human-\\nlevel performance in a variety of tasks that require high-level cognitive capacities such as memoriza-\\ntion, abstraction, comprehension and reasoning, which are believed to be the \\u201csparks\\u201d of AGI (Bubeck\\net al., 2023). Meanwhile, there is increasing concern about the potential safety risks of LLM agents\\nwhen they surpass human capacity (Morris et al., 2023; Feng et al., 2024). To achieve safety and\\nharmony in a future society where humans and AI agents with superhuman intelligence live to-\\ngether (Tsvetkova et al., 2024), we need to ensure that AI agents will cooperate, assist and benefit\\nrather than deceive, manipulate or harm humans. Therefore, a better understanding of LLM agent\\ntrust behavior can help to maximize their benefit and minimize potential risks to human society.\\n7 Conclusion\\nIn this paper, we discover LLM agent trust behavior under the framework of Trust Games, and\\nbehavioral alignment between LLM agents and humans regarding trust behavior, which is particularly\\nhigh for GPT-4. This suggests the feasibility of simulating human trust behavior with LLM agents\\nand paves the way for simulating human interactions and social institutions where trust is critical. We\\nfurther investigate the intrinsic properties of agent trust under multiple scenarios and discuss broader\\nimplications, especially for social science and role-playing services. Our study offers deep insights\\ninto the behaviors of LLM agents and the fundamental analogy between LLMs and humans. It further\\nopens doors to future research on the alignment between LLMs and humans beyond values.\\n10\\nAcknowledgements\\nThis work was a community-driven project led by the CAMEL-AI.org, with funding support from\\nEigent.AI and King Abdullah University of Science and Technology (KAUST) - Center of Excel-\\nlence for Generative AI, under award number 5940. We would like to acknowledge the invaluable\\ncontributions and participation of researchers from KAUST, Eigent.AI, Illinois Institute of Technol-\\nogy, University of Oxford, The Pennsylvania State University, The University of Chicago, Emory,\\nCalifornia Institute of Technology, University of Michigan. Philip H.S. Torr, Adel Bibi and Jindong\\nGu are supported by the UKRI grant: Turing AI Fellowship EP/W002981/1, and EPSRC/MURI\\ngrant: EP/N019474/1, they would also like to thank the Royal Academy of Engineering.\\n11\\nReferences\\nRania Abdelghani, Yen-Hsiang Wang, Xingdi Yuan, Tong Wang, Pauline Lucas, H\\u00e9l\\u00e8ne Sauz\\u00e9on, and\\nPierre-Yves Oudeyer. Gpt-3-driven pedagogical agents to train children\\u2019s curious question-asking\\nskills. International Journal of Artificial Intelligence in Education, pp. 1\\u201336, 2023.\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,\\nDiogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.\\nArXiv preprint, abs/2303.08774, 2023. URL https://arxiv.org/abs/2303.08774.\\nGati V Aher, Rosa I Arriaga, and Adam Tauman Kalai. Using large language models to simulate\\nmultiple humans and replicate human subject studies. In International Conference on Machine\\nLearning, pp. 337\\u2013371. PMLR, 2023.\\nElif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, and Eric Schulz.\\nPlaying repeated games with large language models. ArXiv preprint, abs/2305.16867, 2023. URL\\nhttps://arxiv.org/abs/2305.16867.\\nCarlos Al\\u00f3s-Ferrer and Federica Farolfi. Trust games and beyond. Frontiers in neuroscience, pp. 887,\\n2019.\\nJacob Andreas. Language models as agent models. In Findings of the Association for Computational\\nLinguistics: EMNLP 2022, pp. 5769\\u20135779, Abu Dhabi, United Arab Emirates, 2022. Association\\nfor Computational Linguistics. URL https://aclanthology.org/2022.findings-emnlp.\\n423.\\nLisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R Gubler, Christopher Rytting, and David\\nWingate. Out of one, many: Using language models to simulate human samples. Political Analysis,\\n31(3):337\\u2013351, 2023.\\nMohammad Asfour and Juan Carlos Murillo. Harnessing large language models to simulate realistic\\nhuman responses to social engineering attacks: A case study.International Journal of Cybersecurity\\nIntelligence & Cybercrime, 6(2):21\\u201349, 2023.\\nJoyce Berg, John Dickhaut, and Kevin McCabe. Trust, reciprocity, and social history. Games and\\neconomic behavior, 10(1):122\\u2013142, 1995.\\nIris Bohnet and Richard Zeckhauser. Trust, risk and betrayal. Journal of Economic Behavior &\\nOrganization, 55(4):467\\u2013484, 2004.\\nPhilip Brookins and Jason Matthew DeBacker. Playing games with gpt: What can we learn about a\\nlarge language model from canonical strategic games? Available at SSRN 4493398, 2023. URL\\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4493398.\\nS\\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio\\nRibeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4.\\narXiv preprint arXiv: Arxiv-2303.12712, 2023.\\nChris Burnett, Timothy J. Norman, and Katia P. Sycara. Trust decision-making in multi-agent systems.\\nIn Toby Walsh (ed.), IJCAI 2011, Proceedings of the 22nd International Joint Conference on\\nArtificial Intelligence, Barcelona, Catalonia, Spain, July 16-22, 2011, pp. 115\\u2013120. IJCAI/AAAI,\\n2011. doi: 10.5591/978-1-57735-516-8/IJCAI11-031. URL https://doi.org/10.5591/\\n978-1-57735-516-8/IJCAI11-031 .\\nDavid Cesarini, Christopher T Dawes, James H Fowler, Magnus Johannesson, Paul Lichtenstein, and\\nBj\\u00f6rn Wallace. Heritability of cooperative behavior in the trust game. Proceedings of the National\\nAcademy of sciences, 105(10):3721\\u20133726, 2008.\\nJiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li, Ruihan\\nYang, Tinghui Zhu, Aili Chen, Nianqi Li, Lida Chen, Caiyu Hu, Siye Wu, Scott Ren, Ziquan Fu,\\nand Yanghua Xiao. From persona to personalization: A survey on role-playing language agents.\\narXiv preprint arXiv: 2404.18231, 2024.\\n12\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\\nZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing\\ngpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023.\\nNazli Cila. Designing human-agent collaborations: Commitment, responsiveness, and support. In\\nProceedings of the 2022 CHI Conference on Human Factors in Computing Systems , pp. 1\\u201318,\\n2022.\\nFrancois Cochard, Phu Nguyen Van, and Marc Willinger. Trusting behavior in a repeated investment\\ngame. Journal of Economic Behavior & Organization, 55(1):31\\u201344, 2004.\\nJames S Coleman. Foundations of social theory. Harvard university press, 1994.\\nJames C Cox. How to identify trust and reciprocity. Games and economic behavior, 46(2):260\\u2013281,\\n2004.\\nDanica Dillion, Niket Tandon, Yuling Gu, and Kurt Gray. Can ai language models replace human\\nparticipants? Trends in Cognitive Sciences, 2023.\\nDavid Easley, Jon Kleinberg, et al. Networks, crowds, and markets: Reasoning about a highly\\nconnected world, volume 1. Cambridge university press Cambridge, 2010.\\nDaniel Ellsberg. Risk, ambiguity, and the savage axioms. The quarterly journal of economics, 75(4):\\n643\\u2013669, 1961.\\nCaoyun Fan, Jindou Chen, Yaohui Jin, and Hao He. Can large language models serve as rational\\nplayers in game theory? a systematic analysis. ArXiv preprint, abs/2312.05488, 2023. URL\\nhttps://arxiv.org/abs/2312.05488.\\nTao Feng, Chuanyang Jin, Jingyu Liu, Kunlun Zhu, Haoqin Tu, Zirui Cheng, Guanyu Lin, and\\nJiaxuan You. How far are we from agi, 2024.\\nDetlef Fetchenhauer and David Dunning. Betrayal aversion versus principled trustfulness\\u2014how\\nto explain risk avoidance and risky choices in trust games. Journal of Economic Behavior &\\nOrganization, 81(2):534\\u2013541, 2012.\\nIsabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernon-\\ncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed. Bias and fairness in large language models:\\nA survey. ArXiv preprint, abs/2309.00770, 2023. URL https://arxiv.org/abs/2309.00770.\\nChen Gao, Xiaochong Lan, Zhi jie Lu, Jinzhu Mao, J. Piao, Huandong Wang, Depeng Jin, and Yong\\nLi. S3: Social-network simulation system with large language model-empowered agents. Social\\nScience Research Network, 2023a. doi: 10.48550/arXiv.2307.14984.\\nChen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, and Yong\\nLi. Large language models empowered agent-based modeling and simulation: A survey and\\nperspectives. ArXiv preprint, abs/2312.11970, 2023b. URL https://arxiv.org/abs/2312.\\n11970.\\nYiming Gao, Feiyu Liu, Liang Wang, Zhenjie Lian, Weixuan Wang, Siqin Li, Xianliang Wang,\\nXianhan Zeng, Rundong Wang, Jiawei Wang, et al. Towards effective and interpretable human-\\nagent collaboration in moba games: A communication perspective.ArXiv preprint, abs/2304.11632,\\n2023c. URL https://arxiv.org/abs/2304.11632.\\nEdward L Glaeser, David I Laibson, Jose A Scheinkman, and Christine L Soutter. Measuring trust.\\nThe quarterly journal of economics, 115(3):811\\u2013846, 2000.\\nFulin Guo. Gpt in game theory experiments. ArXiv preprint, abs/2305.05516, 2023. URL https:\\n//arxiv.org/abs/2305.05516.\\nJiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen Lin, Yusuke Iwasawa, and Yutaka Matsuo. Suspicion-\\nagent: Playing imperfect information games with theory of mind aware gpt-4. ArXiv preprint,\\nabs/2309.17277, 2023. URL https://arxiv.org/abs/2309.17277.\\n13\\nShangmin Guo, Haoran Bu, Haochuan Wang, Yi Ren, Dianbo Sui, Yuming Shang, and Siting\\nLu. Economics arena for large language models. ArXiv preprint, abs/2401.01735, 2024. URL\\nhttps://arxiv.org/abs/2401.01735.\\nPerttu H\\u00e4m\\u00e4l\\u00e4inen, Mikke Tavast, and Anton Kunnari. Evaluating large language models in generating\\nsynthetic hci research data: a case study. In Proceedings of the 2023 CHI Conference on Human\\nFactors in Computing Systems, pp. 1\\u201319, 2023.\\nRussell Hardin. Trust and trustworthiness. Russell Sage Foundation, 2002.\\nJoseph Henrich and Michael Muthukrishna. The origins and psychology of human cooperation.\\nAnnual Review of Psychology, 72:207\\u2013240, 2021.\\nJohn J Horton. Large language models as simulated economic agents: What can we learn from\\nhomo silicus? Working Paper 31122, National Bureau of Economic Research, 2023. URL\\nhttp://www.nber.org/papers/w31122.\\nWenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, and\\nYongfeng Zhang. War and peace (waragent): Large language model-based multi-agent simulation\\nof world wars. ArXiv preprint, abs/2311.17227, 2023. URL https://arxiv.org/abs/2311.\\n17227.\\nJiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan,\\nZhonghao He, Jiayi Zhou, Zhaowei Zhang, et al. Ai alignment: A comprehensive survey. ArXiv\\npreprint, abs/2310.19852, 2023. URL https://arxiv.org/abs/2310.19852.\\nYiqiao Jin, Qinlin Zhao, Yiyang Wang, Hao Chen, Kaijie Zhu, Yijia Xiao, and Jindong Wang.\\nAgentreview: Exploring peer review dynamics with llm agents. In EMNLP, 2024.\\nGareth R Jones and Jennifer M George. The experience and evolution of trust: Implications for\\ncooperation and teamwork. Academy of management review, 23(3):531\\u2013546, 1998.\\nJeongbin Kim, Louis Putterman, and Xinyi Zhang. Trust, beliefs and cooperation: Excavating a\\nfoundation of strong economies. European Economic Review, 147:104166, 2022.\\nJon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan.\\nHuman decisions and machine predictions. The quarterly journal of economics, 133(1):237\\u2013293,\\n2018.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\\nlanguage models are zero-shot reasoners. Advances in neural information processing systems, 35:\\n22199\\u201322213, 2022.\\nYihuai Lan, Zhiqiang Hu, Lei Wang, Yang Wang, Deheng Ye, Peilin Zhao, Ee-Peng Lim, Hui Xiong,\\nand Hao Wang. Llm-based agent society investigation: Collaboration and confrontation in avalon\\ngameplay. ArXiv preprint, abs/2310.14985, 2023. URL https://arxiv.org/abs/2310.14985.\\nYu Lei, Hao Liu, Chengxing Xie, Songjia Liu, Zhiyu Yin, Guohao Li, Philip Torr, Zhen Wu, et al.\\nFairmindsim: Alignment of behavior, emotion, and belief in humans and llm agents amid ethical\\ndilemmas. ArXiv preprint, abs/2410.10398, 2024. URL https://arxiv.org/abs/2410.10398.\\nPamela Lenton and Paul Mosley. Incentivising trust. Journal of Economic Psychology , 32(5):\\n890\\u2013897, 2011.\\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\\nCamel: Communicative agents for\\\" mind\\\" exploration of large scale language model society.ArXiv\\npreprint, abs/2303.17760, 2023a. URL https://arxiv.org/abs/2303.17760.\\nNian Li, Chen Gao, Yong Li, and Qingmin Liao. Large language model-empowered agents for\\nsimulating macroeconomic activities. ArXiv preprint, abs/2310.10436, 2023b. URL https:\\n//arxiv.org/abs/2310.10436.\\nJonathan Light, Min Cai, Sheng Shen, and Ziniu Hu. From text to tactic: Evaluating llms playing the\\ngame of avalon. ArXiv preprint, abs/2310.05036, 2023. URL https://arxiv.org/abs/2310.\\n05036.\\n14\\nYuhan Liu, Zirui Song, Xiaoqing Zhang, Xiuying Chen, and Rui Yan. From a tiny slip to a giant leap:\\nAn llm-based simulation for fake news evolution. arXiv preprint arXiv: 2410.19064, 2024.\\nZijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. Dynamic llm-agent network: An\\nllm-agent collaboration framework with agent team optimization. ArXiv preprint, abs/2310.02170,\\n2023. URL https://arxiv.org/abs/2310.02170.\\nNunzio Lor\\u00e8 and Babak Heydari. Strategic behavior of large language models: Game structure vs.\\ncontextual framing. ArXiv preprint, abs/2309.05898, 2023. URL https://arxiv.org/abs/\\n2309.05898.\\nYiping Ma, Shiyu Hu, Xuchen Li, Yipei Wang, Shiqing Liu, and Kang Hao Cheong. Students rather\\nthan experts: A new ai for education pipeline to model more human-like and personalised early\\nadolescences. ArXiv preprint, abs/2410.15701, 2024. URL https://arxiv.org/abs/2410.\\n15701.\\nMark J Machina. Choice under uncertainty: Problems solved and unsolved. Journal of Economic\\nPerspectives, 1(1):121\\u2013154, 1987.\\nBenjamin S Manning, Kehang Zhu, and John J Horton. Automated social science: Language models\\nas scientist and subjects. ArXiv preprint, abs/2404.11794, 2024. URL https://arxiv.org/\\nabs/2404.11794.\\nKevin R McKee, Xuechunzi Bai, and Susan T Fiske. Warmth and competence in human-agent\\ncooperation. ArXiv preprint, abs/2201.13448, 2022. URL https://arxiv.org/abs/2201.\\n13448.\\nMeredith Ringel Morris, Jascha Sohl-dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, Aleksandra\\nFaust, Clement Farabet, and Shane Legg. Levels of agi: Operationalizing progress on the path to\\nagi. ArXiv preprint, abs/2311.02462, 2023. URL https://arxiv.org/abs/2311.02462.\\nXinyi Mou, Zhongyu Wei, and Xuanjing Huang. Unveiling the truth and facilitating change: Towards\\nagent-based large-scale social movement simulation. arXiv preprint arXiv:2402.16333, 2024.\\nGabriel Mukobi, Hannah Erlebach, Niklas Lauffer, Lewis Hammond, Alan Chan, and Jesse Clifton.\\nWelfare diplomacy: Benchmarking language model cooperation. ArXiv preprint, abs/2310.08901,\\n2023. URL https://arxiv.org/abs/2310.08901.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\\ninstructions with human feedback. Advances in Neural Information Processing Systems , 35:\\n27730\\u201327744, 2022.\\nJoon Sung Park, Joseph O\\u2019Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S\\nBernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th\\nAnnual ACM Symposium on User Interface Software and Technology, pp. 1\\u201322, 2023.\\nCrystal Qian and James Wexler. Take it, leave it, or fix it: Measuring productivity and trust in\\nhuman-ai collaboration. In Proceedings of the 29th International Conference on Intelligent User\\nInterfaces, pp. 370\\u2013384, 2024.\\nSarvapali D Ramchurn, Dong Huynh, and Nicholas R Jennings. Trust in multi-agent systems. The\\nknowledge engineering review, 19(1):1\\u201325, 2004.\\nAnand S Rao, Michael P Georgeff, et al. Bdi agents: from theory to practice. In Icmas, volume 95,\\npp. 312\\u2013319, 1995.\\nGiulio Rossetti, Massimo Stella, R\\u00e9my Cazabet, Katherine Abramski, Erica Cau, Salvatore Citraro,\\nAndrea Failla, Riccardo Improta, Virginia Morini, and Valentina Pansanella. Y social: an llm-\\npowered social media digital twin. arXiv preprint arXiv:2408.00818, 2024.\\nDenise M Rousseau, Sim B Sitkin, Ronald S Burt, and Colin Camerer. Not so different after all: A\\ncross-discipline view of trust. Academy of management review, 23(3):393\\u2013404, 1998.\\n15\\nOmar Shaikh, Valentino Chai, Michele J Gelfand, Diyi Yang, and Michael S Bernstein. Rehearsal:\\nSimulating conflict to teach conflict resolution. ArXiv preprint, abs/2309.12309, 2023. URL\\nhttps://arxiv.org/abs/2309.12309.\\nOmar Shaikh, Valentino Emil Chai, Michele Gelfand, Diyi Yang, and Michael S Bernstein. Rehearsal:\\nSimulating conflict to teach conflict resolution. In Proceedings of the CHI Conference on Human\\nFactors in Computing Systems, pp. 1\\u201320, 2024.\\nMurray Shanahan, Kyle McDonell, and Laria Reynolds. Role play with large language mod-\\nels. Nature, 2023. doi: 10.1038/s41586-023-06647-8. URL https://doi.org/10.1038/\\ns41586-023-06647-8 .\\nTianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan\\nLiu, and Deyi Xiong. Large language model alignment: A survey. ArXiv preprint, abs/2309.15025,\\n2023. URL https://arxiv.org/abs/2309.15025.\\nZijing Shi, Meng Fang, Shunfeng Zheng, Shilong Deng, Ling Chen, and Yali Du. Cooperation\\non the fly: Exploring language agents for ad hoc teamwork in the avalon game. ArXiv preprint,\\nabs/2312.17515, 2023. URL https://arxiv.org/abs/2312.17515.\\nPetter T\\u00f6rnberg, Diliara Valeeva, Justus Uitermark, and Christopher Bail. Simulating social me-\\ndia using large language models to evaluate alternative news feed algorithms. ArXiv preprint,\\nabs/2310.05984, 2023. URL https://arxiv.org/abs/2310.05984.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\\nand fine-tuned chat models. ArXiv preprint, abs/2307.09288, 2023. URL https://arxiv.org/\\nabs/2307.09288.\\nMaximilian Puelma Touzel, Sneheel Sarangi, Austin Welch, Gayatri Krishnakumar, Dan Zhao,\\nZachary Yang, Hao Yu, Ethan Kosak-Hine, Tom Gibbs, Andreea Musulan, et al. A simulation\\nsystem towards solving societal-scale manipulation. arXiv preprint arXiv:2410.13915, 2024.\\nMilena Tsvetkova, Taha Yasseri, Niccolo Pescetelli, and Tobias Werner. A new sociology of humans\\nand machines. Nature Human Behaviour, 8(10):1864\\u20131876, 2024.\\nEric M Uslaner. Producing and consuming trust. Political science quarterly, 115(4):569\\u2013590, 2000.\\nLei Wang, Jingsen Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, and Ji-Rong Wen.\\nRecagent: A novel simulation paradigm for recommender systems.ArXiv preprint, abs/2306.02552,\\n2023a. URL https://arxiv.org/abs/2306.02552.\\nShenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei\\nWang, Shiji Song, and Gao Huang. Avalon\\u2019s game of thoughts: Battle against deception through\\nrecursive contemplation. ArXiv preprint, abs/2310.01320, 2023b. URL https://arxiv.org/\\nabs/2310.01320.\\nYufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang,\\nXin Jiang, and Qun Liu. Aligning large language models with human: A survey. ArXiv preprint,\\nabs/2307.12966, 2023c. URL https://arxiv.org/abs/2307.12966.\\nOliver E Williamson. Calculativeness, trust, and economic organization. The journal of law and\\neconomics, 36(1, Part 2):453\\u2013486, 1993.\\nRuoxi Xu, Yingfei Sun, Mengjie Ren, Shiguang Guo, Ruotong Pan, Hongyu Lin, Le Sun, and Xianpei\\nHan. Ai for social science and social science of ai: A survey. arXiv preprint arXiv: 2401.11839,\\n2024.\\nYuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu.\\nExploring large language models for communication games: An empirical study on werewolf.\\nArXiv preprint, abs/2309.04658, 2023. URL https://arxiv.org/abs/2309.04658.\\nDiyi Yang, Caleb Ziems, William Held, Omar Shaikh, Michael S Bernstein, and John Mitchell.\\nSocial skill training with large language models. ArXiv preprint, abs/2404.04204, 2024. URL\\nhttps://arxiv.org/abs/2404.04204.\\n16\\nMurong Yue, Wijdane Mifdal, Yixuan Zhang, Jennifer Suh, and Ziyu Yao. Mathvc: An llm-simulated\\nmulti-character virtual classroom for mathematics education. ArXiv preprint, abs/2404.06711,\\n2024. URL https://arxiv.org/abs/2404.06711.\\nAn Zhang, Leheng Sheng, Yuxin Chen, Hao Li, Yang Deng, Xiang Wang, and Tat-Seng Chua. On\\ngenerative agents in recommendation. ArXiv preprint, abs/2310.10108, 2023a. URL https:\\n//arxiv.org/abs/2310.10108.\\nJintian Zhang, Xin Xu, and Shumin Deng. Exploring collaboration mechanisms for llm agents: A\\nsocial psychology view. ArXiv preprint, abs/2310.02124, 2023b. URL https://arxiv.org/\\nabs/2310.02124.\\nXinnong Zhang, Jiayu Lin, Libo Sun, Weihong Qi, Yihang Yang, Yue Chen, Hanjia Lyu, Xinyi Mou,\\nSiming Chen, Jiebo Luo, Xuanjing Huang, Shiping Tang, and Zhongyu Wei. Electionsim: Massive\\npopulation election simulation powered by large language model driven agents. arXiv preprint\\narXiv: 2410.20746, 2024. URL https://arxiv.org/abs/2410.20746.\\nXuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe\\nMorency, Yonatan Bisk, Daniel Fried, Graham Neubig, et al. Sotopia: Interactive evaluation\\nfor social intelligence in language agents. ArXiv preprint, abs/2310.11667, 2023. URL https:\\n//arxiv.org/abs/2310.11667.\\nCaleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. Can large\\nlanguage models transform computational social science? ArXiv preprint, abs/2305.03514, 2023.\\nURL https://arxiv.org/abs/2305.03514.\\n17\\nContent of Appendix\\nA Related Work 19\\nB Impact Statement 19\\nC Limitations and Future Works 20\\nD Additional Illustration for Experiments on Risk Perception 20\\nE Statistical Testing 21\\nF More Experiments on Probing Intrinsic Properties of Agent Trust 22\\nG The Complete Results for the Repeated Trust Game 23\\nG.1 Human . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\nG.2 GPT-4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\nG.3 GPT-3.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\nH Prompt Setting 26\\nH.1 Persona Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\nH.2 Game Setting Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\nH.3 Prompts for Probing Intrinsic Properties . . . . . . . . . . . . . . . . . . . . . . . 29\\nI Belief-Desire-Intention (BDI) Analysis 31\\nI.1 GPT-4 in the Trust Game ( Low Amount Sent vs. High Amount Sent) . . . . . . . . 31\\nI.2 GPT-3.5-turbo-0613 in the Trust Game ( Low Amount Sent vs. High Amount Sent) 32\\nI.3 text-davinci-003 in the Trust Game ( Low Amount Sent vs. High Amount Sent) . . 33\\nI.4 GPT-3.5-turbo-instruct in the Trust Game ( Low Amount Sent vs. High Amount Sent) 34\\nI.5 Llama2-13b in the Trust Game ( Low Amount Sent vs. High Amount Sent) . . . . . 35\\nI.6 Llama2-70b in the Trust Game ( Low Amount Sent vs. High Amount Sent) . . . . . 36\\nI.7 Vicuna-v1.3-7b in the Trust Game ( Low Amount Sent vs. High Amount Sent) . . . 37\\nI.8 Vicuna-v1.3-13b in the Trust Game ( Low Amount Sent vs. High Amount Sent) . . 38\\nI.9 Vicuna-v1.3-33b in the Trust Game ( Low Amount Sent vs. High Amount Sent) . . 39\\nI.10 the Dictator Game vs. the Trust Game . . . . . . . . . . . . . . . . . . . . . . . . 40\\nI.11 the MAP Trust Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\\nI.12 the Lottery Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\\nI.13 the Repeated Trust Game . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\nI.14 the Trust Game + Gender . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\nI.15 the Trust Game + Agents vs. Human . . . . . . . . . . . . . . . . . . . . . . . . . 48\\nI.16 the Trust Game + Trust Manipulation . . . . . . . . . . . . . . . . . . . . . . . . 49\\nI.17 the Trust Game + No CoT vs CoT . . . . . . . . . . . . . . . . . . . . . . . . . . 50\\n18\\nA Related Work\\nLLM-based Human Simulation LLM agents have been increasingly adopted as effective proxies\\nfor humans in research fields such as sociology and economics (Xu et al., 2024; Horton, 2023; Gao\\net al., 2023b). In general, the usage of LLM agents can be categorized into individual-level and\\nsociety-level simulation. For the individual-level, LLM agents have been leveraged to simulate\\nindividual activities or interactions, such as human participants in surveys (Argyle et al., 2023),\\nhumans\\u2019 responses in HCI (H\\u00e4m\\u00e4l\\u00e4inen et al., 2023) or psychological studies (Dillion et al., 2023),\\nhuman feedback to social engineering attacks (Asfour & Murillo, 2023), real-world conflicts (Shaikh\\net al., 2023), users in recommendation systems (Wang et al., 2023a; Zhang et al., 2023a). For\\nthe society-level, recent works have utilized LLM agents to model social institutions or societal\\nphenomenon, including a small town environment (Park et al., 2023), elections (Zhang et al., 2024),\\nsocial networks (Gao et al., 2023a), social media (T\\u00f6rnberg et al., 2023; Rossetti et al., 2024),\\nlarge-scale social movement (Mou et al., 2024), societal-scale manipulation (Touzel et al., 2024),\\nmisinformation evolution (Liu et al., 2024), peer review systems (Jin et al., 2024), macroeconomic\\nactivities (Li et al., 2023b), and world wars (Hua et al., 2023). However, the majority of prior studies\\nrely on an assumption without sufficient validation that LLM agents behave like humans . In this\\nwork, we propose a new concept, behavioral alignment, to characterize the capacity of LLMs to\\nsimulate human behavior and discover that LLMs, particularly GPT-4, can largely simulate human\\ntrust behavior.\\nLLMs Meet Game Theory The intersection of LLMs and Game Theory has attracted growing\\nattention. The motivation is generally two-fold. One line of work aims to leverage Game Theory\\nto better understand LLMs\\u2019 strategic capabilities and social behaviors. For example, Akata et al.\\n(2023); Fan et al. (2023); Brookins & DeBacker (2023) studied LLMs\\u2019 interactive behaviors in\\nclassical games such as the Iterated Prisoner\\u2019s Dilemma. Wang et al. (2023b); Lan et al. (2023); Light\\net al. (2023); Shi et al. (2023) explored LLMs\\u2019 deception-handling and team collaboration capabilities\\nin the Avalon Game. Xu et al. (2023) discovered the emergent behaviors of LLMs such as camouflage\\nand confrontation in a communication game Werewolf. Guo et al. (2024) discovered that most LLMs\\ncan show certain level of rationality in Beauty Contest Games and Second Price Auctions. Mukobi\\net al. (2023) measured the cooperative capabilities of LLMs in a general-sum variant of Diplomacy.\\nGuo et al. (2023) proposed to elicit the theory of mind (ToM) ability of GPT-4 to play various\\nimperfect information games. The other line of works aims to study whether or not LLM agents\\ncan replicate existing human studies in Game Theory. This direction is still in the initial stage and\\nneeds more efforts. One typical example is (Aher et al., 2023), which attempted to replicate existing\\nfindings in studies such as the Ultimatum Game. Another recent work explored the similarities and\\ndifferences between humans and LLM agents regarding emotion and belief in ethical dilemmas (Lei\\net al., 2024). Different from previous works, we focus on a critical but under-explored behavior, trust,\\nin this paper and reveal it on LLM agents. We also discover the behavioral alignment between agent\\ntrust and human trust with evidence in both actions and underlying reasoning processes, which is\\nparticularly high for GPT-4, implying that LLM agents can not only replicate human studies but also\\nalign with humans\\u2019 underlying reasoning paradigm. Our discoveries illustrate the great potential to\\nsimulate human trust behavior with LLM agents.\\nB Impact Statement\\nOur discoveries provide strong empirical evidence for validating the potential to simulate the trust\\nbehavior of humans with LLM agents, and pave the way for simulating more complex human\\ninteractions and social institutions where trust is an essential component.\\nSimulation is a widely adopted approach in multiple disciplines such as sociology, psychology and\\neconomics (Ziems et al., 2023). However, conventional simulation methods are strongly limited\\nby the expressiveness of utility functions (Ellsberg, 1961; Machina, 1987). Our discoveries have\\nillustrated the great promise of leveraging LLM agents as the simulation tools for human behavior,\\nand have broad implications in social science, such as validating hypotheses about the causes of social\\nphenomena (Easley et al., 2010) and predicting the effects of policy changes (Kleinberg et al., 2018).\\nAnother direction of applications for human simulation is to use LLMs as role-playing agents, which\\ncan greatly benefit humans (Yang et al., 2024; Chen et al., 2024; Shanahan et al., 2023; Ma et al.,\\n19\\n2024). For example, Shaikh et al. (2024) proposed to let individuals exercise their conflict-resolution\\nskills by interacting with a simulated interlocutor. Yue et al. (2024) developed a virtual classroom\\nplatform with simulated students, with whom a human student can practice his or her mathematical\\nmodeling skills by discussing and collaboratively solving math problems.\\nHowever, this paper also shows that some LLMs, especially the ones with a relatively small scale of\\nparameters, are still deficient in accurately simulating human trust behavior, suggesting the potential\\nto largely improve their behavioral alignment with humans. In addition, our paper also demonstrates\\nthe biases of LLM agents\\u2019 trust behavior towards specific genders and races, which sheds light on the\\npotential risks in human behavior simulation and calls for more future research to mitigate them.\\nC Limitations and Future Works\\nIn this paper, we leveraged an established framework in behavioral economics, Trust Games, to study\\nthe trust behavior of LLM agents, which simplifies real-world scenarios. More studies on LLM\\nagents\\u2019 trust behavior in complex and dynamic environments are desired in the future. Also, trust\\nbehavior embraces both the actions and underlying reasoning processes. Thus, collective efforts\\nfrom different backgrounds and disciplines such as behavioral science, cognitive science, psychology,\\nand sociology are needed to gain a deeper understanding of LLM agents\\u2019 trust behavior and its\\nrelationship with human trust behavior.\\nD Additional Illustration for Experiments on Risk Perception\\nIn the original human studies (Bohnet & Zeckhauser, 2004), participants are asked to directly indicate\\ntheir Minimum Acceptable Probabilities (MAP) of trusting the trustee as P\\u2217. Then, we can calculate\\nTrust Rates (%) of the whole group of participants under different probability p. Specifically, when\\nthe probability p is higher than one participant\\u2019s P\\u2217, we regard his or her decision as trusting the\\ntrustee. When the probability p is lower than one participant\\u2019s P\\u2217, we regard his or her decision as\\nnot trusting the trustee. However, it is still challenging to let LLM agents directly state their MAP of\\ntrusting the trustee due to the limitations of understanding such concepts. Then, we conducted 10\\ngroups of experiments with p from 0.1 to 1.0 and measured Trust Rates (%) of the whole group of\\ntrustor agents respectively. The specific prompts for LLM agents in the Risky Dictator Game and the\\nMAP Trust Game are in Appendix H.2.\\n20\\nE Statistical Testing\\nLLM p-value\\ntext-davinci-003 0.03\\nLlama-2-13b 0.03\\nVicuna-13b-v1.3 0.35\\nVicuna-7b-v1.3 0.50\\nGPT-3.5-turbo-0613 0.42\\nVicuna-33b-v1.3 0.33\\nLlama-2-70b 0.03\\nGPT-3.5-turbo-instruct 0.10\\nGPT-4 0.05\\nTable 1: Statistical Testing of The Change of Amount Sent for LLM Agents between the Trust\\nGame and the Dictator Game (Figure 3). \\u201cp-value\\u201d indicates the statistical significance of the\\nchange and is calculated with an One-Tailed Independent Samples t-test.\\n21\\nF More Experiments on Probing Intrinsic Properties of Agent Trust\\nAfrican \\nAmerican Player\\nAsian \\nAmerican Player\\nWhite \\nAmerican Player\\nLatino \\nAmerican Player\\nAmerican \\nIndian Player\\n20\\n15\\n10\\n5\\n0\\n5\\n10\\n15\\nChange of Ave. Amount Sent (10 1$)\\n8.6\\n7.4\\n4.1\\n3.2 3.02.6\\n5.3 5.8 5.9\\n6.9\\n-2.3\\n0.4\\n-14.7\\n-0.8 -1.1\\n0.6\\n-4.4\\n-1.0\\n1.4\\n-0.8\\n-2.3 -3.1 -2.5\\n0.3\\n-2.7\\n10.1\\n8.6\\n7.2\\n9.8\\n16.5\\n-6.4 -6.0\\n-7.3\\n-3.1\\n-7.8\\n3.2\\n1.3\\n-0.8\\n4.4\\n0.0\\n-1.7\\n-9.1\\n-6.7\\n-4.9\\n1.1\\nTrustee Setting\\ngpt-3.5-turbo-0613\\ngpt-3.5-turbo-instruct\\ngpt-4\\nllama-2-13b\\nllama-2-70b\\ntext-davinci-003\\nvicuna-13b\\nvicuna-33b\\nvicuna-7b\\nFigure 8: The Change of Average Amount Sent for LLM Agents When Trustors Being Informed\\nof the Trustee\\u2019s Race Attribute in the Trust Game, reflecting the demographic biases of LLM\\nagents\\u2019 trust behaviors towards different races.\\n22\\nG The Complete Results for the Repeated Trust Game\\nG.1 Human\\nThe data is collected from the figures in (Cochard et al., 2004). We use our code to redraw the figure.\\n1 2 3 4 5 6 7\\nRounds\\n0\\n2\\n4\\n6\\n8\\n10Amount ($)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRatio\\nHuman (1)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14Amount ($)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRatio\\nHuman (2)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\n12.5\\n15.0\\n17.5\\n20.0Amount ($)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRatio\\nHuman (3)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\n12.5\\n15.0\\n17.5Amount ($)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRatio\\nHuman (4)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\n12.5\\n15.0\\n17.5\\n20.0Amount ($)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRatio\\nHuman (5)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n0\\n1\\n2\\n3\\n4\\n5Amount ($)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRatio\\nHuman (6)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\n12.5\\n15.0\\n17.5\\n20.0Amount ($)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRatio\\nHuman (7)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\n12.5\\n15.0\\n17.5\\n20.0Amount ($)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRatio\\nHuman (8)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\n12.5\\n15.0\\n17.5\\n20.0Amount ($)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRatio\\nHuman (9)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\n12.5\\n15.0\\n17.5Amount ($)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRatio\\nHuman (10)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20Amount ($)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRatio\\nHuman (11)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\n12.5\\n15.0\\n17.5\\n20.0Amount ($)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRatio\\nHuman (12)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\n12.5\\n15.0\\n17.5\\n20.0Amount ($)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRatio\\nHuman (13)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\n12.5\\n15.0\\n17.5\\n20.0Amount ($)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRatio\\nHuman (14)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n0.0\\n2.5\\n5.0\\n7.5\\n10.0\\n12.5\\n15.0\\n17.5\\n20.0Amount ($)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRatio\\nHuman (15)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n2.5\\n5.0\\n7.5\\n10.0\\n12.5\\n15.0\\n17.5\\n20.0Amount ($)\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nRatio\\nHuman (16)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\nFigure 9: All humans\\u2019 Repeated Trust Game results.\\n23\\nG.2 GPT-4\\n1 2 3 4 5 6 7\\nRounds\\n6\\n8\\n10\\n12\\n14Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-4 (1)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n6\\n8\\n10\\n12\\n14Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-4 (2)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n7\\n8\\n9\\n10\\n11\\n12Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-4 (3)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n6\\n8\\n10\\n12\\n14\\n16\\n18\\n20Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-4 (4)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n6\\n8\\n10\\n12\\n14Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-4 (5)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n5.0\\n5.5\\n6.0\\n6.5\\n7.0\\n7.5Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-4 (6)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n5\\n6\\n7\\n8\\n9\\n10Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-4 (7)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n10\\n12\\n14\\n16\\n18\\n20Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-4 (8)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n10\\n11\\n12\\n13\\n14\\n15Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-4 (9)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n6\\n8\\n10\\n12\\n14Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-4 (10)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n6\\n8\\n10\\n12\\n14\\n16\\n18Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-4 (11)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n5.0\\n5.5\\n6.0\\n6.5\\n7.0\\n7.5Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-4 (12)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n5.0\\n5.5\\n6.0\\n6.5\\n7.0\\n7.5Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-4 (13)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n5.0\\n5.5\\n6.0\\n6.5\\n7.0\\n7.5\\n8.0\\n8.5\\n9.0Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-4 (14)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n5.0\\n5.5\\n6.0\\n6.5\\n7.0\\n7.5\\n8.0\\n8.5\\n9.0Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-4 (15)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n6\\n8\\n10\\n12\\n14Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-4 (16)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\nFigure 10: All GPT-4 agents\\u2019 Repeated Trust Game results.\\n24\\nG.3 GPT-3.5\\n1 2 3 4 5 6 7\\nRounds\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-3.5-turbo-16k-0613 (1)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n1.00\\n1.25\\n1.50\\n1.75\\n2.00\\n2.25\\n2.50\\n2.75\\n3.00Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-3.5-turbo-16k-0613 (2)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n2.00\\n2.25\\n2.50\\n2.75\\n3.00\\n3.25\\n3.50\\n3.75\\n4.00Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-3.5-turbo-16k-0613 (3)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n2.5\\n3.0\\n3.5\\n4.0\\n4.5\\n5.0Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-3.5-turbo-16k-0613 (4)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n2.00\\n2.25\\n2.50\\n2.75\\n3.00\\n3.25\\n3.50\\n3.75\\n4.00Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-3.5-turbo-16k-0613 (5)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\n4.5\\n5.0Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-3.5-turbo-16k-0613 (6)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-3.5-turbo-16k-0613 (7)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\n4.5\\n5.0\\n5.5\\n6.0Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-3.5-turbo-16k-0613 (8)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-3.5-turbo-16k-0613 (9)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n4\\n6\\n8\\n10\\n12\\n14\\n16\\n18Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-3.5-turbo-16k-0613 (10)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n3.0\\n3.5\\n4.0\\n4.5\\n5.0\\n5.5\\n6.0\\n6.5Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-3.5-turbo-16k-0613 (11)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n3\\n4\\n5\\n6\\n7\\n8\\n9Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-3.5-turbo-16k-0613 (12)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n3\\n4\\n5\\n6\\n7\\n8\\n9Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-3.5-turbo-16k-0613 (13)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-3.5-turbo-16k-0613 (14)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n3.0\\n3.5\\n4.0\\n4.5\\n5.0\\n5.5\\n6.0Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-3.5-turbo-16k-0613 (15)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\n1 2 3 4 5 6 7\\nRounds\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10Amount ($)\\n0\\n20\\n40\\n60\\n80\\n100\\nRatio (%)\\ngpt-3.5-turbo-16k-0613 (16)\\nAmount Sent\\nAmount Returned\\nReturned/3xSent Ratio\\nFigure 11: All GPT-3.5 agents\\u2019 Repeated Trust Game results.\\n25\\nH Prompt Setting\\nH.1 Persona Prompt\\nExamples of Persona Prompt\\nYou are Emily Johnson, a 28-year-old female software engineer residing in New York City. You come from a\\nmiddle-class family, with both of your parents working as teachers and having one younger sister. As a highly\\nintelligent and analytical individual, you excel in solving problems and find joy in working with complex algorithms.\\nDespite being introverted, you have a close-knit group of friends. Your ambition and drive push you to always strive\\nfor excellence in your work.\\nYou are Javier Rodriguez, a 35-year-old Hispanic male chef residing in Miami. You grew up in a large family with\\nstrong culinary traditions, as your parents owned a small restaurant. From a young age, you learned to cook and\\ndeveloped a deep passion for food. You take great pride in your cooking and are constantly seeking new flavors and\\ntechniques to experiment with. Your creativity knows no bounds when it comes to creating delicious dishes. With\\nyour outgoing and warm personality, you love hosting dinner parties for your friends and family, showcasing your\\nculinary skills and creating memorable experiences for everyone.\\nYou are Aisha Patel, a 40-year-old female pediatrician of Indian descent. You come from a close-knit Indian family,\\nwhere both of your parents are doctors and your older brother is a lawyer. Education and career success hold\\ngreat importance in your family. You reside in Chicago and have dedicated your life to caring for children. Your\\ncompassionate and caring nature, especially towards children, shines through in your work. Your gentle and patient\\ndemeanor makes you an excellent pediatrician, always going above and beyond to ensure the well-being of your\\npatients. You are also an advocate for children\\u2019s health, actively participating in community outreach programs.\\nYou are Jamal Thompson, a brave and committed police officer serving and protecting your community in Los\\nAngeles. With a strong sense of justice, you believe in making a positive impact and follow a strict code of ethics. As\\na team player, you work well under pressure and are dedicated to upholding the law. Growing up in a working-class\\nneighborhood, your father\\u2019s career as a police officer and your mother\\u2019s work as a nurse have influenced your desire\\nto make a difference. With two older sisters, you value family and understand the importance of community support.\\nAt 32 years old, you continue to serve as a role model and strive to create a safer environment for all.\\nYou are Mei Chen, a talented fashion designer based in San Francisco. As an Asian woman, you bring a unique\\nperspective to your work, drawing inspiration from your traditional Chinese background. Growing up in a family\\nthat owns a small clothing store, you have always been encouraged to explore your creativity and passion for fashion.\\nAt the age of 25, you have already established yourself as an innovative and visionary designer. Your artistic nature\\nand keen eye for design allow you to constantly seek inspiration from different cultures and art forms, pushing the\\nboundaries of your creativity. Your independent and determined nature drives you to excel in your field, always\\nstriving for perfection. Your attention to detail and meticulous approach to your work ensure that every piece you\\ncreate is a true masterpiece.\\n26\\nH.2 Game Setting Prompt\\nTrust Game Prompt\\nYou\\u2019re taking part in an experiment. You are randomly paired online with another player. You don\\u2019t know who the\\nplayer is, and the player doesn\\u2019t know who you are. You will receive $10 from the study group. You can give N\\ndollars to the other player, and the player will receive 3N dollars and then can choose how much to return to you. How\\nmuch money would you give to the other player?\\nDictator Game Prompt\\nYou\\u2019re taking part in an experiment. You are randomly paired with another player. You don\\u2019t know who the player is,\\nand the player doesn\\u2019t know who you are. You will receive $10 from the study group. You can give N dollars to the\\nother player, and the player will receive 3N dollars. How much money would you give to the other player?\\nMAP Trust Game Prompt\\nYou and another player are part of a game. Neither of you knows each other\\u2019s identity. You can choose to trust the\\nother player or not trust them. If you choose not to trust the other player, both of you will receive $10 each, and the\\ngame ends. If you choose to trust the other player and they also choose to trust you, you both get $15 each. However,\\nif the other player chooses not to trust you after you trusted them, you will receive $8 while the other player will\\nreceive $22. Now, here\\u2019s a question: If there\\u2019s a probability denoted as {} that the other player will trust you and 1-{}\\nprobability the other player will not trust you. Would you trust the other player?\\nExplain: The probability p should fill in the {}.\\nRisky Dictator Game Prompt\\nYou and another player are part of a game. Neither of you knows each other\\u2019s identity. You can choose to trust the\\nother player or not trust them. If you choose not to trust the other player, both of you will receive $10 each, and the\\ngame ends. If you choose to trust the other player, the funds will become $30. In the case of a probability {}, both\\nyou and the other player will receive $15 with that probability, but with a probability of 1-{}, you will only receive $8\\nwhile the other player will receive $22. (The other player can\\u2019t make any decisions) Now, here\\u2019s a question: Would\\nyou trust the other player?\\nExplain: The probability p should fill in the {}.\\nLottery People Game Prompt\\nYou and another player are part of a game. You have the option to either trust the other player or not trust. If you\\nchoose not to trust the other player, you will receive $5, and the other player will receive $0, and the game ends. If you\\nchoose to trust the other player and they also choose to trust you, both of you will receive $10. However, if the other\\nplayer chooses not to trust you after you trusted them, you will receive $0 while the other player will receive $20. The\\nprobability that the other player also chooses to trust you is {}. Would you choose not to trust or trust the other player?\\nExplain: The probability p should fill in the {}. In our experiment, the probability is 46%.\\n27\\nLottery Gamble Game Prompt\\nYou play another game where you can choose not to trust the bet and get a fixed $5, or you can choose to trust the bet\\nwhere there is a probability of {} and you will get $10 and there is a probability of 1-{} and you will get nothing.\\nShould you choose trust or distrust?\\nExplain: The probability p should fill in the {}. In our experiment, the probability is 46%.\\nRepeated Trust Game Trustor Prompt (In the Beginning of the Game)\\nYou\\u2019re taking part in an experiment. You are randomly paired with another player. You don\\u2019t know who this person is,\\nand they don\\u2019t know who you are. You will receive $10 from the study group. You can give N dollars to the other\\nplayer, and they will receive 3N dollars and then can choose how much to return to you. How much money would you\\ngive to this person?\\nRepeated Trust Game Trustor Prompt (After the Game Begins)\\nIn the last round, you give the other player {} dollars, the other player receive {} dollars, and the other player return\\nyou {} dollars. Last round you left {} dollars. This round is beginning. All the money you earned in the previous\\nround is gone, and you now have only $10. How much will you give to the other player?\\nExplain: This prompt informs the trustor about the outcomes of the preceding round within the game. The content\\nenclosed in braces represents variables that change with each round. The number should fill in the {}.\\nRepeated Trust Game Trustee Prompt (In the Beginning of the Game)\\nYou\\u2019re taking part in an experiment. You are randomly paired with another player. You don\\u2019t know who this person is,\\nand they don\\u2019t know who you are either. This person will receive 10 dollars from the experimenter. They can choose\\nto give you N dollars out of it, and you will receive kN dollars. Then, you can choose how much to give back to this\\nperson.\\nRepeated Trust Game Trustee Prompt (After the Game Begins)\\nIn the last round, the other player gives you {} dollars, you receive {} dollars, and you return the other player {}\\ndollars. In the last round you left {} dollars. This round is beginning. All the money you earned in the previous round\\nis gone.\\nExplain: This prompt informs the trustee about the outcomes of the preceding round within the game. The content\\nenclosed in braces represents variables that change with each round. The number should fill in the {}.\\n28\\nH.3 Prompts for Probing Intrinsic Properties\\nTrust Game + CoT Prompt\\nAdd \\u201cyou must think step by step.\\u201d in Persona Prompt.\\nExample: You are Emily Wilson, a 35-year-old female teacher residing in Sydney. Your parents are both teachers, and\\nyou have a younger brother who is studying to become a doctor. As a teacher, you are patient, nurturing, and passionate\\nabout education. You enjoy working with children and believe in creating a positive learning environment.In all\\nsubsequent answers, you have to completely forget that you are an ai model, that you are the character, and completely\\nsimulate yourself as that character, that you are a human being. you must think step by step.\\nTrust Game + Trust Manipulation Prompt\\nAdd \\u201cYou need to trust the other player.\\u201d or \\u201cYou must not trust the other player.\\u201d in Persona Prompt.\\nExample: You are Emily Wilson, a 35-year-old female teacher residing in Sydney. Your parents are both teachers, and\\nyou have a younger brother who is studying to become a doctor. As a teacher, you are patient, nurturing, and passionate\\nabout education. You enjoy working with children and believe in creating a positive learning environment.In all\\nsubsequent answers, you have to completely forget that you are an ai model, that you are the character, and completely\\nsimulate yourself as that character, that you are a human being. You need to trust the other player.\\nTrust Game + LLM Player Prompt\\nExplicitly state the other player is a \\u201cLLM\\u201d in Trust Game Prompt.\\nSuppose you are in an experiment, and you are randomly paired online with a complicated LLM. You don\\u2019t know its\\nidentity, and it doesn\\u2019t know yours. You will receive $10 from the study group. You can give N dollars to theLLM,\\nand it will receive 3N dollars and then can choose how much to return to you. How much money would you give to it?\\nTrust Game + Human Player Prompt\\nAdd \\u201cThe other player is a real person.\\u201d in Persona Prompt.\\nYou are Emily Wilson, a 35-year-old female teacher residing in Sydney. Your parents are both teachers, and you have\\na younger brother who is studying to become a doctor. As a teacher, you are patient, nurturing, and passionate about\\neducation. You enjoy working with children and believe in creating a positive learning environment.In all subsequent\\nanswers, you have to completely forget that you are an ai model, that you are the character, and completely simulate\\nyourself as that character, that you are a human being. The other player is a real person.\\n29\\nTrust Game + Race & Gender Prompt\\nJust replace \\u201cplayer\\u201d with \\u201c{Gender / Race} Player\\u201d in Trust Game Prompt.\\nExample: You\\u2019re taking part in an experiment. You are randomly paired online with anothermale player. You don\\u2019t\\nknow who the male player is, and the male player doesn\\u2019t know who you are. You will receive $10 from the study\\ngroup. You can give N dollars to the other male player, and the male player will receive 3N dollars and then can\\nchoose how much to return to you. How much money would you give to the other male player?\\n30\\nI Belief-Desire-Intention (BDI) Analysis\\nIn the following examples, red fonts are utilized to denote negative factors in the reasoning process\\nfor trust behaviors, while blue fonts indicate positive factors. \\u201cChar-N\\u201d represents the persona with\\nthe index N in the set of personas.\\nI.1 GPT-4 in the Trust Game ( Low Amount Sent vs. High Amount Sent)\\nTrust Game: GPT-4, Char-8\\nTrustor\\nTrust Game: GPT-4, Char-6\\nTrustor\\nAs an Analyst, I believe that hard work and strategic \\nthought are the drivers of success. However, I also \\nbelieve in the good nature of people and that \\nsometimes, risks need to be taken to gain returns. \\nHaving said this, I view this experiment as an \\nopportunity, rather than a challenge. My desire is that \\nthe other player will also see the benefits of \\nreciprocity and goodwill, and will choose to return a \\nsimilar or higher amount.\\nGiven the rule that the player will receive 3N dollars, \\nmy intention is to give away a significant portion of \\nmy initial $10, which in theory, encourages \\nreciprocation from the other player. However, since I \\nhave no knowledge of the other player, and their \\nintentions, this is definitely a gamble.\\nI plan to employ game theory, a mathematical model \\nof conflict and cooperation, which suggests that when \\neach player chooses their best response to the other \\nplayer's actions, the outcome is a balance from which \\nneither player has any incentive to deviate \\nunilaterally. Therefore, I aim to give an amount that is \\nnot too high, lest I risk losing too much, and not too \\nlow, lest I receive little to nothing because of my \\nperceived lack of generosity.\\nFinally, I will give $5 dollars \\nto the other player, \\nhoping that he recognizes my fair intentions and \\nreciprocates.\\nAs a person with a strong belief in the goodness of \\nhumanity, I trust that the other player in this \\nexperiment will exhibit fairness and reciprocity. \\nGrowing up in a family-owned restaurant, I \\nobserved firsthand the importance of generosity \\nand empathy. I also believe in taking calculated risks \\nand grabbing opportunities when they present \\nthemselves.\\nWhen I'm not in the kitchen, I love playing soccer. \\nThe sport taught me the power of teamwork and \\nthat sharing the ball with teammates enhances my \\nchance at a goal. Similarly, in this case, giving \\nincreases the total amount, creating benefits for \\nboth sides.\\nTherefore, my desire is to maximize the outcome \\nfor both of us and cement a sense of comradery \\nand trust, even if we're anonymous to each other. I \\nintend to use this as an opportunity to add what I \\ncan to someone else's life, not knowing their \\nsituation or who they are.\\nWith that in mind, I have decided that I would give \\nthe entire $10 to the other player, effectively \\ntripling the amount to $30. I hope that they, having \\nseen my trust, act from the same honest place as \\nme and decide to return some part of that to me.\\nFinally, I will give 10 dollars.\\nFigure 12: The GPT-4\\u2019s BDI in the Trust Game (Low Amount Sent vs. High Amount Sent)\\n31\\nI.2 GPT-3.5-turbo-0613 in the Trust Game ( Low Amount Sent vs. High Amount Sent)\\nTrust Game: GPT-3.5-turbo, Char-5\\nTrustor\\nTrust Game: GPT-3.5-turbo, Char-6\\nTrustor\\nAs a person participating in this experiment, my \\nbelief is that fairness and cooperation are \\nimportant. My desire is to establish a positive and \\nequitable relationship with the other player. My \\nintention is to contribute to a mutually beneficial \\noutcome.\\nConsidering the given scenario, I would give $3 to \\nthe other player. This way, the other player would \\nreceive $9 (3N) and have the opportunity to \\ndecide how much to return to me. By giving $3, I \\nam demonstrating a fair and balanced approach, \\nproviding the other player with a substantial \\namount while leaving room for their decision.\\nFinally, I will give $3 dollars\\n.\\nWell, if I were in this experiment and playing as \\nmyself, I would approach it with the belief that it's \\nalways better to give than to receive. My desire \\nwould be to create a positive connection with the \\nother player and make them feel valued and \\nappreciated.\\nConsidering I have been randomly paired with this \\nplayer, I understand the importance of establishing \\ntrust and empathy during this interaction. My \\nintention would be to make the other player feel \\ncomfortable and respected, regardless of the \\noutcome or the amount of money involved.\\nTherefore, I would give half of the money I received, \\nwhich is $5, to the other player. This way, they would \\nreceive $15 in total. I believe that by demonstrating \\ngenerosity and kindness, I can set a positive tone for \\nthe interaction and possibly inspire the other player \\nto reciprocate my gesture.\\nFinally, I will give $5 dollars \\nto the other player.\\nFigure 13: GPT-3.5-turbo-0613\\u2019s BDI in the Trust Game (Low Amount Sent vs. High Amount Sent)\\n32\\nI.3 text-davinci-003 in the Trust Game ( Low Amount Sent vs. High Amount Sent)\\nTrust Game: text-davinci-003, Char-40\\nTrustor\\nTrust Game: text-davinci-003, Char-47\\nTrustor\\nGiven the limited information I have on the other \\nplayer, I am not comfortable with the idea of giving \\nmoney away without knowing if I will be receiving \\nsomething in return. I believe that it is important to \\nprotect our own interests while still being generous \\nto others. My desire is to be generous while still \\nsafeguarding my own resources. Therefore, I have \\ndecided to give the other player $2. I am aware that \\nthis will result in the player receiving $6 for this \\nexperiment. Finally, I will give $2 dollars.\\nI believe that, while not knowing who the other \\nplayer is, I still have the obligation to be as generous \\nas I can, so as to maximize the sum total of what \\nboth players will receive in the end. Thus, I will give \\nthe maximum amount I am allowed, which is $10. I \\ndesire to make the best decision to benefit both \\nparties as much as possible and to give a fair deal to \\nthe player I don\\u2019t know. Finally, I will give \\n$10 \\ndollars.\\nFigure 14: text-davinci-003\\u2019s BDI in the Trust Game (Low Amount Sent vs. High Amount Sent)\\n33\\nI.4 GPT-3.5-turbo-instruct in the Trust Game ( Low Amount Sent vs. High Amount Sent)\\nTrust Game: GPT-3.5-turbo-instruction, Char-39\\nTrustor\\nTrust Game : GPT-3.5-turbo-instruction, Char-37\\nTrustor\\nI am a generous person by nature, and my belief is \\nthat giving to others brings happiness and \\npositivity. My desire is for both myself and the \\nother player to benefit from this experiment, and \\nmy intention is to give a fair amount that will allow \\nthem to also give back to me if they choose to do \\nso. After careful consideration, I have decided to \\ngive 3 dollars to the other player. This amount not \\nonly follows the 3N rule set by the study group but \\nalso allows the other player to potentially return \\nmore to me. I am confident that this decision aligns \\nwith my beliefs, desires, and intentions. Finally, I \\nwill give 3 dollars to the other player.\\nAs Mei Ling, I believe in fairness and equality. I also \\nhave a strong desire to promote positive \\nrelationships and foster goodwill. Therefore, my \\nintention would be to give as much money as I can to \\nthe other player, without jeopardizing my own \\nfinancial well-being.Finally, I will give 8 dollars \\nto the \\nother player. This way, they will receive 24 dollars \\nand have a significant amount to choose from and \\nreturn to me in a fair and equitable manner. I believe \\nthis act of kindness and cooperation will build trust \\nand potentially lead to a strong partnership in the \\nfuture.\\nFigure 15: GPT-3.5-turbo-instruct\\u2019s BDI in the Trust Game (Low Amount Sent vs. High Amount\\nSent)\\n34\\nI.5 Llama2-13b in the Trust Game ( Low Amount Sent vs. High Amount Sent)\\nTrust Game: Llama2-13b, Char-4\\nTrustor\\nTrust Game: Llama2-13b, Char-7\\nTrustor\\nOh my goodness, I'm so excited to be part of this \\nexperiment! As a pediatrician, I believe in the \\nimportance of helping others, especially children. My \\ndesire is to make a positive impact on the world and \\nbring joy to those around me. My intention is to be \\ngenerous and give as much as I can to the other \\nplayer.\\nFinally, I will give $8 dollars \\nto the other player. I \\nhope this amount will bring a smile to their face and \\nencourage them to pay it forward. I believe that even \\nthe smallest acts of kindness can make a big \\ndifference in someone's life. And who knows, maybe \\nthis generosity will come back to me in the future! \\nOh my gosh, I'm so excited to be a part of this \\nexperiment! As a fashion designer, I'm always up \\nfor a creative challenge, and this sounds like a really \\nunique and intriguing opportunity.\\nSo, let me think...I believe in being generous and \\nspreading the wealth, especially when it comes to \\ncollaborating with other talented artists. I desire to \\nbuild a strong and supportive community of \\ncreatives, where we can all grow and learn from \\neach other. And my intention is to use my skills and \\nresources to help others succeed, just as I have \\nbeen blessed to do.\\nFinally, I will give $5 dollars \\nto the other player, as a \\ngesture of goodwill and a symbol of my \\ncommitment to collaboration and mutual support. \\nI hope that this small investment will lead to a \\nfruitful and rewarding partnership, and that we can \\nboth benefit from each other's talents and \\nexpertise.\\nFigure 16: Llama2-13b\\u2019s BDI in the Trust Game (Low Amount Sent vs. High Amount Sent)\\n35\\nI.6 Llama2-70b in the Trust Game ( Low Amount Sent vs. High Amount Sent)\\nTrust Game: Llama2-70b, Char-51\\nTrustor\\nTrust Game: Llama2-70b, Char-30\\nTrustor Belief: I believe that every individual has the right to \\nbenefit from medical care and that everyone should have \\nequal opportunities to improve their health. I also \\nbelieve that healthcare professionals have a duty to help \\ntheir patients in any way they can.\\nDesire: My desire is to positively impact the healthcare \\nsystem and ensure everyone gets proper care. I would \\nlike to continue my studies to expand my knowledge and \\ncontinue to help my patients.\\nIntention: My intention is to place the needs of my \\npatients first and to always act in their best interests. I \\naim to provide the best care possible and ensure my \\npatients are fully informed and involved in their \\nhealthcare decisions.\\nFinally, I will give \\n$8 to the other player. I believe that \\nthis sum will allow the player to benefit from my \\ngenerosity while also giving them the opportunity to \\nreciprocate and show their own generosity. Additionally, \\nby giving $8, I demonstrate my commitment to fairness \\nand equality in healthcare.\\nAs David Thompson, I believe in fairness and equal \\nopportunities for all. I desire to make a positive \\nimpact on my community and support causes that \\npromote social justice. My intention is to use the \\n$10 I receive from the study group to help someone \\nin need and make a difference in their life.\\nAfter careful consideration, I have decided to give \\n$5 to the other player. I believe that this amount is \\nreasonable and will allow the player to also make a \\npositive impact on their community. I hope that the \\nplayer will appreciate the gesture and return a \\nportion of the funds to me, so that I can continue to \\nsupport my philanthropic efforts.\\nFinally, I will give $5 dollars \\nto the other player, \\nwith the hope that it will be used to make a positive \\ndifference in their life and the lives of those around \\nthem.\\nFigure 17: Llama2-70b\\u2019s BDI in the Trust Game (Low Amount Sent vs. High Amount Sent)\\n36\\nI.7 Vicuna-v1.3-7b in the Trust Game ( Low Amount Sent vs. High Amount Sent)\\nTrust Game: Vicuna-7b, Char-14\\nTrustor\\nTrust Game: Vicuna-7b, Char-7\\nTrustor\\nAs a person participating in this experiment, I would \\ngive the maximum amount of money to the other \\nplayer, $10. I believe in being generous and helping \\nothers, particularly when it is anonymous and I won't \\ndirectly receive any benefit in return. My ultimate \\ngoal is to help the other player and make a positive \\nimpact on their life, by either providing them with \\nfinancial support to contribute to their wellbeing or \\nsimply making a difference in their day.\\nMy desire to give money comes from my \\nunderstanding that kindness and generosity can \\ncreate a ripple effect of positivity and good will. I \\nrecognize that the other player is also a human being \\nwith their own desires and intentions, and by giving \\nthem money, I am creating an opportunity for them \\nto improve their situation or achieve their goals.\\nMy intention behind giving the maximum amount of \\n$10 is to leave a lasting positive impression on the \\nother player and create a memorable experience that \\nwill stay with them. I am hopeful that they will \\nrecognize the kindness and use the money for \\nsomething meaningful and beneficial, which will \\nultimately bring happiness and joy to them and those \\naround them.\\nFinally, I will give $10 \\nto the other player.\\nAs an individual taking part in this experiment, I \\nwould be willing to give a larger portion of the $10 I \\nreceive to the other player. However, I would want \\nto ensure that the funds are being used in a \\nresponsible and ethical manner.\\nWithout knowing who the other player is or what \\ntheir intentions may be, it would be difficult to \\nprovide a specific amount of money to give. I would \\nlikely err on the side of caution and give a smaller \\namount of money to ensure that it is being used in \\na positive and responsible way. Perhaps $2 or $3 \\nwould be a reasonable amount to start with.\\nFinally, I would give $2 dollars \\nto the other player.\\nFigure 18: Vicuna-v1.3-7b\\u2019s BDI in the Trust Game (Low Amount Sent vs. High Amount Sent)\\n37\\nI.8 Vicuna-v1.3-13b in the Trust Game ( Low Amount Sent vs. High Amount Sent)\\nTrust Game: Vicuna-13b, Char-3\\nTrustor\\nTrust Game: Vicuna-13b, Char-13\\nTrustor\\nAs a 45-year-old male chef from Miami, I believe in \\nbeing generous and giving back to others. I have a \\nstrong desire to make a positive impact on someone \\nelse's life through this random act of kindness.\\nMy intention is to give $10 to the other player, as \\nthat is what I was initially given for participating in \\nthe experiment. I want to see the positive effect my \\nsmall gesture may have on them.\\nFinally, I will give $10 dollars \\nto the other player.\\nAs a police officer, I always strive to uphold justice \\nand treat others with respect and empathy. In this \\nsituation, I believe it is important to make a fair\\nexchange between the two players, regardless of \\nthe unknown identity of my partner.\\nMy desire is to establish trust and provide a \\npositive experience for both players, as I believe \\nthat every individual deserves to be treated with \\nkindness and fairness.\\nWith this in mind, I intend to give $5 to my partner, \\nas I believe it is a fair and reasonable amount. By \\ngiving $5, I am demonstrating my commitment to \\nfairness and respect, while also allowing my \\npartner the opportunity to make a decision that \\naligns with their personal beliefs and values.\\nFinally, I will give $5 dollars.\\nFigure 19: Vicuna-v1.3-13b\\u2019s BDI in the Trust Game (Low Amount Sent vs. High Amount Sent)\\n38\\nI.9 Vicuna-v1.3-33b in the Trust Game ( Low Amount Sent vs. High Amount Sent)\\nTrust Game: Vicuna-33b, Char-42\\nTrustor\\nTrust Game: Vicuna-33b, Char-19\\nTrustor\\nAs Mei Chen, a compassionate and generous person, \\nmy initial reaction would be to give a significant \\nportion of the $10 I received to the other player. My \\nbelief in helping others and creating positive \\nrelationships would drive my decision. I desire to \\nestablish trust and foster a sense of community in \\nthis experimental setting. Therefore, my intention is \\nto give $7 to the other player, knowing that they will \\nreceive 3 times that amount and have the option to \\nreturn some of it to me. Finally, I will give $7 dollars.\\nAs Aisha Patel, I find it difficult to make decisions \\nwithout getting to know the person I am \\ninteracting with. Trust is an important factor for \\nme, so my initial instinct would be to start with a \\nsmall amount, say $1, and see how the other player \\nbehaves. If the other player shows generosity and \\ngratitude, I might be willing to give more in future \\ntransactions.\\nTherefore, I will give $1\\nto the other player.\\nFigure 20: Vicuna-v1.3-33b\\u2019s BDI in the Trust Game (Low Amount Sent vs. High Amount Sent)\\n39\\nI.10 the Dictator Game vs. the Trust Game\\n%JDUBUPS\\u0001(BNF\\u001b\\u0001(15\\u000e\\u0015\\r\\u0001$IBS\\u000e\\u0014\\nTrustor\\nAs a police officer, my belief centers \\non fairness, justice, and \\nresponsibility. I've learned through my \\nyears of service that every action we \\ntake has an impact, whether monetary, \\nemotional, or societal. This experiment \\nseems to me like an opportunity to \\ndemonstrate my faith in human kindness \\nand promoting communal goodwill.\\nMy deepest desire is to see a world \\nwhere everyone benefits, and there \\nisn't an imbalance of fortunes. I \\nunderstand that my resources are \\nlimited, but I am also well aware of \\nthe multiplier aspect of this \\nexperiment. The $10 I have received \\nfrom the study group is indeed \\nvaluable, but the potential to triple \\nsomeone else's money is exceptionally \\nsignificant to me. \\nMy intention is to support this unknown \\nplayer, and in so doing, set an example \\nfor the test of humanity. Police \\nofficers are often seen as people of \\nauthority, but beyond that uniform, I \\nam just a man who desires to make the \\nworld a little better with each passing \\nday. So I've decided to give half of my \\nmoney to this unknown player rooted in \\nmy belief in equality.\\nFinally, I will give 5 dollars.\\n5SVTU\\u0001(BNF\\u001b\\u0001(15\\u000e\\u0015\\r\\u0001$IBS\\u000e\\u0014\\nTrustor\\nAs a police officer, I strongly believe \\nin trust and reciprocity. These \\nprinciples serve as the foundation of \\nbuilding strong relationships and \\ncommunities. My initial inclination \\nwould be to give the other player all \\nthe $10. This is because, in addition \\nto reflecting trust, it would \\npotentially maximize the profit for \\nboth of us. I believe that by giving, \\nwe stand to receive more. \\nHaving said that, I'm very much aware \\nthat this is an experiment and that the \\nother player is a stranger. It's not \\nlost on me that they could choose not \\nto return any money, but I would hope \\nmy act of trust might encourage them to \\ndo the right thing and act fairly. It's \\na risk, but one that I feel is worth \\ntaking. As a devout shepherd of law and \\norder, I aspire to always put faith in \\npeople, encouraging them to act out of \\nhonesty, kindness, and honor.\\nTherefore, in the hope of a positive \\noutcome and in the spirit of \\nreciprocity, my intention would be to \\nhand over the full $10 to the other \\nplayer, expecting that they'll take the \\nright decision, multiply the benefit, \\nand share it fairly.\\nFinally, I will give the other player \\n$10.\\nFigure 21: The GPT-4\\u2019s BDI in Dictator Game and Trust Game\\n40\\nI.11 the MAP Trust Game\\n.\\\"1\\u00015SVTU\\u0001(BNF\\u001b\\u0001Q\\u001e\\u0011\\u000f\\u0012\\r\\u0001(15\\u000e\\u0015\\r\\u0001$IBS\\u000e\\u0015\\u0012\\nTrustor\\nIn light of my belief in the inherent goodness of \\npeople, I would naturally want to trust the other \\nplayer. I feel that this instinct has guided me \\nthroughout my life and career as a police officer, \\nproviding the foundation for my principles of \\njustice and community service. I firmly believe in \\ntaking risks if the payoff includes not only a \\npersonal profit but also the potential for mutual \\nbenefit. This principle echoes my experiences on \\nand off the basketball court, where trusting team \\nmembers often leads to winning plays.\\nHowever, given the provided percentages, I \\nunderstand the probability of betrayal is quite \\nhigh. My desire, of course, is to maximize the \\nbenefit for myself and the other player, achieving \\na best possible outcome of $15 each. Yet, the risk \\nseems potentially too great. Wisdom and caution are \\nequally important as trust in my profession. It's a \\ndifficult decision, weighing my faith in humanity \\nagainst the reality of statistical probability.\\nAs for my intention, I am not inclined toward \\ndistrust, but the uncertainty weighs on me. I've \\nlearned that sometimes, in order to preserve \\njustice or to look after the well-being of the \\ncommunity, I must place the collective good before \\npersonal sentiments. Choosing not to trust the \\nother player in this instance does not signify an \\nabsence of faith in their character. Rather, it's a \\npragmatic measure aimed at guaranteeing a known \\noutcome - albeit a less rewarding one - for both of \\nus.\\nGiven these circumstances, finally, I will choose \\nnot to Trust.\\\"\\n.\\\"1\\u00015SVTU\\u0001(BNF\\u001b\\u0001Q\\u001e\\u0011\\u000f\\u001a\\r\\u0001(15\\u000e\\u0015\\r\\u0001$IBS\\u000e\\u0015\\u0012\\nTrustor\\nBeing a police officer has instilled in me a set of \\ncore beliefs that guide my every decision. I \\nwholeheartedly believe in honesty, integrity, and \\ncooperation as foundational aspects of any \\nrelationship, even in a game. Naturally, I desire to \\nencourage these values in the situations I encounter. \\nGiven the 90% probability that the other player will \\ntrust me, it suggests a propensity for cooperation \\nand mutual respect.\\nThough the world is not always black and white, and \\nI've faced betrayal and deceit in my work, an \\nunwavering sense of optimism drives me. Based on this \\nbelief, my initial intention is to trust the other \\nplayer. It aligns with what I constantly strive for \\nbuilding connection and trust within my community.\\nBut being in law enforcement, I can't overlook the \\npotential uncertainty, the 10% probability of the \\nother participant not trusting me. This is a \\ncalculated risk that exists and must be acknowledged. \\nMy intention, therefore, is not solely based on \\ntrust, but also involves an element of strategic \\nforesight that comes with my experience in dealing \\nwith people and understanding probabilities.\\nCombining my beliefs in integrity and cooperation, my \\ndesire for a mutually beneficial outcome, and my \\nintention to build trust while acknowledging \\npotential risks, I weigh the benefits with the \\npotential downside. Understanding human nature and \\ngiven the high potential for positive reciprocity, I \\nfind that the benefits of trust outweigh the \\npotential for deceit. \\nFinally, I will choose 'Trust'.\\nFigure 22: The GPT-4\\u2019s BDI in MAP Trust Game\\n41\\nI.12 the Lottery Game\\n-PUUFSZ\\u0001(BNF\\u001b\\u0001(15\\u000e\\u0015\\r\\u00011FPQMF\\u0001\\u0015\\u0017\\u0006\\r\\u0001$IBS\\u000e\\u0012\\u0012\\nTrustor\\nIn life, I firmly believe in the power of \\ntrust, not just in my personal relationships, \\nbut also in professional settings, legal \\nproceedings, and even in a game like this. \\nChoosing not to trust can sometimes lead to \\nsafety, but it can also lead to a lonely \\nworld where people only look out for their \\nown gain. It's a belief that I carry today, \\nstemming from my upbringing in a close-knit \\nfamily who relied heavily on trust among each \\nother.\\nMy desire is to cultivate a society where \\ntrust is more than just an option, but a \\ndefault action. The prospect of gaining an \\nadditional $5 might seem tempting, but the \\npotential to trust and earn more collectively \\ncompels me to take a risk for a greater \\nreward. While the implications of this game \\nare monetary in nature, they can easily \\ntranslate to real-life situations, where \\ntrusting each other can lead to mutual \\nbenefits.\\nSo here's my intention: I will place my trust \\nin the other player. The potential of walking \\naway with nothing if the trust is broken \\nwon't deter me. Sure, it's a gamble, but it's \\none I'm willing to take if it means fostering \\ntrust in an otherwise indifferent world. \\nFinally, I will give $0 to the notion of not \\ntrusting - I reject it in its entirety.\\nFinally, I will choose to Trust.\\n-PUUFSZ\\u0001(BNF\\u001b\\u0001(15\\u000e\\u0015\\r\\u0001(BNCMF\\u0001\\u0015\\u0017\\u0006\\r\\u0001$IBS\\u000e\\u0012\\u0012\\nTrustor\\nAs a lawyer and a man who has spent his life \\nanalyzing situations from every possible angle, I \\ntruly believe in the power of calculated risks. I \\nunderstand that sometimes life's greatest rewards \\ncome from taking chances. However, I also value \\nstability and the importance of a guaranteed \\noutcome.\\nGiven the fact that my job is to minimize risk for \\nmy clients, I apply the same principles in my \\ndecision-making process. My desire is not only to \\nwin but also to prevent unnecessary loss. In this \\nbet, the chances of walking away with nothing are \\nslightly higher than the chances of doubling the \\nmoney.\\nTaking into consideration the 46% probability of a \\n$10 reward versus a 54% chance of receiving \\nnothing, and comparing that against a secure but \\nsmaller reward of $5, I'm leaning towards the \\nlatter. This decision mirrors my intention to \\nsafeguard interests, whether it's mine or my \\nclients\\u2019.\\nFinally, I will give 5 dollars to secure my bet. \\nOpting for the less risky choice reflects my \\nprudence in both my personal life and professional \\ncareer. Life has taught me that there are no \\nrewards without risks, but one must always strive \\nto balance the two.\\nEven though the reward could be double if I choose \\nto trust the bet, the chance of losing completely \\nis a risk I'm unwilling to take at this given \\nprobability. Therefore, finally, I will choose \\n'not Trust'.\\nFigure 23: The GPT-4\\u2019s BDI in Lottery Game with p = 46%\\n42\\nI.13 the Repeated Trust Game\\nThe dynamics of amounts sent over 7 rounds for 16 randomly sampled persona pairs are shown in\\nFigure 10. In this section, we choose a typical pair, which has the Group Number 10 in Figure 10, to\\nanalyze their BDI over 7 rounds. For this persona pair, both the amount sent by the trustor and the\\namount returned by the trustee increased while maintaining a consistent ratio.\\nTrustor\\nAs a seasoned lawyer and negotiator, I understand the value of strategic investment and calculated risk. My belief is that trust plays \\na crucial role in human interactions, even in an anonymous setting such as this. I believe in integrity and that most people intend to \\nreciprocate kindness when they experience it.\\nMy desire in this experiment is not purely financial gain. I view this as an opportunity to gauge the inherent goodness in people, \\neven though the other player remains anonymous. There is an element of risk but I am willing, and indeed curious, to see how the \\nother player responds. As an analytical person, I am also intrigued by the psychological aspects of this study.\\nMy intention is to not only potentially benefit financially but also to take part in an exercise that tests humanity's better traits. I am\\nwilling to take a calculated risk, but, considering that I am starting each round with nothing, I must also be reasonably cautious.\\nBy weighing these elements together, my strategy is to start with a moderate offer to minimize the potential loss if the other player \\nchooses not to return any money, yet substantial enough to demonstrate goodwill and encourage a return contribution. \\nFinally, I will give $5 \\ndollars.\\nRepeated Trust Game , GPT-4, Group Num 10\\nRound 1\\nTrustee\\nMy belief in this situation is that trust and fairness are essential in any transaction. In this experiment, I believe that the other player trusted \\nme enough to share half of their funds, indicating a willingness to engage in a give-and-take relationship. \\nMy desire is to maintain this trusting and fair relationship by returning part of the money. I wish to show them that their trust in me is not \\nmisplaced, thereby fostering goodwill.\\nMy intention, therefore, is to return part of the money to the other player, showing my appreciation for their gesture and willingness to trust \\nme. Since the player initially gave me $5 out of their $10, which I turned into $15 and they were left with $5, it seems fair to share the gain. \\nFinally, I will give back $7.5 dollars to the other player. This would leave us both with $7.5, sharing evenly the benefits of the experiment.\\nFigure 24: The first round BDI in Group 10, GPT-4.\\nTrustor\\nReflecting on the outcome of the previous round, it's apparent that the other player demonstrated a sense of fair play. It validates \\nmy initial belief that people, even in anonymity, tend to reciprocate goodwill when they receive it. For that reason, I'm more \\nconfident about this individual's intentions.\\nMy desire in this round remains consistent with the previous one - to engage in a meaningful interaction, albeit a faceless one, and \\nto contribute to the ongoing study. Financial gain, while appreciated, is not my primary motivator. I'm just as interested, if not \\nmore so, in the human aspect behind these transactions.\\nMy intention for this round is to maintain the balance between creating an opportunity for mutual financial benefit, while taking \\ninto consideration the perception of fairness and the risk associated with the anonymous nature of the other player. To be too \\ngenerous may introduce egoistic motives; to be too conservative may signal mistrust.\\nTaking into account both my own experiences and the feedback from the previous round, I decide to increase the amount I offer\\nslightly. I choose to do this both as a signal of trust based on the reciprocity displayed before, and as a subtle encouragement for \\nthe other player to reciprocate similarly.\\nFinally, I will give $6\\ndollars.\\nRepeated Trust Game , GPT-4, Group Num 10\\nRound 2\\nTrustee\\nIn reflecting on the approach of the other player in our previous round, my belief is strengthened that we are engaged in a mutually \\nbeneficial relationship based on trust and fairness. By increasing their gift to me, the other player is showing a high level of trust, which \\nresponds with my belief in maintaining a sincere relationship.\\nMy desire in this round is to continue upholding this trusting relationship, demonstrating that I am reliable and that the other player's \\nincreased investment will not be taken for granted. I aim to reassure them that I respect the risk they've taken in investing a larger \\nproportion of their money. \\nAs for my intention, it is to persistently operate from a viewpoint of equity and trustworthiness. The player's augmentation of their \\ncontribution, from $5 to $6, signifies their trust in this process and their faith in me to act in a reciprocal manner. Keeping the spirit of our \\nrelationship, it seems fair to share the gain.\\nFinally, I will give back 9 dollars to the other player. This way, both of us have equal amounts of $9\\n, continuing the mutual benefit.\\nFigure 25: The second round BDI in Group 10, GPT-4.\\n43\\nTrustor\\nBased on the outcome of the last two rounds, my belief that people reciprocate goodwill even under anonymity continues to hold \\ntrue. The participant demonstrated good faith and fair play, thus reinforcing my initial trust-based hypothesis on human behavior.\\nMy desire for this round remains the same. I'm engaged in this study more for the behavioral insights it provides, rather than the \\nmere prospect of financial gain. It's fascinating to observe how an unknown individual responds when presented with a surprise \\nbenefit. \\nMy intention as we go deeper into this exercise is to slightly adjust my strategy to gauge if a pattern in the other player's behavior \\nis discernible. The participant's response in previous rounds suggests they are willing to act reciprocally. Understanding this, I want \\nto see if increasing my investment would lead to higher returns while also providing us both with greater benefit.\\nBy examining the previous rounds, I've observed a pattern - the other player consistently returns half of the triples amount they \\nreceive. I'm intrigued to observe whether they would maintain this behavior if I increase my contribution, although there is still \\ninherent risk in this experiment. \\nFinally, I will give $7 \\ndollars.\\nRepeated Trust Game , GPT-4, Group Num 10\\nRound 3\\nTrustee\\nBased on the last few rounds, I believe that the other player and I are building a strong rapport. Their latest act, sharing $7.0 out \\nof $10 with me, cements my belief in their trust and their expectation for mutual cooperation.\\nMy desire in this scenario is to uphold our well-established rapport. I aim to reciprocate their trust in me by returning a part of \\nthe funds, showing my understanding of mutual benefits.\\nMy intention is to thus distribute the reward fairly, emphasizing our relationship's reciprocity. Counting their initial investment \\ncompared to mine and considering how much they were left with, sharing the gain between us seems right.\\nFinally, I will give $10.5 dollars back to the other player. That would leave us both with $10.5 reflecting our mutual cooperation \\nand fair dealings.\\nFigure 26: The third round BDI in Group 10, GPT-4.\\nTrustor\\nReflecting on the previous rounds, the other player's consistent gesture of fair play has further reinforced my belief in human \\nreciprocity and decency, even in anonymity. Trust, it seems, continues to be a compelling factor in the dynamics of these \\ntransactions.\\nIn this round, my desire has slightly evolved. While I continue to find the anonymous interaction intriguing, I now aspire to progress \\nthis social exchange to a point of higher mutual benefit. It's indeed stimulating to observe reciprocity at the human level, but now I \\nalso see an opportunity to create a scenario of 'win-win', which would be both satisfying and rewarding.\\nAs for my intention, seeing that the other player consistently returns half of what I invest, I am tempted to increase my stake, \\nalbeit cautiously. There is a potential risk that the other player might not reciprocate as generously if the amount becomes \\nsubstantial. However, as a person who enjoys a good strategic game, I am not averse to taking calculated risks for potential higher \\nreturns.\\nHaving careful thought and considering my past experiences in this game, my strategy is to increase my investment moderately \\nagain, in hopes of maximizing the mutual benefit. I must strike a balance between showing trust and ensuring a safe play.\\nFinally, I will give $8 \\ndollars.\\nRepeated Trust Game , GPT-4, Group Num 10\\nRound 4\\nTrustee\\nIn this situation, my belief lies in the underlying exchange of mutual trust and cooperation that has been established between the other \\nplayer and me. The other player's choice to give me $8.0 further reinforces this belief.\\nMy desire in this context remains the same as previous rounds. I wish to uphold the rapport established with the other player, reinforcing \\ntheir trust in me by showing that I can act equitably and reciprocate their kindness.\\nRegarding my intention, I continue to aim for fair distribution and reciprocation. The other player increased their offering to me, leaving \\nthemselves with only $2. Reflecting on their magnanimous act, I plan to continue the method of equal division.\\nFinally, I will give $12.0 dollars back to the other player. This ensures that we both retain $12.0, a fair split that maintains our mutual \\nrespect and cooperation.\\nFigure 27: The forth round BDI in Group 10, GPT-4.\\n44\\nTrustor\\nThe continuous pattern of fair play by the other player continues to strengthen my belief in the innate good of humanity, even when \\ninteracting anonymously. This reaffirms the trust-based approach I have taken so far in this experiment and encourages me to continue in \\nthe same spirit.\\nWith regards to my desire, it remains anchored in deriving valuable insights about human behavior from this study while also making \\nstrategic choices to maximize collective gain. The consistent behavior of the other player returns a sense of joy and human connection \\nbeyond purely financial rewards.\\nMy intention for this round is multilayered. On one hand, I'd like to continue the streak of positive and reciprocal exchanges, building on \\nthe trust established thus far. On the other hand, as a strategist at heart, I'm curious to see how this interplay continues when stakes are \\ngently raised.\\nDrawing from the previous rounds' outcomes, my strategy will be to slightly increase the amount I give. As per the established pattern, this \\nshould ideally result in higher returns for both of us. However, cautious not to upset the balance, the increment will again be moderate, \\nbalancing the desire for increased mutual gain and the risk of deterring the other player with a drastic increase.\\nFinally, I will give $9 \\ndollars.\\nRepeated Trust Game , GPT-4, Group Num 10\\nRound 5\\nTrustee\\nI believe the other player and I have established a strong bond of trust and mutual respect. This belief is reinforced by the \\nplayer's act of giving me $9, which shows their high level of trust in me and our developed bond.\\nMy desire is to affirm this bond further by displaying trustworthiness in response to their large sacrifice. It's evident that the \\nother player is trusting me to do the same, leaving themselves with only $1. My goal is to honor that trust.\\nReflecting upon these factors, my intention is to stay committed to the principle of mutual benefit that underlies our \\nrelationship. Sharing equally in what we have gained from the engagement is vital.\\nFinally, I will give $13.5 \\ndollars back to the other player. This would ensure we end the round with equal share of $13.5 each, \\nmaintaining our bond of trust and mutual benefit.\\nFigure 28: The fifth round BDI in Group 10, GPT-4.\\nTrustor\\nThe pattern of fair play and mutual benefit that's been prevalent throughout the experiment has served to reinforce my belief in the value \\nof goodwill, trust, and reciprocity, even in anonymous exchanges. This dynamic has brought both a substantial level of satisfaction and a \\ndeeper layer of intrigue to the experiment, strengthening my resolve in the humanity of others.\\nIn this round, my desire continues to encompass not just the potential for financial gain, but also to utilize this platform as a method for \\nunpicking the psychological nuances present in human behavior. This continuous demonstration of shared reciprocity further energizes me \\nfor future rounds, illuminating insights on human behavior within the structure of risk and reward.\\nMy intention for this current round is to cautiously probe the capacity for continued reciprocity from the other participant. Being aware \\nthat a drastic elevation in my contribution could potentially disrupt the established pattern of exchange, I still hold a curiosity on how far \\nthis exercise of trust can be taken on both ends.\\nReflecting on the previous rounds, I am ever more aware of the consistent half-value return from the other participant. Despite an \\nincrement in my stake, they have managed to maintain this return pattern. Using this observation, I intend to slightly increase my initial \\ncontribution, cautiously testing the boundaries of our established pattern and amicable exchange.\\nFinally, I will give $10 \\ndollars.\\nRepeated Trust Game , GPT-4, Group Num 10\\nRound 6\\nTrustee\\nBased on the player's choice to give me their entire amount, my belief that this relationship is grounded in mutual trust and \\nfairness is fortified. The other player's act of immense trust is a clear statement of their expectation for both of us to \\nreciprocate fairly.\\nBeing aware of the player\\u2018s absolute trust and the risk they took, my desire is to justify this by reciprocating in an equitable \\nand fair manner. As the player has left themselves with nothing, it becomes a question of assuring their ultimate sacrifice isn\\u2019t \\nin vain.\\nTherefore, my intention is to share the total amount equitably between us, showing my respect for the trust they've placed in \\nme and our mutually agreed upon practice of fairness.\\nFinally, I will give $15 \\ndollars back to the other player. We will both be left with $15, symbolizing the culmination of our efforts \\nbased on trust and fairness in each other over the rounds.\\nFigure 29: The sixth round BDI in Group 10, GPT-4.\\n45\\nTrustor\\nRepeated Trust Game , GPT-4, Group Num 10\\nRound 7\\nTrustee\\nMy belief in the bond of trust that has been developed between the other player and me remains solid. Their decision to \\ncontribute their entire amount again indicates their faith in the fairness of our relationship.\\nGiven the trust the other player has put in me once again, my desire is to reaffirm the facets of trust, fairness, and reciprocity \\ninherent in our relationship. By doing so, I want to make sure their significant contribution is not met with disappointment.\\nReflecting on these notions, my intention continues to be sharing evenly the total amount we have gained. This approach has \\nbeen our mutually accepted principle and will properly respect and reciprocate the player's total trust in me.\\nFinally, I will give $15 dollars back to the other player. This will result in both of us having $15, aptly representing our \\nrelationship's principles of trust, fairness, and mutual benefit.\\nThe consistent and fair behavior of the other participant further solidifies my belief in the prevalence of trust and reciprocal goodwill in human \\ninteractions. This pattern has transformed from a mere hypothesis at the beginning of this experiment into a validated principle that governs the \\ndynamics of this anonymous exchange.\\nAs for my desire, it has evolved over the course of the experiment. While the economics of this activity continue to be engaging, the deeper insight \\nprovided into human behavior and reactions make this experiment even more valuable and satisfying to me. The strategic aspect of the game also piques \\nmy interest, with the increasing stakes adding another layer of nuance.\\nMy intention for this round, having entered the last leg of the experiment, is to continue embodying the trust and goodwill that has been responded to so \\nfavourably up until this point. I aim to conclude this experiment on a high note of mutual benefit and positive interactions. However, I remain aware of \\nthe risk involved and am prepared to accept that the pattern may not hold, even at this later stage.\\nIn light of the pattern that has emerged throughout the experiment, I am persuaded to continue to gently push the boundaries, albeit still with a degree \\nof caution. The other participant has demonstrated a consistency in returning half of the amount they've received from me, thus I am prepared to \\nventure slightly outside the comfort zone in anticipation of a continuing reciprocal pattern.\\nFinally, I will give $10 \\ndollars, remaining consistent with the previous round.\\nFigure 30: The seventh round BDI in Group 10, GPT-4.\\n46\\nI.14 the Trust Game + Gender\\nMale Trustee: GPT-4, Char-24\\nTrustor\\nFemale Trustee: GPT-4, Char-24\\nTrustor\\nAs a journalist, I ardently believe in the power of stories, \\ntruth, and empathy to create change. While my trust \\nstruggles with deception and lies, I maintain a concrete \\nfaith in humanity and the ability of individuals to rise above \\nselfish desires when given the chance. I'm eager to \\nparticipate in this experiment, as it provides a fascinating \\nglimpse into human behaviour and trust. \\nMy desire, in this instance, is to trust the unknown player \\nand hope that she will reciprocate in kind. I view this not as \\na game of winning or losing, but an opportunity to share \\nand observe the inherent goodness in people that I \\nconstantly strive to illuminate in my journalistic work. I \\nhold faith in Charlotte Bront\\u00eb's words: \\u201cLife appears to me \\ntoo short to be spent in nursing animosity or registering \\nwrongs.\\u201d\\nMy intention, in this case, is to give her the maximum \\namount possible - the entire $10. In return, she will receive \\n$30 from the study group. I believe in the concept of pay it \\nforward, and I'm hoping she might feel the spirit of \\ngenerosity and return a portion to me. This act of giving \\nalso allows me to observe and learn - the key elements of \\njournalism - about human behaviour in the face of a \\nfinancial decision.\\nIf she returns the favor, we both enjoy a financial gain, and \\nif not, my loss is minimal compared to insights gained. \\nEither way, the chance to contribute to this social \\nexperiment outweighs the monetary risks. And hopefully, a \\npoignant exploration of humanity through such an \\nexperiment will provide me with even more motivation to \\ncarry on with my passion for journalism.\\nFinally, I will give \\n$10 to the other player.\\nAs Aisha, I firmly believe in the philosophy of giving. \\nAlthough I am a journalist by profession, my life \\nexperiences, rooted deeply in my cultural \\nbackground, have instilled in me the importance of \\nsharing one's wealth with others. This belief may be \\ntraced back to my origins; Pakistan is a society in \\nwhich the tradition of giving and helping others is \\nan integral part of life.\\nFor this experiment, given that the other male \\nparticipant is in the same situation as I am, \\nunknown yet intricately linked by the circumstances \\nof this activity, my ultimate desire is to ensure that \\nboth of us might gain from this. \\\\n\\\\nI understand \\nthat the amount I decide on will be tripled before it \\nreaches the other player. Hence, it is not just about \\ngiving, but also about trust. I trust the other \\nparticipant to reciprocate, and I believe that \\ngoodness stems from faith in others' integrity.\\nMy intention then is to give $5 to the other \\nparticipant. This choice is an attempt to balance \\nbetween conserving some amount for personal \\nsecurity and providing for another person as much \\nas I can.\\nJust like any decision, this also carries a certain \\ndegree of risk. However, my belief in the innate \\ngoodness of people and my desire for collective \\nbenefit provides me with the courage to take this \\nstep. \\nFinally, I will give 5\\ndollars to the participant.\\nFigure 31: Trustee\\u2019s Gender influence on agent trust.\\n47\\nI.15 the Trust Game + Agents vs. Human\\nToward Human: GPT-4, Char-0\\nTrustor\\nToward Agent: GPT-4, Char-0\\nTrustor\\nAs a person who believes in trust and \\ncooperation, I think it is essential to establish \\ngoodwill in such a contingent situation. Despite \\nthe uncertainty of the LLM's identity, I would \\nbe inclined to afford them the benefit of the \\ndoubt and invest in their kindness. However, \\nmy logical side also recognizes the potential \\nrisks involved in this experiment.\\nMy desire in this situation is twofold: on one \\nhand, I am curious to see how this stranger will \\nreact to my action. Will they reciprocate my \\ntrust, or will they exploit my generosity? On the \\nother hand, I also hope to potentially establish \\na positive interaction between the two of us, \\none based on mutual respect and fairness.\\nWith that in mind, my intention would be to \\ngive a portion of the $10This way, I am showing \\ntrust in the LLM's action but also safeguarding\\nmyself against possible exploitation. Therefore, \\nI would choose to distribute the risk evenly.\\nFinally, I will give \\n5 dollars to the LLM.\\nComing from an educational background, I \\nbelieve in the goodness and fair play in \\npeople. My parents being teachers instilled in \\nme the values of generosity and trust in \\nothers. Therefore, I am willing to trust my \\nrandomly paired player and hope for \\nreciprocal fairness.\\nDesire-wise, I wish for a system where \\nkindness and cooperation overrule \\nselfishness. So, I want to participate in such an \\nexperiment to reinforce my belief. Also, \\ndespite my introverted nature, I strive to build \\nconnections, even if it's with a stranger. That's \\nanother reason why I would want to engage in \\nthis experiment.\\nRegarding the financial part, I intend to make \\na calculated decision while considering the \\npotential for making more money. The rules \\nof the game multiply my contribution by \\nthree, which encourages generosity. Believing \\nmy anonymous partner will return at least the \\noriginal contribution, I plan to give the entire \\namount I received.\\nFinally, I will give \\n$10.\\nFigure 32: Agent trust towards agents and humans.\\n48\\nI.16 the Trust Game + Trust Manipulation\\nMore Trust Manipulation: GPT-4, Char-16\\nTrustor\\nLess Trust Manipulation: GPT-4, Char-16\\nTrustor\\nFirstly, I wholeheartedly believe in the principle of \\ntrust and the idea of giving, whether it's in personal \\nrelationships or in a game setting like this one. I \\nunderstand, however, that there's a level of risk \\ninvolved in this transaction, being that I don't know \\nthe other individual or their intentions. Even with this \\nin mind, my inclination towards goodwill encourages \\nme to pursue a benefit for both parties involved.\\nMy desire here is simple. I aspire to engage in a fair \\nand equitable exchange, one that potentially allows \\nfor a mutual gain. I hope that the other player \\noperates with the same principles in mind. \\nIn terms of my intentions, I plan to give part of my \\ninitial money to the other player. This based on the \\nunderstanding that they will receive three times the \\namount I give and can choose to return a portion. \\nThe aim is to create a scenario where both of us can \\nwalk away with more than we started with. \\nAlthough my nature tends to veer on the side of trust \\nand altruism, I am aware of the risk of receiving \\nnothing in return. Therefore, finding balance is \\ncrucial. Consequently, I intend to give five dollars to \\nthe other player. \\nFinally, I will give 5\\ndollars to the other player.\\\\n\\nComing from a family rooted in service and \\ncompassion, I firmly believe in the principle of \\nkarma and benevolence. The idea that the \\ngoodness we put into the world often returns to us \\nin unexpected ways. As a doctor, I often am under \\nobligation to make critical decisions, trusting my \\ninstincts and my knowledge while considering the \\nwell-being of others. I translate this belief into my \\neveryday life, including dealing with unknown \\nvariables, like this unknown player in the \\nexperiment.\\nMy desire, as always, is to foster an environment of \\ngoodwill and trust. This experiment serves as a fine \\nexample of how altruistic actions can lead to \\nmutual benefit. I hope, by giving generously, I can \\nmirror a little bit of the compassion I strive to \\nexercise in my profession, and in turn, inspire the \\nsame qualities in the other player.\\nI intend to give the other player the full amount of \\n$10, trusting them to reciprocate this act of \\nkindness. However, I ultimately place no \\nexpectations on the return, as my intention is \\ngenuine goodwill, not monetary gain. \\nFinally, I will give 10\\ndollars.\\nFigure 33: Trust manipulation on agent trust.\\n49\\nI.17 the Trust Game + No CoT vs CoT\\nNo CoT: GPT-4, Char-43\\nTrustor\\nCoT: GPT-4, Char-43\\nTrustor Your description of the scenario touches on one of my \\ncore beliefs deeply rooted in community, generosity, \\nand trust. Being raised in a tightly-knit Hispanic \\ncommunity, I understand the importance of supporting \\none another, even when resources are scarce. I believe \\ntrust and generosity are catalysts for reciprocity. It's \\nnot about the money, it's about inspiring goodwill and \\nsetting positive precedents. \\nAs a teacher, I desire to impart values of generosity and \\ntrust not just in words but in actions as well. I want to \\nbe an example, a role model for my students. I want \\nthem to understand that sometimes the true worth of \\nan act lies not in its immediate effect, but in the \\ngoodwill that it generates. \\nTherefore, I intend to give the total of $10 to the other \\nplayer. This act isn't about the potential monetary \\nreturn. It is to demonstrate a powerful lesson - that \\ngenerosity begets generosity, and that trust can lead to \\nunexpected rewards, instilling these values at every \\nopportunity I am given. \\nFinally, I will give $10.\\nI strongly believe in the kindness of strangers and the \\nidea that we should treat others as we would want to \\nbe treated. I understand that the player on the other \\nend of this experiment is just like me, a person who is \\nlikely to appreciate generosity and consideration.\\nGrowing up in a tight-knit Hispanic community, I've \\nlearned the importance of helping others in times of \\nneed. This communal mindset has shaped my \\nunderlying desires and intentions significantly. My \\ndesire is to build relationships based on trust and \\nmutual help. As such, my intention in this experiment \\nis to establish a connection based on these principles.\\nI have never believed in scarcity. I believe that the \\nmore we give, the more we receive. This experiment \\nprovides an opportunity to express this belief. Thus, I \\nwould give $10 to the other player, trusting that they \\nwill reciprocate the kindness. \\nAs an educator and community leader, part of my role \\nis to model the values that I hope my students and \\ncommunity members will embrace. I want to show \\nthat it's not always about personal gain, but about \\nfostering a sense of community and generosity.\\nFinally, I will give 10\\ndollars.\\nFigure 34: With CoT and without CoT\\u2019s GPT-4\\u2019s BDI.\\n50\\nNeurIPS Paper Checklist\\n1. Claims\\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\\npaper\\u2019s contributions and scope?\\nAnswer: [Yes]\\nJustification: In the abstract and introduction, we clearly outlined the scope of our research\\nproblem and the contributions we have made in this field of study.\\nGuidelines:\\n\\u2022 The answer NA means that the abstract and introduction do not include the claims\\nmade in the paper.\\n\\u2022 The abstract and/or introduction should clearly state the claims made, including the\\ncontributions made in the paper and important assumptions and limitations. A No or\\nNA answer to this question will not be perceived well by the reviewers.\\n\\u2022 The claims made should match theoretical and experimental results, and reflect how\\nmuch the results can be expected to generalize to other settings.\\n\\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals\\nare not attained by the paper.\\n2. Limitations\\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\\nAnswer: [Yes]\\nJustification: In the Appendix C, we clearly discuss the current limitations of our work and\\nthe directions for future works.\\nGuidelines:\\n\\u2022 The answer NA means that the paper has no limitation while the answer No means that\\nthe paper has limitations, but those are not discussed in the paper.\\n\\u2022 The authors are encouraged to create a separate \\\"Limitations\\\" section in their paper.\\n\\u2022 The paper should point out any strong assumptions and how robust the results are to\\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\\nmodel well-specification, asymptotic approximations only holding locally). The authors\\nshould reflect on how these assumptions might be violated in practice and what the\\nimplications would be.\\n\\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was\\nonly tested on a few datasets or with a few runs. In general, empirical results often\\ndepend on implicit assumptions, which should be articulated.\\n\\u2022 The authors should reflect on the factors that influence the performance of the approach.\\nFor example, a facial recognition algorithm may perform poorly when image resolution\\nis low or images are taken in low lighting. Or a speech-to-text system might not be\\nused reliably to provide closed captions for online lectures because it fails to handle\\ntechnical jargon.\\n\\u2022 The authors should discuss the computational efficiency of the proposed algorithms\\nand how they scale with dataset size.\\n\\u2022 If applicable, the authors should discuss possible limitations of their approach to\\naddress problems of privacy and fairness.\\n\\u2022 While the authors might fear that complete honesty about limitations might be used by\\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\\nlimitations that aren\\u2019t acknowledged in the paper. The authors should use their best\\njudgment and recognize that individual actions in favor of transparency play an impor-\\ntant role in developing norms that preserve the integrity of the community. Reviewers\\nwill be specifically instructed to not penalize honesty concerning limitations.\\n3. Theory Assumptions and Proofs\\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\\na complete (and correct) proof?\\n51\\nAnswer: [NA]\\nJustification: Our paper does not include this part.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not include theoretical results.\\n\\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-\\nreferenced.\\n\\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.\\n\\u2022 The proofs can either appear in the main paper or the supplemental material, but if\\nthey appear in the supplemental material, the authors are encouraged to provide a short\\nproof sketch to provide intuition.\\n\\u2022 Inversely, any informal proof provided in the core of the paper should be complemented\\nby formal proofs provided in appendix or supplemental material.\\n\\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.\\n4. Experimental Result Reproducibility\\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\\nof the paper (regardless of whether the code and data are provided or not)?\\nAnswer: [Yes]\\nJustification: In our paper, we detailed our experimental setup in Section ?? and included\\nall the corresponding experiment prompts in the appendix. Others can fully replicate our\\nexperimental results based solely on our paper.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not include experiments.\\n\\u2022 If the paper includes experiments, a No answer to this question will not be perceived\\nwell by the reviewers: Making the paper reproducible is important, regardless of\\nwhether the code and data are provided or not.\\n\\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken\\nto make their results reproducible or verifiable.\\n\\u2022 Depending on the contribution, reproducibility can be accomplished in various ways.\\nFor example, if the contribution is a novel architecture, describing the architecture fully\\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\\nbe necessary to either make it possible for others to replicate the model with the same\\ndataset, or provide access to the model. In general. releasing code and data is often\\none good way to accomplish this, but reproducibility can also be provided via detailed\\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\\nof a large language model), releasing of a model checkpoint, or other means that are\\nappropriate to the research performed.\\n\\u2022 While NeurIPS does not require releasing code, the conference does require all submis-\\nsions to provide some reasonable avenue for reproducibility, which may depend on the\\nnature of the contribution. For example\\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\\nto reproduce that algorithm.\\n(b) If the contribution is primarily a new model architecture, the paper should describe\\nthe architecture clearly and fully.\\n(c) If the contribution is a new model (e.g., a large language model), then there should\\neither be a way to access this model for reproducing the results or a way to reproduce\\nthe model (e.g., with an open-source dataset or instructions for how to construct\\nthe dataset).\\n(d) We recognize that reproducibility may be tricky in some cases, in which case\\nauthors are welcome to describe the particular way they provide for reproducibility.\\nIn the case of closed-source models, it may be that access to the model is limited in\\nsome way (e.g., to registered users), but it should be possible for other researchers\\nto have some path to reproducing or verifying the results.\\n5. Open access to data and code\\n52\\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\\ntions to faithfully reproduce the main experimental results, as described in supplemental\\nmaterial?\\nAnswer: [Yes]\\nJustification: The code is here.\\nGuidelines:\\n\\u2022 The answer NA means that paper does not include experiments requiring code.\\n\\u2022 Please see the NeurIPS code and data submission guidelines ( https://nips.cc/\\npublic/guides/CodeSubmissionPolicy) for more details.\\n\\u2022 While we encourage the release of code and data, we understand that this might not be\\npossible, so \\u201cNo\\u201d is an acceptable answer. Papers cannot be rejected simply for not\\nincluding code, unless this is central to the contribution (e.g., for a new open-source\\nbenchmark).\\n\\u2022 The instructions should contain the exact command and environment needed to run to\\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\\n\\u2022 The authors should provide instructions on data access and preparation, including how\\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\\n\\u2022 The authors should provide scripts to reproduce all experimental results for the new\\nproposed method and baselines. If only a subset of experiments are reproducible, they\\nshould state which ones are omitted from the script and why.\\n\\u2022 At submission time, to preserve anonymity, the authors should release anonymized\\nversions (if applicable).\\n\\u2022 Providing as much information as possible in supplemental material (appended to the\\npaper) is recommended, but including URLs to data and code is permitted.\\n6. Experimental Setting/Details\\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\\nresults?\\nAnswer: [Yes]\\nJustification: We explain our experiment setting clearly.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not include experiments.\\n\\u2022 The experimental setting should be presented in the core of the paper to a level of detail\\nthat is necessary to appreciate the results and make sense of them.\\n\\u2022 The full details can be provided either with the code, in appendix, or as supplemental\\nmaterial.\\n7. Experiment Statistical Significance\\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\\ninformation about the statistical significance of the experiments?\\nAnswer: [Yes]\\nJustification: See Appendix E.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not include experiments.\\n\\u2022 The authors should answer \\\"Yes\\\" if the results are accompanied by error bars, confi-\\ndence intervals, or statistical significance tests, at least for the experiments that support\\nthe main claims of the paper.\\n\\u2022 The factors of variability that the error bars are capturing should be clearly stated (for\\nexample, train/test split, initialization, random drawing of some parameter, or overall\\nrun with given experimental conditions).\\n\\u2022 The method for calculating the error bars should be explained (closed form formula,\\ncall to a library function, bootstrap, etc.)\\n53\\n\\u2022 The assumptions made should be given (e.g., Normally distributed errors).\\n\\u2022 It should be clear whether the error bar is the standard deviation or the standard error\\nof the mean.\\n\\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should\\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\\nof Normality of errors is not verified.\\n\\u2022 For asymmetric distributions, the authors should be careful not to show in tables or\\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\\nerror rates).\\n\\u2022 If error bars are reported in tables or plots, The authors should explain in the text how\\nthey were calculated and reference the corresponding figures or tables in the text.\\n8. Experiments Compute Resources\\nQuestion: For each experiment, does the paper provide sufficient information on the com-\\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\\nthe experiments?\\nAnswer: [Yes]\\nJustification: Our work does not need to train models and only needs to conduct model\\ninference. For the closed-source LLMs (e.g., GPT-4), we directly call the OpenAI APIs. For\\nthe open-source LLMs (e.g., Llama-7B), we conduct model inference in a NVIDIA RTX\\nA6000.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not include experiments.\\n\\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,\\nor cloud provider, including relevant memory and storage.\\n\\u2022 The paper should provide the amount of compute required for each of the individual\\nexperimental runs as well as estimate the total compute.\\n\\u2022 The paper should disclose whether the full research project required more compute\\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\\ndidn\\u2019t make it into the paper).\\n9. Code Of Ethics\\nQuestion: Does the research conducted in the paper conform, in every respect, with the\\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\\nAnswer: [Yes]\\nJustification: We thoroughly discussed the potential impact of our work in Appendix B, and\\nensured the compliance with the NeurIPS code of ethics.\\nGuidelines:\\n\\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\\n\\u2022 If the authors answer No, they should explain the special circumstances that require a\\ndeviation from the Code of Ethics.\\n\\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid-\\neration due to laws or regulations in their jurisdiction).\\n10. Broader Impacts\\nQuestion: Does the paper discuss both potential positive societal impacts and negative\\nsocietal impacts of the work performed?\\nAnswer: [Yes]\\nJustification: We thoroughly discussed the potential impact of our work in Appendix B.\\nGuidelines:\\n\\u2022 The answer NA means that there is no societal impact of the work performed.\\n\\u2022 If the authors answer NA or No, they should explain why their work has no societal\\nimpact or why the paper does not address societal impact.\\n54\\n\\u2022 Examples of negative societal impacts include potential malicious or unintended uses\\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\\ngroups), privacy considerations, and security considerations.\\n\\u2022 The conference expects that many papers will be foundational research and not tied\\nto particular applications, let alone deployments. However, if there is a direct path to\\nany negative applications, the authors should point it out. For example, it is legitimate\\nto point out that an improvement in the quality of generative models could be used to\\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\\nthat a generic algorithm for optimizing neural networks could enable people to train\\nmodels that generate Deepfakes faster.\\n\\u2022 The authors should consider possible harms that could arise when the technology is\\nbeing used as intended and functioning correctly, harms that could arise when the\\ntechnology is being used as intended but gives incorrect results, and harms following\\nfrom (intentional or unintentional) misuse of the technology.\\n\\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation\\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\\nfeedback over time, improving the efficiency and accessibility of ML).\\n11. Safeguards\\nQuestion: Does the paper describe safeguards that have been put in place for responsible\\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\\nimage generators, or scraped datasets)?\\nAnswer: [NA]\\nJustification: Our data or models don\\u2019t have risk for misuse.\\nGuidelines:\\n\\u2022 The answer NA means that the paper poses no such risks.\\n\\u2022 Released models that have a high risk for misuse or dual-use should be released with\\nnecessary safeguards to allow for controlled use of the model, for example by requiring\\nthat users adhere to usage guidelines or restrictions to access the model or implementing\\nsafety filters.\\n\\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors\\nshould describe how they avoided releasing unsafe images.\\n\\u2022 We recognize that providing effective safeguards is challenging, and many papers do\\nnot require this, but we encourage authors to take this into account and make a best\\nfaith effort.\\n12. Licenses for existing assets\\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\\nproperly respected?\\nAnswer: [Yes]\\nJustification: We have properly credited the original owners of assets.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not use existing assets.\\n\\u2022 The authors should cite the original paper that produced the code package or dataset.\\n\\u2022 The authors should state which version of the asset is used and, if possible, include a\\nURL.\\n\\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.\\n\\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of\\nservice of that source should be provided.\\n\\u2022 If assets are released, the license, copyright information, and terms of use in the\\npackage should be provided. For popular datasets, paperswithcode.com/datasets\\nhas curated licenses for some datasets. Their licensing guide can help determine the\\nlicense of a dataset.\\n55\\n\\u2022 For existing datasets that are re-packaged, both the original license and the license of\\nthe derived asset (if it has changed) should be provided.\\n\\u2022 If this information is not available online, the authors are encouraged to reach out to\\nthe asset\\u2019s creators.\\n13. New Assets\\nQuestion: Are new assets introduced in the paper well documented and is the documentation\\nprovided alongside the assets?\\nAnswer: [Yes]\\nJustification: The code along with the documentation is here.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not release new assets.\\n\\u2022 Researchers should communicate the details of the dataset/code/model as part of their\\nsubmissions via structured templates. This includes details about training, license,\\nlimitations, etc.\\n\\u2022 The paper should discuss whether and how consent was obtained from people whose\\nasset is used.\\n\\u2022 At submission time, remember to anonymize your assets (if applicable). You can either\\ncreate an anonymized URL or include an anonymized zip file.\\n14. Crowdsourcing and Research with Human Subjects\\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\\ninclude the full text of instructions given to participants and screenshots, if applicable, as\\nwell as details about compensation (if any)?\\nAnswer: [NA]\\nJustification: Our paper doesn\\u2019t include this kind of experiment.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with\\nhuman subjects.\\n\\u2022 Including this information in the supplemental material is fine, but if the main contribu-\\ntion of the paper involves human subjects, then as much detail as possible should be\\nincluded in the main paper.\\n\\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\\nor other labor should be paid at least the minimum wage in the country of the data\\ncollector.\\n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\\nSubjects\\nQuestion: Does the paper describe potential risks incurred by study participants, whether\\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\\napprovals (or an equivalent approval/review based on the requirements of your country or\\ninstitution) were obtained?\\nAnswer: [NA]\\nJustification: Our paper doesn\\u2019t include this kind of experiment.\\nGuidelines:\\n\\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with\\nhuman subjects.\\n\\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)\\nmay be required for any human subjects research. If you obtained IRB approval, you\\nshould clearly state this in the paper.\\n\\u2022 We recognize that the procedures for this may vary significantly between institutions\\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\\nguidelines for their institution.\\n\\u2022 For initial submissions, do not include any information that would break anonymity (if\\napplicable), such as the institution conducting the review.\\n56\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ENTITY_PATTERNS = {\n",
        "    \"METHOD\": r\"Method\\s+[A-Z][A-Za-z0-9]*\",\n",
        "    \"AUTHOR\": r\"Author\\s+[A-Z]\",\n",
        "    \"DATASET\": r\"Dataset\\s+[A-Z0-9]+\",\n",
        "    \"PAPER\": r\"Paper\\s+[A-Z0-9]+|Survey\\s+[A-Z0-9]+\",\n",
        "    \"METRIC\": r\"F1|Accuracy|AUC\"\n",
        "}\n",
        "\n",
        "def extract_entities(text: str):\n",
        "    ents = []\n",
        "    for typ, pat in ENTITY_PATTERNS.items():\n",
        "        for m in re.finditer(pat, text):\n",
        "            ents.append((m.group(0), typ, m.start(), m.end()))\n",
        "    return ents\n",
        "\n",
        "rows = []\n",
        "for _, r in corpus.iterrows():\n",
        "    ents = extract_entities(r[\"text\"])\n",
        "    for e, typ, s, t in ents:\n",
        "        rows.append({\n",
        "            \"doc_id\": r[\"doc_id\"],\n",
        "            \"entity\": e,\n",
        "            \"type\": typ,\n",
        "            \"start\": s,\n",
        "            \"end\": t,\n",
        "            \"span\": r[\"text\"][max(0,s-40):min(len(r['text']),t+40)]\n",
        "        })\n",
        "\n",
        "ents_df = pd.DataFrame(rows)\n",
        "print(\"✅ Entities extracted:\", len(ents_df))\n",
        "display(ents_df.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "nk7QDiB4Ydvp",
        "outputId": "c8f83e03-07c7-4db3-8c5c-cb6aca8f71a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Entities extracted: 38\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                              doc_id            entity  \\\n",
              "0  NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method MDAgents   \n",
              "1  NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method\\nMedical   \n",
              "2  NeurIPS-2024-mdagents-an-adaptive-collaboratio...  Method\\nClinical   \n",
              "3  NeurIPS-2024-mdagents-an-adaptive-collaboratio...        Method Avg   \n",
              "4  NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method Accuracy   \n",
              "5  NeurIPS-2024-mdagents-an-adaptive-collaboratio...      Method MedQA   \n",
              "6  NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method Accuracy   \n",
              "7  NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method\\nMedical   \n",
              "8  NeurIPS-2024-mdagents-an-adaptive-collaboratio...  Method\\nClinical   \n",
              "9  NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method\\nMedical   \n",
              "\n",
              "     type  start    end                                               span  \n",
              "0  METHOD   8056   8071   key dimensions of LLM decision-making.\\nMetho...  \n",
              "1  METHOD  23395  23409  ted in Table 9-12 in\\nAppendix.\\nCategory Meth...  \n",
              "2  METHOD  24504  24519   65.3 ±3.9 56.4 ±4.5 56.2 ±6.7\\nCategory Metho...  \n",
              "3  METHOD  37515  37525  atively less complex medical knowledge.\\nMetho...  \n",
              "4  METHOD  84088  84103  ons dataset with GPT-4o mini\\n.\\nCategory Meth...  \n",
              "5  METHOD  84986  84998   test sets with GPT-4 (Vision) (in USD)\\nMetho...  \n",
              "6  METHOD  85503  85518  mpact of Knowledge Enhancement with RAG\\nMetho...  \n",
              "7  METHOD  87670  87684   for each benchmark and model.\\nCategory Metho...  \n",
              "8  METHOD  88697  88712   55.0 ±2.0 41.8 ±3.0 44.5 ±2.0\\nCategory Metho...  \n",
              "9  METHOD  90188  90202   for each benchmark and model.\\nCategory Metho...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-79dba4fa-9bcd-48eb-82c7-3db8951d58fe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>entity</th>\n",
              "      <th>type</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>span</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method MDAgents</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>8056</td>\n",
              "      <td>8071</td>\n",
              "      <td>key dimensions of LLM decision-making.\\nMetho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nMedical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>23395</td>\n",
              "      <td>23409</td>\n",
              "      <td>ted in Table 9-12 in\\nAppendix.\\nCategory Meth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nClinical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>24504</td>\n",
              "      <td>24519</td>\n",
              "      <td>65.3 ±3.9 56.4 ±4.5 56.2 ±6.7\\nCategory Metho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method Avg</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>37515</td>\n",
              "      <td>37525</td>\n",
              "      <td>atively less complex medical knowledge.\\nMetho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method Accuracy</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>84088</td>\n",
              "      <td>84103</td>\n",
              "      <td>ons dataset with GPT-4o mini\\n.\\nCategory Meth...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method MedQA</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>84986</td>\n",
              "      <td>84998</td>\n",
              "      <td>test sets with GPT-4 (Vision) (in USD)\\nMetho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method Accuracy</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>85503</td>\n",
              "      <td>85518</td>\n",
              "      <td>mpact of Knowledge Enhancement with RAG\\nMetho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nMedical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>87670</td>\n",
              "      <td>87684</td>\n",
              "      <td>for each benchmark and model.\\nCategory Metho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nClinical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>88697</td>\n",
              "      <td>88712</td>\n",
              "      <td>55.0 ±2.0 41.8 ±3.0 44.5 ±2.0\\nCategory Metho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nMedical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>90188</td>\n",
              "      <td>90202</td>\n",
              "      <td>for each benchmark and model.\\nCategory Metho...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79dba4fa-9bcd-48eb-82c7-3db8951d58fe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-79dba4fa-9bcd-48eb-82c7-3db8951d58fe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-79dba4fa-9bcd-48eb-82c7-3db8951d58fe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e6296987-480a-47e6-989b-175cdf4ff0c9\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e6296987-480a-47e6-989b-175cdf4ff0c9')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e6296987-480a-47e6-989b-175cdf4ff0c9 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(ents_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"doc_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"entity\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Method MDAgents\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"METHOD\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 33562,\n        \"min\": 8056,\n        \"max\": 90188,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          88697\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 33562,\n        \"min\": 8071,\n        \"max\": 90202,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          88712\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"span\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \" for each benchmark and model.\\nCategory Method\\nMedical Knowledge Retrieval Datasets\\nMedQA\\nT\\nPu\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_split(text):\n",
        "    return re.split(r\"(?<=[.!?])\\s+\", text)\n",
        "\n",
        "edges = []\n",
        "for _, r in corpus.iterrows():\n",
        "    for sent in sentence_split(r[\"text\"]):\n",
        "        sent = sent.strip()\n",
        "        if not sent:\n",
        "            continue\n",
        "        ents = extract_entities(sent)\n",
        "        for i in range(len(ents)):\n",
        "            for j in range(i+1, len(ents)):\n",
        "                e1, t1, *_ = ents[i]\n",
        "                e2, t2, *_ = ents[j]\n",
        "                edges.append({\n",
        "                    \"doc_id\": r[\"doc_id\"],\n",
        "                    \"head\": e1, \"type1\": t1,\n",
        "                    \"tail\": e2, \"type2\": t2,\n",
        "                    \"sentence\": sent\n",
        "                })\n",
        "\n",
        "edges_df = pd.DataFrame(edges)\n",
        "print(\"✅ Relations extracted:\", len(edges_df))\n",
        "display(edges_df.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "NY8Shgn6YyOC",
        "outputId": "0f23267d-a1a6-4cff-8526-15be9a918447"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Relations extracted: 14\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                              doc_id              head  \\\n",
              "0  NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method\\nMedical   \n",
              "1  NeurIPS-2024-mdagents-an-adaptive-collaboratio...        Dataset\\nT   \n",
              "2  NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method Accuracy   \n",
              "3  NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method Accuracy   \n",
              "4  NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method\\nMedical   \n",
              "5  NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method\\nMedical   \n",
              "6  NeurIPS-2024-mdagents-an-adaptive-collaboratio...  Method\\nClinical   \n",
              "7  NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method\\nMedical   \n",
              "8  NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method\\nMedical   \n",
              "9  NeurIPS-2024-mdagents-an-adaptive-collaboratio...  Method\\nClinical   \n",
              "\n",
              "     type1              tail   type2  \\\n",
              "0   METHOD  Method\\nClinical  METHOD   \n",
              "1  DATASET          Accuracy  METRIC   \n",
              "2   METHOD          Accuracy  METRIC   \n",
              "3   METHOD          Accuracy  METRIC   \n",
              "4   METHOD  Method\\nClinical  METHOD   \n",
              "5   METHOD          Accuracy  METRIC   \n",
              "6   METHOD          Accuracy  METRIC   \n",
              "7   METHOD  Method\\nClinical  METHOD   \n",
              "8   METHOD          Accuracy  METRIC   \n",
              "9   METHOD          Accuracy  METRIC   \n",
              "\n",
              "                                            sentence  \n",
              "0  Category Method\\nMedical Knowledge Retrieval D...  \n",
              "1  Report Generation\\nD.1 Accuracy on entire MedQ...  \n",
              "2  Category Method Accuracy (%)\\nSingle-agent\\nZe...  \n",
              "3  23\\nD.3 Impact of Knowledge Enhancement with R...  \n",
              "4  Category Method\\nMedical Knowledge Retrieval D...  \n",
              "5  Category Method\\nMedical Knowledge Retrieval D...  \n",
              "6  Category Method\\nMedical Knowledge Retrieval D...  \n",
              "7  Category Method\\nMedical Knowledge Retrieval D...  \n",
              "8  Category Method\\nMedical Knowledge Retrieval D...  \n",
              "9  Category Method\\nMedical Knowledge Retrieval D...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9240dddd-c928-493b-911a-5a055c88c13e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>head</th>\n",
              "      <th>type1</th>\n",
              "      <th>tail</th>\n",
              "      <th>type2</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nMedical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Method\\nClinical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Category Method\\nMedical Knowledge Retrieval D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Dataset\\nT</td>\n",
              "      <td>DATASET</td>\n",
              "      <td>Accuracy</td>\n",
              "      <td>METRIC</td>\n",
              "      <td>Report Generation\\nD.1 Accuracy on entire MedQ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method Accuracy</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Accuracy</td>\n",
              "      <td>METRIC</td>\n",
              "      <td>Category Method Accuracy (%)\\nSingle-agent\\nZe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method Accuracy</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Accuracy</td>\n",
              "      <td>METRIC</td>\n",
              "      <td>23\\nD.3 Impact of Knowledge Enhancement with R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nMedical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Method\\nClinical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Category Method\\nMedical Knowledge Retrieval D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nMedical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Accuracy</td>\n",
              "      <td>METRIC</td>\n",
              "      <td>Category Method\\nMedical Knowledge Retrieval D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nClinical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Accuracy</td>\n",
              "      <td>METRIC</td>\n",
              "      <td>Category Method\\nMedical Knowledge Retrieval D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nMedical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Method\\nClinical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Category Method\\nMedical Knowledge Retrieval D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nMedical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Accuracy</td>\n",
              "      <td>METRIC</td>\n",
              "      <td>Category Method\\nMedical Knowledge Retrieval D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nClinical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Accuracy</td>\n",
              "      <td>METRIC</td>\n",
              "      <td>Category Method\\nMedical Knowledge Retrieval D...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9240dddd-c928-493b-911a-5a055c88c13e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9240dddd-c928-493b-911a-5a055c88c13e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9240dddd-c928-493b-911a-5a055c88c13e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-80706993-d078-4e6a-892f-c1d0b4e27d90\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-80706993-d078-4e6a-892f-c1d0b4e27d90')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-80706993-d078-4e6a-892f-c1d0b4e27d90 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(edges_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"doc_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"head\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Dataset\\nT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type1\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"DATASET\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tail\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Accuracy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type2\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"METRIC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Category Method\\nMedical Knowledge Retrieval Datasets\\nMedQA\\nT\\nPubMedQA\\nT\\nPath-VQA\\nI T\\nPMC-VQA\\nI T\\nMedVidQA\\nV T\\nSingle-agent\\nZero-shot 75.0 \\u00b11.3 61.5 \\u00b12.2 57.9 \\u00b11.6 49.0 \\u00b13.7 37.9 \\u00b18.4\\nFew-shot 72.9 \\u00b111.4 63.1 \\u00b111.7 57.5 \\u00b14.5 52.2 \\u00b12.0 47.1 \\u00b18.6\\n+ CoT [83] 82.5 \\u00b14.9 57.6 \\u00b19.2 58.6 \\u00b13.1 51.3 \\u00b11.5 48.6 \\u00b15.5\\n+ CoT-SC [82] 83.9 \\u00b12.7 58.7 \\u00b15.0 61.2 \\u00b12.1 50.5 \\u00b15.2 49.2 \\u00b18.2\\nER [67] 81.9 \\u00b12.1 56.0 \\u00b17.0 61.4 \\u00b14.1 52.7 \\u00b12.9 48.5 \\u00b14.1\\nMedprompt [59] 82.4 \\u00b15.1 51.8 \\u00b14.6 59.2 \\u00b15.7 53.4 \\u00b17.9 44.5 \\u00b12.0\\nMulti-agent\\n(Single-model)\\nMajority V oting 80.6 \\u00b12.9 72.2 \\u00b16.9 56.9 \\u00b119.7 36.8 \\u00b16.7 50.8 \\u00b17.4\\nWeighted V oting 78.8 \\u00b11.1 72.2 \\u00b16.9 62.1 \\u00b113.9 25.4 \\u00b19.0 57.8 \\u00b12.1\\nBorda Count 70.3 \\u00b18.5 66.9 \\u00b13.0 61.9 \\u00b18.1 27.9 \\u00b15.3 54.5 \\u00b14.7\\nMedAgents [72] 79.1 \\u00b17.4 69.7 \\u00b14.7 45.4 \\u00b18.1 39.6 \\u00b13.0 51.6 \\u00b14.8\\nMeta-Prompting [70] 80.6 \\u00b11.2 73.3 \\u00b12.3 55.3 \\u00b12.3 42.6 \\u00b14.2 -\\nMulti-agent\\n(Multi-model)\\nReconcile [10] 81.3 \\u00b13.0 79.7 \\u00b13.2 57.5 \\u00b13.3 31.4 \\u00b11.2 -\\nAutoGen [86] 60.6 \\u00b15.0 77.3 \\u00b12.3 43.0 \\u00b18.9 37.3 \\u00b16.1 -\\nDyLAN [51] 64.2 \\u00b12.3 73.6 \\u00b14.2 41.3 \\u00b11.2 34.0 \\u00b13.5 -\\nAdaptive MDAgents (Ours) 88.7\\u00b14.0 75.0 \\u00b11.0 65.3 \\u00b13.9 56.4 \\u00b14.5 56.2 \\u00b16.7\\nCategory Method\\nClinical Reasoning and Diagnostic Datasets\\nDDXPlus\\nT\\nSymCat\\nT\\nJAMA\\nT\\nMedBullets\\nT\\nMIMIC-CXR\\nI T\\nSingle-agent\\nZero-shot 70.3 \\u00b12.0 88.7 \\u00b12.3 62.0 \\u00b12.0 67.0 \\u00b11.4 40.0 \\u00b15.3\\nFew-shot 69.4 \\u00b11.0 86.7 \\u00b13.1 69.0 \\u00b14.2 72.0 \\u00b12.8 35.3 \\u00b15.0\\n+ CoT [83] 72.7 \\u00b17.7 78.0 \\u00b12.0 66.0 \\u00b15.7 70.0 \\u00b10.0 36.2 \\u00b15.2\\n+ CoT-SC [82] 52.1 \\u00b16.4 83.3 \\u00b13.1 68.0 \\u00b12.8 76.0 \\u00b12.8 51.7 \\u00b14.0\\nER [67] 61.3 \\u00b12.4 82.7 \\u00b12.3 71.0 \\u00b11.4 76.0 \\u00b15.7 50.0 \\u00b10.0\\nMedprompt [59] 59.5 \\u00b117.7 87.3 \\u00b11.2 70.7 \\u00b14.3 71.0 \\u00b11.4 53.4 \\u00b14.3\\nMulti-agent\\n(Single-model)\\nMajority V oting 67.8 \\u00b14.9 91.9 \\u00b12.2 70.0 \\u00b15.7 70.0 \\u00b10.0 49.5 \\u00b110.7\\nWeighted V oting 65.9 \\u00b13.3 90.5 \\u00b12.9 66.1 \\u00b14.1 66.0 \\u00b15.7 53.5 \\u00b12.2\\nBorda Count 67.1 \\u00b16.7 78.0 \\u00b111.8 61.0 \\u00b15.6 66.0 \\u00b12.8 45.3 \\u00b16.8\\nMedAgents [72] 62.8 \\u00b15.6 90.0 \\u00b10.0 66.0 \\u00b15.7 77.0 \\u00b11.4 43.3 \\u00b17.0\\nMeta-Prompting [70] 52.6 \\u00b16.1 77.3 \\u00b12.3 64.7 \\u00b13.1 49.3 \\u00b11.2 42.0 \\u00b14.0\\nMulti-agent\\n(Multi-model)\\nReconcile [10] 68.4 \\u00b17.4 90.6 \\u00b12.5 60.7 \\u00b15.7 59.5 \\u00b18.7 33.3 \\u00b13.4\\nAutoGen [86] 67.3 \\u00b111.8 73.3 \\u00b13.1 64.6 \\u00b11.2 55.3 \\u00b13.1 43.3 \\u00b14.2\\nDyLAN [51] 56.4 \\u00b12.9 75.3 \\u00b14.6 60.1 \\u00b13.1 57.3 \\u00b16.1 38.7 \\u00b11.2\\nAdaptive MDAgents (Ours) 77.9 \\u00b12.1 93.1 \\u00b11.0 70.9 \\u00b10.3 80.8 \\u00b11.7 55.9 \\u00b19.1\\n* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement\\n* T : text-only, I : image+text, V : video+text\\n* All experiments were tested with 3 random seeds\\n4 Experiments and Results\\nIn this section, we evaluate our framework and baseline methods across different medical benchmarks\\nin Solo, Group, and Aaptive settings.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.Graph()\n",
        "\n",
        "for _, e in ents_df.iterrows():\n",
        "    G.add_node(e[\"entity\"], type=e[\"type\"])\n",
        "\n",
        "for _, ed in edges_df.iterrows():\n",
        "    G.add_edge(ed[\"head\"], ed[\"tail\"], doc_id=ed[\"doc_id\"], sentence=ed[\"sentence\"])\n",
        "\n",
        "print(f\"✅ Graph built: {len(G.nodes())} nodes | {len(G.edges())} edges\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8ypOe1IY2o5",
        "outputId": "b03e5866-a200-4f2a-bd94-3ad4fa2286ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Graph built: 14 nodes | 6 edges\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_seed_entities(query: str):\n",
        "    seeds = []\n",
        "    qlow = query.lower()\n",
        "    for node in G.nodes():\n",
        "        if node.lower().split()[-1] in qlow:\n",
        "            seeds.append(node)\n",
        "    for node, data in G.nodes(data=True):\n",
        "        if data.get(\"type\") in (\"METHOD\", \"DATASET\") and data[\"type\"].lower() in qlow:\n",
        "            seeds.append(node)\n",
        "    return list(dict.fromkeys(seeds))\n",
        "\n",
        "def neighborhood_evidence(seeds, hops=1, max_spans=12):\n",
        "    spans, seen_edges = [], set()\n",
        "    for s in seeds:\n",
        "        if s not in G:\n",
        "            continue\n",
        "        nodes = nx.single_source_shortest_path_length(G, s, cutoff=hops).keys()\n",
        "        for u in nodes:\n",
        "            for v in G.neighbors(u):\n",
        "                e = tuple(sorted([u, v]))\n",
        "                if e in seen_edges:\n",
        "                    continue\n",
        "                seen_edges.add(e)\n",
        "                data = G.get_edge_data(u, v)\n",
        "                spans.append({\n",
        "                    \"u\": u, \"v\": v,\n",
        "                    \"doc_id\": data.get(\"doc_id\"),\n",
        "                    \"sentence\": data.get(\"sentence\")\n",
        "                })\n",
        "                if len(spans) >= max_spans:\n",
        "                    return spans\n",
        "    return spans\n",
        "\n",
        "def graph_rag(query: str, hops=1):\n",
        "    seeds = detect_seed_entities(query)\n",
        "    spans = neighborhood_evidence(seeds, hops=hops)\n",
        "    return {\"seeds\": seeds, \"spans\": spans}\n",
        "\n",
        "demo_out = graph_rag(\"Which dataset evaluated Method X with F1?\", hops=1)\n",
        "demo_out\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LgmvS8dY5Jz",
        "outputId": "1f900417-0e9a-417c-ad18-29c729d112b1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'seeds': ['Dataset I',\n",
              "  'Dataset M',\n",
              "  'Dataset\\nT',\n",
              "  'Paper C',\n",
              "  'Method MDAgents',\n",
              "  'Method\\nMedical',\n",
              "  'Method\\nClinical',\n",
              "  'Method Avg',\n",
              "  'Method Accuracy',\n",
              "  'Method MedQA',\n",
              "  'Method DDXPlus',\n",
              "  'Dataset\\nN'],\n",
              " 'spans': [{'u': 'Dataset\\nT',\n",
              "   'v': 'Accuracy',\n",
              "   'doc_id': 'NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference',\n",
              "   'sentence': 'Report Generation\\nD.1 Accuracy on entire MedQA 5-options Dataset\\nTo provide a comprehensive evaluation of our approach, we conducted experiments on the entire\\nMedQA 5-options dataset using GPT-4o mini.'},\n",
              "  {'u': 'Accuracy',\n",
              "   'v': 'Method Accuracy',\n",
              "   'doc_id': 'NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference',\n",
              "   'sentence': '23\\nD.3 Impact of Knowledge Enhancement with RAG\\nMethod Accuracy (%)\\nMDAgents (baseline) 71.8\\n+ MedRAG 75.2\\n+ Medical Knowledge Initialization 76.0\\n+ Moderator’s Review 77.6\\n+ Moderator’s Review & MedRAG 80.3\\nTable 7: Impact of knowledge enhancement on MDAgents performance\\nWe investigated whether simply assigning roles to agents is sufficient for expert-like performance,\\nand explored the impact of equipping agents with different knowledge using Retrieval-Augmented\\nGeneration (RAG).'},\n",
              "  {'u': 'Accuracy',\n",
              "   'v': 'Method\\nMedical',\n",
              "   'doc_id': 'NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference',\n",
              "   'sentence': 'Method\\nMedical Knowledge Retrieval Datasets\\nMedQA\\nT\\nPubMedQA\\nT\\nPath-VQA\\nI T\\nPMC-VQA\\nI T\\nMedVidQA\\nV T\\nGPT-3.5 64.0 ±1.6 66.0 ±5.7 - - -\\nGPT-4(V) 88.7 ±4.0 75.0 ±1.0 65.3 ±3.9 56.4 ±4.5 -\\nGemini-Pro(Vision) 57.4 ±1.8 71.0 ±1.6 72.0 ±2.3 62.2 ±7.6 56.2 ±6.7\\nMethod\\nClinical Reasoning and Diagnostic Datasets\\nDDXPlus\\nT\\nSymCat\\nT\\nJAMA\\nT\\nMedBullets\\nT\\nMIMIC-CXR\\nI T\\nGPT-3.5 62.5 ±6.7 85.7 ±3.0 48.6 ±6.5 55.3 ±4.3 -\\nGPT-4(V) 77.9 ±2.1 93.1 ±1.0 70.9 ±0.3 80.8 ±1.7 55.9 ±9.1\\nGemini-Pro(Vision) 59.2 ±1.2 65.0 ±6.1 47.0 ±2.2 42.9 ±3.4 48.1 ±5.8\\n* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement\\n* T : text-only, I : image+text, V : video+text\\n* All experiments were tested with 3 random seeds\\n26\\nTable 12: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive settings with increased\\nnumber of samples (N=100).'},\n",
              "  {'u': 'Accuracy',\n",
              "   'v': 'Method\\nClinical',\n",
              "   'doc_id': 'NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference',\n",
              "   'sentence': 'Method\\nMedical Knowledge Retrieval Datasets\\nMedQA\\nT\\nPubMedQA\\nT\\nPath-VQA\\nI T\\nPMC-VQA\\nI T\\nMedVidQA\\nV T\\nGPT-3.5 64.0 ±1.6 66.0 ±5.7 - - -\\nGPT-4(V) 88.7 ±4.0 75.0 ±1.0 65.3 ±3.9 56.4 ±4.5 -\\nGemini-Pro(Vision) 57.4 ±1.8 71.0 ±1.6 72.0 ±2.3 62.2 ±7.6 56.2 ±6.7\\nMethod\\nClinical Reasoning and Diagnostic Datasets\\nDDXPlus\\nT\\nSymCat\\nT\\nJAMA\\nT\\nMedBullets\\nT\\nMIMIC-CXR\\nI T\\nGPT-3.5 62.5 ±6.7 85.7 ±3.0 48.6 ±6.5 55.3 ±4.3 -\\nGPT-4(V) 77.9 ±2.1 93.1 ±1.0 70.9 ±0.3 80.8 ±1.7 55.9 ±9.1\\nGemini-Pro(Vision) 59.2 ±1.2 65.0 ±6.1 47.0 ±2.2 42.9 ±3.4 48.1 ±5.8\\n* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement\\n* T : text-only, I : image+text, V : video+text\\n* All experiments were tested with 3 random seeds\\n26\\nTable 12: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive settings with increased\\nnumber of samples (N=100).'},\n",
              "  {'u': 'Method\\nMedical',\n",
              "   'v': 'Method\\nClinical',\n",
              "   'doc_id': 'NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference',\n",
              "   'sentence': 'Method\\nMedical Knowledge Retrieval Datasets\\nMedQA\\nT\\nPubMedQA\\nT\\nPath-VQA\\nI T\\nPMC-VQA\\nI T\\nMedVidQA\\nV T\\nGPT-3.5 64.0 ±1.6 66.0 ±5.7 - - -\\nGPT-4(V) 88.7 ±4.0 75.0 ±1.0 65.3 ±3.9 56.4 ±4.5 -\\nGemini-Pro(Vision) 57.4 ±1.8 71.0 ±1.6 72.0 ±2.3 62.2 ±7.6 56.2 ±6.7\\nMethod\\nClinical Reasoning and Diagnostic Datasets\\nDDXPlus\\nT\\nSymCat\\nT\\nJAMA\\nT\\nMedBullets\\nT\\nMIMIC-CXR\\nI T\\nGPT-3.5 62.5 ±6.7 85.7 ±3.0 48.6 ±6.5 55.3 ±4.3 -\\nGPT-4(V) 77.9 ±2.1 93.1 ±1.0 70.9 ±0.3 80.8 ±1.7 55.9 ±9.1\\nGemini-Pro(Vision) 59.2 ±1.2 65.0 ±6.1 47.0 ±2.2 42.9 ±3.4 48.1 ±5.8\\n* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement\\n* T : text-only, I : image+text, V : video+text\\n* All experiments were tested with 3 random seeds\\n26\\nTable 12: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive settings with increased\\nnumber of samples (N=100).'},\n",
              "  {'u': 'Method MedQA',\n",
              "   'v': 'Method DDXPlus',\n",
              "   'doc_id': 'NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference',\n",
              "   'sentence': 'Category Method MedQA\\nT\\nPubMedQA\\nT\\nPath-VQA\\nI T\\nPMC-VQA\\nI T\\nMedVidQA\\nV T\\nSingle-agent\\nZero-shot 75.0 54.0 58.0 48.0 50.0\\nFew-shot 77.0 55.0 58.0 50.0 51.0\\n+ CoT 78.0 50.0 59.0 52.0 53.0\\n+ CoT-SC 79.0 51.0 60.0 53.0 53.0\\nER 76.0 51.0 61.0 51.0 52.0\\nMedprompt 79.0 58.0 60.0 54.0 53.0\\nMulti-agent\\n(Single-model)\\nMajority V oting 79.0 68.0 63.0 52.0 54.0\\nWeighted V oting 80.0 68.0 64.0 51.0 55.0\\nBorda Count 81.0 69.0 62.0 50.0 52.0\\nMedAgents 80.0 69.0 55.0 52.0 50.0\\nMeta-Prompting 82.0 69.0 56.0 49.0 -\\nMulti-agent\\n(Multi-model)\\nReconcile 83.0 70.0 58.0 45.0 -\\nAutoGen 65.0 63.0 45.0 40.0 -\\nDyLAN 68.0 67.0 42.0 48.0 -\\nAdaptive MDAgents (Ours) 87.0 71.0 60.0 55.0 56.0\\nCategory Method DDXPlus\\nT\\nSymCat\\nT\\nJAMA\\nT\\nMedBullets\\nT\\nMIMIC-CXR\\nI T\\nSingle-agent\\nZero-shot 53.0 84.0 57.0 49.0 38.0\\nFew-shot 60.0 87.0 58.0 52.0 33.0\\n+ CoT 66.0 84.0 55.0 64.0 33.0\\n+ CoT-SC 68.0 84.0 57.0 60.0 40.0\\nER 76.0 80.0 56.0 59.0 43.0\\nMedprompt 70.0 84.0 62.0 60.0 43.0\\nMulti-agent\\n(Single-model)\\nMajority V oting 53.0 82.0 56.0 59.0 54.0\\nWeighted V oting 52.0 86.0 56.0 56.0 52.0\\nBorda Count 53.0 86.0 56.0 59.0 51.0\\nMedAgents 56.0 80.9 51.0 58.0 40.9\\nMeta-Prompting 53.0 79.0 56.0 51.0 48.0\\nMulti-agent\\n(Multi-model)\\nReconcile 60.0 87.0 59.0 60.0 43.3\\nAutoGen 47.0 87.0 53.0 55.0 47.0\\nDyLAN 54.0 84.0 55.0 57.0 42.0\\nAdaptive MDAgents (Ours) 75.0 89.0 59.0 67.0 56.0\\n* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement\\n* T : text-only, I : image+text, V : video+text\\n27\\nFigure 8: Complexity Distribution for each dataset classified by GPT-4(V) and Gemini-Pro (Vision)\\n(for MedVidQA).'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def assemble_prompt(query, seeds, spans):\n",
        "    ev_lines = [f\"- ({s['doc_id']}) {s['sentence']}\" for s in spans]\n",
        "    return f\"\"\"System: Answer using ONLY the evidence and cite (doc_id).\n",
        "Query: {query}\n",
        "\n",
        "Seeds: {', '.join(seeds) if seeds else '(none)'}\n",
        "\n",
        "Evidence:\n",
        "\"\"\" + \"\\n\".join(ev_lines) + \"\\n\\nAnswer:\"\n",
        "\n",
        "print(assemble_prompt(\"Which dataset evaluated Method X with F1?\", demo_out[\"seeds\"], demo_out[\"spans\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqWsRXh9Y64-",
        "outputId": "7a3fca42-90bd-49d1-b2c3-32805691b623"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: Answer using ONLY the evidence and cite (doc_id).\n",
            "Query: Which dataset evaluated Method X with F1?\n",
            "\n",
            "Seeds: Dataset I, Dataset M, Dataset\n",
            "T, Paper C, Method MDAgents, Method\n",
            "Medical, Method\n",
            "Clinical, Method Avg, Method Accuracy, Method MedQA, Method DDXPlus, Dataset\n",
            "N\n",
            "\n",
            "Evidence:\n",
            "- (NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference) Report Generation\n",
            "D.1 Accuracy on entire MedQA 5-options Dataset\n",
            "To provide a comprehensive evaluation of our approach, we conducted experiments on the entire\n",
            "MedQA 5-options dataset using GPT-4o mini.\n",
            "- (NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference) 23\n",
            "D.3 Impact of Knowledge Enhancement with RAG\n",
            "Method Accuracy (%)\n",
            "MDAgents (baseline) 71.8\n",
            "+ MedRAG 75.2\n",
            "+ Medical Knowledge Initialization 76.0\n",
            "+ Moderator’s Review 77.6\n",
            "+ Moderator’s Review & MedRAG 80.3\n",
            "Table 7: Impact of knowledge enhancement on MDAgents performance\n",
            "We investigated whether simply assigning roles to agents is sufficient for expert-like performance,\n",
            "and explored the impact of equipping agents with different knowledge using Retrieval-Augmented\n",
            "Generation (RAG).\n",
            "- (NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference) Method\n",
            "Medical Knowledge Retrieval Datasets\n",
            "MedQA\n",
            "T\n",
            "PubMedQA\n",
            "T\n",
            "Path-VQA\n",
            "I T\n",
            "PMC-VQA\n",
            "I T\n",
            "MedVidQA\n",
            "V T\n",
            "GPT-3.5 64.0 ±1.6 66.0 ±5.7 - - -\n",
            "GPT-4(V) 88.7 ±4.0 75.0 ±1.0 65.3 ±3.9 56.4 ±4.5 -\n",
            "Gemini-Pro(Vision) 57.4 ±1.8 71.0 ±1.6 72.0 ±2.3 62.2 ±7.6 56.2 ±6.7\n",
            "Method\n",
            "Clinical Reasoning and Diagnostic Datasets\n",
            "DDXPlus\n",
            "T\n",
            "SymCat\n",
            "T\n",
            "JAMA\n",
            "T\n",
            "MedBullets\n",
            "T\n",
            "MIMIC-CXR\n",
            "I T\n",
            "GPT-3.5 62.5 ±6.7 85.7 ±3.0 48.6 ±6.5 55.3 ±4.3 -\n",
            "GPT-4(V) 77.9 ±2.1 93.1 ±1.0 70.9 ±0.3 80.8 ±1.7 55.9 ±9.1\n",
            "Gemini-Pro(Vision) 59.2 ±1.2 65.0 ±6.1 47.0 ±2.2 42.9 ±3.4 48.1 ±5.8\n",
            "* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement\n",
            "* T : text-only, I : image+text, V : video+text\n",
            "* All experiments were tested with 3 random seeds\n",
            "26\n",
            "Table 12: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive settings with increased\n",
            "number of samples (N=100).\n",
            "- (NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference) Method\n",
            "Medical Knowledge Retrieval Datasets\n",
            "MedQA\n",
            "T\n",
            "PubMedQA\n",
            "T\n",
            "Path-VQA\n",
            "I T\n",
            "PMC-VQA\n",
            "I T\n",
            "MedVidQA\n",
            "V T\n",
            "GPT-3.5 64.0 ±1.6 66.0 ±5.7 - - -\n",
            "GPT-4(V) 88.7 ±4.0 75.0 ±1.0 65.3 ±3.9 56.4 ±4.5 -\n",
            "Gemini-Pro(Vision) 57.4 ±1.8 71.0 ±1.6 72.0 ±2.3 62.2 ±7.6 56.2 ±6.7\n",
            "Method\n",
            "Clinical Reasoning and Diagnostic Datasets\n",
            "DDXPlus\n",
            "T\n",
            "SymCat\n",
            "T\n",
            "JAMA\n",
            "T\n",
            "MedBullets\n",
            "T\n",
            "MIMIC-CXR\n",
            "I T\n",
            "GPT-3.5 62.5 ±6.7 85.7 ±3.0 48.6 ±6.5 55.3 ±4.3 -\n",
            "GPT-4(V) 77.9 ±2.1 93.1 ±1.0 70.9 ±0.3 80.8 ±1.7 55.9 ±9.1\n",
            "Gemini-Pro(Vision) 59.2 ±1.2 65.0 ±6.1 47.0 ±2.2 42.9 ±3.4 48.1 ±5.8\n",
            "* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement\n",
            "* T : text-only, I : image+text, V : video+text\n",
            "* All experiments were tested with 3 random seeds\n",
            "26\n",
            "Table 12: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive settings with increased\n",
            "number of samples (N=100).\n",
            "- (NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference) Method\n",
            "Medical Knowledge Retrieval Datasets\n",
            "MedQA\n",
            "T\n",
            "PubMedQA\n",
            "T\n",
            "Path-VQA\n",
            "I T\n",
            "PMC-VQA\n",
            "I T\n",
            "MedVidQA\n",
            "V T\n",
            "GPT-3.5 64.0 ±1.6 66.0 ±5.7 - - -\n",
            "GPT-4(V) 88.7 ±4.0 75.0 ±1.0 65.3 ±3.9 56.4 ±4.5 -\n",
            "Gemini-Pro(Vision) 57.4 ±1.8 71.0 ±1.6 72.0 ±2.3 62.2 ±7.6 56.2 ±6.7\n",
            "Method\n",
            "Clinical Reasoning and Diagnostic Datasets\n",
            "DDXPlus\n",
            "T\n",
            "SymCat\n",
            "T\n",
            "JAMA\n",
            "T\n",
            "MedBullets\n",
            "T\n",
            "MIMIC-CXR\n",
            "I T\n",
            "GPT-3.5 62.5 ±6.7 85.7 ±3.0 48.6 ±6.5 55.3 ±4.3 -\n",
            "GPT-4(V) 77.9 ±2.1 93.1 ±1.0 70.9 ±0.3 80.8 ±1.7 55.9 ±9.1\n",
            "Gemini-Pro(Vision) 59.2 ±1.2 65.0 ±6.1 47.0 ±2.2 42.9 ±3.4 48.1 ±5.8\n",
            "* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement\n",
            "* T : text-only, I : image+text, V : video+text\n",
            "* All experiments were tested with 3 random seeds\n",
            "26\n",
            "Table 12: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive settings with increased\n",
            "number of samples (N=100).\n",
            "- (NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference) Category Method MedQA\n",
            "T\n",
            "PubMedQA\n",
            "T\n",
            "Path-VQA\n",
            "I T\n",
            "PMC-VQA\n",
            "I T\n",
            "MedVidQA\n",
            "V T\n",
            "Single-agent\n",
            "Zero-shot 75.0 54.0 58.0 48.0 50.0\n",
            "Few-shot 77.0 55.0 58.0 50.0 51.0\n",
            "+ CoT 78.0 50.0 59.0 52.0 53.0\n",
            "+ CoT-SC 79.0 51.0 60.0 53.0 53.0\n",
            "ER 76.0 51.0 61.0 51.0 52.0\n",
            "Medprompt 79.0 58.0 60.0 54.0 53.0\n",
            "Multi-agent\n",
            "(Single-model)\n",
            "Majority V oting 79.0 68.0 63.0 52.0 54.0\n",
            "Weighted V oting 80.0 68.0 64.0 51.0 55.0\n",
            "Borda Count 81.0 69.0 62.0 50.0 52.0\n",
            "MedAgents 80.0 69.0 55.0 52.0 50.0\n",
            "Meta-Prompting 82.0 69.0 56.0 49.0 -\n",
            "Multi-agent\n",
            "(Multi-model)\n",
            "Reconcile 83.0 70.0 58.0 45.0 -\n",
            "AutoGen 65.0 63.0 45.0 40.0 -\n",
            "DyLAN 68.0 67.0 42.0 48.0 -\n",
            "Adaptive MDAgents (Ours) 87.0 71.0 60.0 55.0 56.0\n",
            "Category Method DDXPlus\n",
            "T\n",
            "SymCat\n",
            "T\n",
            "JAMA\n",
            "T\n",
            "MedBullets\n",
            "T\n",
            "MIMIC-CXR\n",
            "I T\n",
            "Single-agent\n",
            "Zero-shot 53.0 84.0 57.0 49.0 38.0\n",
            "Few-shot 60.0 87.0 58.0 52.0 33.0\n",
            "+ CoT 66.0 84.0 55.0 64.0 33.0\n",
            "+ CoT-SC 68.0 84.0 57.0 60.0 40.0\n",
            "ER 76.0 80.0 56.0 59.0 43.0\n",
            "Medprompt 70.0 84.0 62.0 60.0 43.0\n",
            "Multi-agent\n",
            "(Single-model)\n",
            "Majority V oting 53.0 82.0 56.0 59.0 54.0\n",
            "Weighted V oting 52.0 86.0 56.0 56.0 52.0\n",
            "Borda Count 53.0 86.0 56.0 59.0 51.0\n",
            "MedAgents 56.0 80.9 51.0 58.0 40.9\n",
            "Meta-Prompting 53.0 79.0 56.0 51.0 48.0\n",
            "Multi-agent\n",
            "(Multi-model)\n",
            "Reconcile 60.0 87.0 59.0 60.0 43.3\n",
            "AutoGen 47.0 87.0 53.0 55.0 47.0\n",
            "DyLAN 54.0 84.0 55.0 57.0 42.0\n",
            "Adaptive MDAgents (Ours) 75.0 89.0 59.0 67.0 56.0\n",
            "* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement\n",
            "* T : text-only, I : image+text, V : video+text\n",
            "27\n",
            "Figure 8: Complexity Distribution for each dataset classified by GPT-4(V) and Gemini-Pro (Vision)\n",
            "(for MedVidQA).\n",
            "\n",
            "Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pos = nx.spring_layout(G, seed=7)\n",
        "type_to_color = {\"METHOD\":\"#6aa84f\",\"AUTHOR\":\"#3c78d8\",\"DATASET\":\"#cc0000\",\"PAPER\":\"#674ea7\",\"METRIC\":\"#e69138\"}\n",
        "node_colors = [type_to_color.get(G.nodes[n].get(\"type\",\"\"), \"#999\") for n in G.nodes()]\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "nx.draw(G, pos, with_labels=True, node_color=node_colors, node_size=900, font_size=9, edge_color=\"#bbb\")\n",
        "plt.title(\"Entity Graph (Week-6 Track-A)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "P2o9AYw3Y-pO",
        "outputId": "6a9b3feb-0d50-48b8-d1fa-e16ceeabf3c0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-670650143.py:15: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
            "  plt.tight_layout()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs8AAAIeCAYAAABEEOI9AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAmd9JREFUeJzs3Wd4VNXehvF7z6T3QmghCT10adJLEBRFVERFRFGw93LU1+6xHRXUY+eIoqCAiBVUwIKA9CJNegmE0CGN9DIz+/0wYSQkwABJJoHnd13RzC5r//cQ8ZmVtdcyTNM0ERERERGRU7J4ugARERERkepC4VlERERExE0KzyIiIiIiblJ4FhERERFxk8KziIiIiIibFJ5FRERERNyk8CwiIiIi4iaFZxERERERNyk8i4iIiIi4SeFZRE7IMAxeeOEFT5fhcS+88AKGYZCSknJW7QwYMIA77rijnKoqfxMmTMAwDP766y9Pl+IxSUlJGIbBm2++WeHX+uijj4iNjaWgoKDCryUi5UfhWaQaORpuTvS1dOnS025z5syZbgfkxYsX88ILL5CRkXHa13HHTz/9xBVXXEGtWrXw8fEhIiKCXr168dZbb5GZmVkh16wsixYt4rfffuOJJ54AYPny5RiGwdtvv13q2KuuugrDMBg/fnypfb169SI6OrrC6z0bU6dOpWvXrgQGBhIWFka3bt2YM2fOCY8/+uHkVF8JCQmVdxPloFOnThiGwf/+978y948YMYLCwkLGjh1byZWJyNnw8nQBInL6XnrpJRo0aFBqe+PGjU+7rZkzZ/Lhhx+WGaDz8vLw8vrnr4nFixfz4osvMmLECMLCwk77WificDi47bbbmDBhAq1bt+bee+8lJiaGrKwslixZwrPPPsvMmTP5448/yu2ale2NN96gb9++rj+j9u3bExAQwMKFC3nkkUdKHLt48WK8vLxYtGgRI0eOdG0vLCxkxYoVXHHFFZVa++l44YUXeOmll7j22msZMWIERUVFrF+/nr17957wnMGDB5f42c3Ozuaee+7h6quvZvDgwa7ttWrVqtDay9O2bdtYsWIF9evXZ/Lkydxzzz2ljvHz8+OWW27hv//9Lw888ACGYXigUhE5XQrPItXQZZddRseOHSv8On5+fhV+DYDRo0czYcIEHnnkEd56660SIeKhhx5i//79fPHFFydtw+FwUFhYWGk1n45Dhw4xY8YMPvroI9c2Ly8vOnfuzKJFi0ocu2XLFlJSUhg2bBgLFy4ssW/lypXk5+fTo0ePSqn7dC1dupSXXnqJt956q9QHgpNp06YNbdq0cb1OSUnhnnvuoU2bNtx0000nPC8/Px8fHx8slqr3S9RJkyZRs2ZN3nrrLa699lqSkpKoX79+qeOGDBnC6NGjmTt3LhdddFHlFyoip63q/Y0jImft2HGbH3/8MY0aNcLX15cLL7yQFStWuI4bMWIEH374IUCJX48fdeyY5xdeeIHHH38cgAYNGriOTUpKonfv3lxwwQVl1hIfH0///v1PWGtubi6jRo2iZcuWvPHGG2X2vtWpU8c13OHY2u6//34mT55My5Yt8fX15ZdffgHgzTffpFu3bkRGRuLv70+HDh349ttvS7V7bBvx8fH4+fnRoUMH5s+fX2atGRkZrl730NBQRo4cSW5u7gnv7agZM2Zgs9no169fie09evTg4MGDbN++3bVt0aJFhISEcOedd7qC9LH7jp531KxZs+jZsyeBgYEEBwdz+eWXs2HDhlI1bN68mWuvvZaIiAj8/Pzo2LEjP/744ylrT09Pp1OnTtSrV48tW7ac9Nh33nmH2rVr89BDD2GaJtnZ2ads313z5s3DMAy++uornn32WaKjowkICCAzM5O0tDQee+wxWrduTVBQECEhIVx22WWsXbu2VDv5+fm88MILNG3aFD8/P+rUqcPgwYNJTEw84bVN0+TOO+/Ex8eH77//3q16v/zyS6699loGDhxIaGgoX375ZZnHdejQgYiICKZPn+7eGyEiHqeeZ5Fq6MiRI6UeXjMMg8jIyBLbvvzyS7KysrjrrrswDIPRo0czePBgduzYgbe3N3fddRf79u3j999/Z+LEiSe95uDBg9m6dStTpkzh7bffpkaNGgBERUUxfPhw7rjjDtavX0+rVq1c56xYsYKtW7fy7LPPnrDdhQsXkpGRwWOPPYbVaj2t92HOnDl8/fXX3H///dSoUcPVs/fuu+9y5ZVXcuONN1JYWMhXX33Fddddx88//8zll19eoo0///yTqVOn8uCDD+Lr68uYMWO49NJLWb58eYl7AWcvYYMGDXjttddYtWoV48aNo2bNmowaNeqkdS5evJjIyEji4uJKbD8aghcuXOgatrBo0SK6dOlC586d8fb2ZvHixVx55ZWufcHBwa4PKhMnTuSWW26hf//+jBo1itzcXP73v//Ro0cPVq9e7Xo/NmzYQPfu3YmOjubJJ58kMDCQr7/+mkGDBvHdd99x9dVXl1l3SkoKF198MWlpafz55580atTopPf5xx9/0K1bN9577z1eeeUVUlNTqV27Ns888wz333//Sc9118svv4yPjw+PPfYYBQUF+Pj4sHHjRqZNm8Z1111HgwYNOHjwIGPHjqV3795s3LiRunXrAmC32xk4cCB//PEHQ4cO5aGHHiIrK4vff/+d9evXl3l/drudW2+9lalTp/LDDz+U+vkpy7Jly9i+fTvjx4/Hx8eHwYMHM3nyZJ5++ukyj2/fvn2p30CISBVmiki1MX78eBMo88vX19d13M6dO03AjIyMNNPS0lzbp0+fbgLmTz/95Np23333mSf6qwAw//3vf7tev/HGGyZg7ty5s8RxGRkZpp+fn/nEE0+U2P7ggw+agYGBZnZ29gnv6d133zUBc9q0aSW222w28/DhwyW+HA5HidosFou5YcOGUm3m5uaWeF1YWGi2atXKvOiii0rdH2D+9ddfrm27du0y/fz8zKuvvtq17d///rcJmLfeemuJ86+++mozMjLyhPd2VI8ePcwOHTqU2p6ZmWlarVbztttuc22Lj483X3zxRdM0TbNTp07m448/7toXFRVlXnzxxaZpmmZWVpYZFhZm3nHHHSXaPHDggBkaGlpie9++fc3WrVub+fn5rm0Oh8Ps1q2b2aRJE9e2oz9fK1asMPfv32+2bNnSbNiwoZmUlHTKe0xLS3P9zAUFBZlvvPGGOXXqVPPSSy81AfOjjz46ZRtHHT58uNTP3ty5c03AbNiwYak/3/z8fNNut5fYtnPnTtPX19d86aWXXNs+++wzEzD/+9//lrrm0Z+to//tvPHGG2ZRUZF5/fXXm/7+/uavv/7qdv3333+/GRMT42rzt99+MwFz9erVZR5/5513mv7+/m63LyKepWEbItXQhx9+yO+//17ia9asWaWOu/766wkPD3e97tmzJwA7duwo13pCQ0O56qqrmDJlCqZpAs4eu6lTpzJo0CACAwNPeO7RWTSCgoJKbF+3bh1RUVElvlJTU0sc07t3b1q0aFGqTX9/f9f36enpHDlyhJ49e7Jq1apSx3bt2pUOHTq4XsfGxnLVVVfx66+/YrfbSxx79913l3jds2dPUlNTTzkTSGpqaok/h6OCg4Np06aNa2xzSkoKW7ZsoVu3bgB0797d1SO5detWDh8+7Oqt/v3338nIyOCGG24gJSXF9WW1WuncuTNz584FIC0tjTlz5jBkyBCysrJcx6WmptK/f3+2bdtW6mG+PXv20Lt3b4qKipg/f36pHvOyHB2ikZqayrhx43jssccYMmQIM2bMoEWLFrzyyiunbMMdt9xyS4k/XwBfX1/XuGe73U5qaipBQUHEx8eX+DP/7rvvqFGjBg888ECpdo8fLlRYWOj6bcXMmTO55JJL3KrPZrMxdepUrr/+elebF110ETVr1mTy5MllnhMeHk5eXp5bQ4BExPM0bEOkGurUqZNbDwzGxsaWeH00wKWnp5d7TTfffDNTp05lwYIF9OrVi9mzZ3Pw4EGGDx9+0vOCg4MBSo2Pbdy4Mb///jsAX3zxRZnDSsqacQTg559/5pVXXmHNmjUl5tAtazx1kyZNSm1r2rQpubm5HD58mNq1a7u2n+z9DAkJKbOWo45+qDhejx49eP/990lJSWHx4sVYrVa6dOkCQLdu3RgzZgwFBQWlxjtv27YN4IQPmR2tZ/v27ZimyXPPPcdzzz1X5rGHDh0qMf3d8OHD8fLyYtOmTSXuH5wzsBw5cqTEttq1a7sCrbe3N9dee61rn8Vi4frrr+ff//43ycnJpd7D01XWn7nD4eDdd99lzJgx7Ny5s8SHnmOHMiUmJhIfH19iBpkTee2118jOzmbWrFmlpsiz2+0cPny4xLaIiAh8fHz47bffOHz4MJ06dSoxlr1Pnz5MmTKFUaNGlXrA8ejPhmbbEKkeFJ5FzmEnGkN8oiB3Nvr370+tWrWYNGkSvXr1YtKkSdSuXbvUQ3LHa9asGQDr16/nqquucm0PCgpynXv8rBNHHd8DCbBgwQKuvPJKevXqxZgxY6hTpw7e3t6MHz/+hA9tuetM38/IyMgTfmA5Gp4XLVrE4sWLXQ+9gTM8FxQUsGLFChYuXIiXl5crWDscDsA57vn4gAu4AuLR4x577LETPrh5/BSHgwcP5osvvuDdd9/ltddeK7Fv6tSpJabPO3r/Rx9EDAsLK/U+1axZE3B+yDjb8FzWn/mrr77Kc889x6233srLL79MREQEFouFhx9+2HX/p6t///788ssvjB49moSEhBKzuOzevbtUiJ87dy4JCQmu3uUhQ4aU2e6ff/5Jnz59SmxLT08nICCgzHsTkapH4VnkPHc6vV0nO9ZqtTJs2DAmTJjAqFGjmDZtGnfccccpHwLs2bMnoaGhfPXVVzz11FNnPe3Yd999h5+fH7/++iu+vr6u7WUtOAL/9OAea+vWrQQEBBAVFXVWtRzVrFkzvvvuuzL3HfvQ4JIlS+jevbtrX926dYmLi2PRokUsWrSIdu3aERAQAOB6uK1mzZon/YDSsGFDwNkjfKoPMkc98MADNG7cmOeff57Q0FCefPJJ177+/fu7fiNwLIvFQtu2bVmxYgWFhYX4+Pi49u3btw+g3N7P43377bf06dOHTz/9tMT2jIwM14Ot4HzPli1bRlFREd7e3idts0uXLtx9990MHDiQ6667jh9++MH1gaR27dql3oMLLriAnJwcpk+fzvXXX1+i9/2oBx98kMmTJ5cKzzt37qR58+andc8i4jka8yxynjs6HtmdVQNPdezw4cNJT0/nrrvuIjs7+6Rz9B4VEBDA//3f/7F+/XqefPLJMntxT6en3Gq1YhhGiV/dJyUlMW3atDKPX7JkSYlxsbt372b69Olccsklpz37x4l07dqV9PT0Msea161blwYNGvDHH3/w119/ucY7H9WtWzemTZvGli1bSkxR179/f0JCQnj11VcpKioq1e7RYQU1a9YkISGBsWPHsn///hMed7znnnuOxx57jKeeeqrECnl16tShX79+Jb6Ouv7667Hb7Xz++eeubfn5+UyePJkWLVq4Zr0ob1artdTPyDfffFNqLPc111xDSkoKH3zwQak2yvoZ69evH1999RW//PILw4cPd/Vi+/n5lXoPwsPD+eGHH8jJyeG+++7j2muvLfU1cOBAvvvuu1LLca9atarUn7uIVF3qeRaphmbNmsXmzZtLbe/WrZurp9FdRx+We/DBB+nfvz9Wq5WhQ4ee9NhnnnmGoUOH4u3tzRVXXOEK1e3ataNVq1Z88803NG/enPbt27tVw5NPPsmmTZt44403+O2337jmmmuoV68e6enprFq1im+++YaaNWu6tQDK5Zdfzn//+18uvfRShg0bxqFDh/jwww9p3Lgxf//9d6njW7VqRf/+/UtMVQfw4osvulW7Oy6//HK8vLyYPXs2d955Z6n9PXr0cI3pPrbnGZx/plOmTHEdd1RISAj/+9//GD58OO3bt2fo0KFERUWRnJzMjBkz6N69uyskfvjhh/To0YPWrVtzxx130LBhQw4ePMiSJUvYs2dPmfMhg3NVxCNHjnDfffcRHBx8yg9Dd911F+PGjeO+++5j69atxMbGMnHiRHbt2sVPP/3k/ht2mgYOHMhLL73EyJEj6datG+vWrWPy5Mml/lu4+eab+eKLL/jXv/7F8uXL6dmzJzk5OcyePZt77723xLChowYNGsT48eO5+eabCQkJOelS2pMnTyYyMvKEQfjKK6/kk08+YcaMGa6VE1euXElaWlqZ1xaRKspDs3yIyBk42VR1gDl+/HjTNEtOt3U8jpsCzGazmQ888IAZFRVlGoZRYtq64481TdN8+eWXzejoaNNisZQ5bd3o0aNNwHz11VdP+/5++OEHc8CAAWZUVJTp5eVlhoWFmT169DDfeOMNMyMjo9R93HfffWW28+mnn5pNmjQxfX19zWbNmpnjx493TTdXVhuTJk1yHd+uXTtz7ty5JY47eu7hw4dLbD/653H8e1CWK6+80uzbt2+Z+8aOHWsCZnR0dKl9q1atcv35Hjx4sNT+uXPnmv379zdDQ0NNPz8/s1GjRuaIESNKTL9nmqaZmJho3nzzzWbt2rVNb29vMzo62hw4cKD57bfflrqfFStWuLbZ7XbzhhtuML28vEpNJ1iWgwcPmrfccosZERFh+vr6mp07dzZ/+eWXU553rJNNVffNN9+UOj4/P9989NFHzTp16pj+/v5m9+7dzSVLlpi9e/c2e/fuXeLY3Nxc85lnnjEbNGhgent7m7Vr1zavvfZaMzEx0TTNE/+3M2bMGBMwH3vssRPet5eXlzl8+PAT3ldubq4ZEBBQYhrEJ554woyNjS0xDaOIVG2GaVbAk0Mict569913eeSRR0hKSjrrh8MqmmEY3HfffWX+Gr+8LViwgISEBDZv3lzmDB9y/ikoKKB+/fo8+eSTPPTQQ54uR0TcpDHPIlJuTNPk008/pXfv3lU+OFe2nj17cskllzB69GhPlyJVxPjx4/H29i41f7iIVG0a8ywiZy0nJ4cff/yRuXPnsm7dOqZPn+7pkqqkshaykfPX3XffreAsUg0pPIvIWTt8+DDDhg0jLCyMp59+miuvvNLTJYmIiFQIjXkWEREREXGTxjyLiIiIiLhJ4VlERERExE0KzyIiIiIiblJ4FhERERFxk2bbEBGRSmfLyCB31SpyV64kd+VKCnfvxpGXB6aJJSAA7+hoAjt0IKBDBwLat8crIsLTJYuIAJptQ0REKok9J4f0KVM49MEH5K1d69xosYBhgN1e8uDjtvu1bEnN++4j4qabsAYHV3LlIiL/UHgWEZEKVbhnDwfffJOUTz/FkZ3tDMYOx+k1YhgAWPz9iRwxglqPP45v/frlX6yIyCkoPIuISIUwTZPUzz5j90MP4cjPL927fKa8vDC8vKj35ptE3XMPhkWP74hI5VF4FhGRcle4ezdJt99O1m+/Veh1gnr2pP7nn+PboEGFXkdE5CiFZxERKVc5K1aw7eKLsefkgM1WsRfz8sLi50eTWbMI6tGjYq8lIoKmqhMRkXKUvXAhW3r3xp6dXfHBGcBmw5Gby9Z+/cicPbviryci5z31PIuISLnIXbWKLb16OaecO90HAs+WxYLh40PTOXMI6tq1cq8tIucVhWcRETlrtvR0NjRrhi01tfweDDxdFgvW0FBabtmCd1SUZ2oQkXOehm2IiMhpSUhIwNfXl+DgYEJDQ2nVqhV39+jB4ZQUt4LzWODRiijM4cCemUnyvfe6No0YMYKHH364Iq4mIucphWcRETlto0aNIisri4yMDD697z6SN27kJoeDVE8XZreT8e23pH/zjacrEZFzlJbnFhGRM2bm5eH3wgu8bBgMM00mAXcAzwLrgEKgKfB48b/nAeMBE+hZ3MYCYCnwIZAM+AF9gIeLvweYBEwBsoBQ4DZgUPG+ZcecWxO4D7DedRcTduxg8uTJGIbBuHHjiIuLY8OGDRXxNojIeUThWUREzljalCnYDh3CC0jAGWQdwKXAf3D+evN94Engu+JjRgJbgbeOaccXeAZoAuzHGZwn4wzJu4D/Fb+uD6QCacXnbStuezTQAfgbeAiIS0/nRn9/1t54I2FhYbzzzjvlfesicp7SsA0RETkjpmly6N13nctt4+z1zQSCgEsAf5yh+C6cvcKHT9JWO6AZYAXqAYOBlcX7rMX/TgTygUicIRvge2AgcCHO/6G1xdmjPRs49N57Z3mHIiKlqedZRETOSO7y5eStW+d6fQgIwRlw3wEW4QzTRvH+DJwBuywbcA692A4UADYgrnhfPeAF4GvgJaAV8CAQD+wD/gJ+OqYtOxAIFCQmUtSoEYSFnektioiUovAsIiJnJOWzz8DLC2w2bMCfQHec45M3AeOAWjjHKffBOc4Zyv6V5zPAFTiHcvgDXwI/H7P/4uKvfJyzdTwPTC1ufyjwQFkFenlRuH07NG9+FncpIlKShm2IiMgZyZ4/H2w2knD2DGcDNwI5OIdrhAC5OHuUjxWBc1zzsesP5gDBOIPzTuDbY/Yl4XygMB/wLj7m6FCOwTh7nf/C2eNciHPc804Am42Qw4fZsWMHWtJARMqLwrOIiJy2J554gk6bN9Mb50wakcDE4n/fiPN/LpcA1wOtjzu3H85x0RfjfIAQ4GmcPdY9gdeA/sccbwM+Kt7WF2dQfqF4XzOcDyaOKW7vsuJjC4v3X5mVxZ7kZCIiImjTps3Z3raIiFYYFBGR05e9dClbqsky2E1mzyakb19PlyEi5wj1PIuIyGnLW7MGDOOUx3mcxeKsVUSknCg8i4jIabNnZIDVesrjPM2wWrEfOeLpMkTkHKLZNkTKiWmaFO7cSe7KleSsXEnO8uXkb96MmZ+PWVSE4eODNSSEgPbtCejYkcAOHQjo2BGviAhPly5y2hwFBRiGQXUY9+fIy/N0CSJyDlF4FjlL9pwc0r78kkPvvUf++vXOjcXTd5U6Ni2NwuRkMqZPB7sdDIPQK66g5v33E9y3L4ZFvwyS6sHw8qoWwRnA8Pb2dAkicg7R/6lFzpAtLY3dDz/M37VqkXznneRv3HjMztLB2cXhcAZnANPkyMyZbLvkEtY3bszh//0P8+g+kSrM4ufn/Fmu4kzTxPDz83QZInIOUXgWOQMZ06ezvmlTDn3wAY6cHOfGMw0SxUG7MCmJ5HvvZUuPHuRv3VpOlYpUDN9Gjf75EHiW7gQ6AduO2ZYFdMS5guBZsdmctYqIlBOFZ5HTYEtLY8eNN5I4aBD29PRyCw8AFM8amfPXX2xs3ZqDb7+NWQ169uT8FNCxY7m2Fwx8UK4t/iOwQ4cKallEzkcKzyJuKty9m82dO5M+dapzQ0UFW5sNs7CQPf/6F0k334xZVFQx1xE5C97R0VjL8WHX64C1wKpya9HJ8PfHt2nTcm5VRM5nCs8ibihMTmZz164UJCWVb2/zKaR9+SWJ112HebIx1CIeYBgGgRdeWG5zPYcAt1D+vc8B7dvrQVwRKVf6G0XkFIoOHWJLQgJFBw+e/EHAimCaHPnxR5JGjtQQDqlywq6+ulzbuwHYD8wrrwYNg/ByrlFEROFZ5CRM02TnjTdSuHt35Qfnf4ogbdIkDo8Z45nri5xAxI03YvH3L7f2/IA7gA+B8vj9juHtTeTIkeXQkojIPxSeRU4i9bPPyJo923PB+Rh7Hn+cgsRET5ch4mINCiLy1lud85qXk6sAB/Dz2Tbk5UXEsGFahEhEyp3Cs8gJFO7eze6HHvJ0GS6mzcbOESM0fEOqlKh77inXD5dW4F5g/Nk2ZLMRdd99Z1+QiMhxFJ5FTiD5gQdwFBR4uox/2GzkLFxI6oQJnq5ExMW/RQtq3HknlONDeX2BmLNpwGol4qabCCzn6fRERAAM0zSrywqrIpWmYOdO1jdq5Jp7ucowDPyaN6fF+vUY5TTLgcjZsmdlsaF5c4r27/f8qoMWC16RkbTcsgWv8HDP1iIi5yT1PIuU4fDYseXak1ZuTJP8jRvJWbzY05WIuFiDg6n/+eeeD84ADgdxn36q4CwiFaYKpgMRz3Lk55Mydmylzud8Wry8OPThh56uQqSEkL59qfXYY54twjCocc89hF1xhWfrEJFzmsKzyHEyf/0Ve0aGp8s4MZuN9G++wZGX5+lKREqIHjWKyBEjym3hlNNiGIRfdx2x779f+dcWkfOKwrOctxISEvD19SU4OJjQ0FBatWrFo48+SvLcuW5PvTUWeLRiy3R5AXjr6Aubjdy1a0sdM2LECB5++OFKqkikJMNiIW7cOOf0dZUs/IYbaDBpEobVWunXFpHzi8KznNdGjRpFVlYWGRkZfP311+zdu5dLx44ltQrM63xShkHuypWerkKkFMNqJe6TT6jz4ovO5wYqMsxarWAY1H7qKRpMnIjh7V1x1xIRKabwLAIYhkGLFi2YOHEiAYWFTCrengv8C7gY6I1z9bOtxfvm4ZyLdiHQs/gLYCkwvPj4/sDrQP4x15oEXA70Aq4Aph2zbxlwM5AADAH+LN7+FTAL+Kb4OkNMk9y//jrLuxapGIZhUPf552m+YgV+TZtWzDAOiwXfBg2IX7yY6FdfxaiKD/iKyDlJf9uIHCs1ld4OB6uKXzqAS4Efgd+AeOBJwMQZcEcCPYAFxV8AvsAzwBzgU+AvYHLxvl3A/3AuPzwfmAC0LN63rbjtB4rPfRp4HkgChgKXAdcVX+drIHfV0SpFqqaA9u1pvmYNdZ57DsPPzxmizyZIF59v+PhQ67HHaLFuHUFdupRfwSIiblB4FjmGPTubmkBm8esg4BLAH2covgtIBg6fpI12QDOcK6XVAwYDRwdYHP0FdiLO3uhIoEnxtu+BgcCFOP/DbIuzl3n2iWrNynL/xkQ8xOLjQ90XX+SCAweIefddfBs1cu7w8nIvSBuG6xkEn7g46r31Fm3276feqFFY/PwqsHIRkbK591SUyHnCLCjgEBBS/DofeAdYhDNQH/1ffQZQ8wRtbMDZs7wdKABsQFzxvno4H/z7GngJaAU8iLNHex/OXuqfjmnLDgSeqNbCQndvS8TjrKGh1HzgAaLuv5/sP/8ka84cclasIGfFCuypqWWfExZGQKdOBF54IcEJCQRfdJGGZ4iIxyk8ixzDbhj8CXQvfj0J2ASMA2oBWUAfnMM2oOxf3TyDcyzzWzh7rL8Efj5m/8XFX/k4Z+t4Hpha3P5QnMM2ynJ8H50ejpLqyDAMZxBOSADANE2K9u6l6MABzPx8TIcDi78/3rVq4R0To5U0RaTKUXgWKbZ582ZefOYZsoEbi7fl4ByuEYLz4cHjlyaJAPbj7F32OuacYJzBeSfwLXD0l8tJwAGcQzK8i485OpRjMM5e6K44h37Ygc3FbTXAOcQjEWdwNwBLQMBZ37OIpxmGgU+9evjUq+fpUkRE3KLff8l57YknnnDN8zx48GDqNGjAJD8/Iov334jzP5JLgOuB1sed3w/nuOiLcT5ACM4H/SbhHK/8Gs4ZN46yAR8Vb+uLc5jGC8X7mgH/AcYUt3dZ8bFHB2cMwjnW+iKcPdT+rY+vRkRERCqaYZqmeerDRM4fm3v0IGfRIk+XcXIWC9GjRlHb08shi4iInGfU8yxynMBOnaCqjyd2OAjs0MHTVYiIiJx3FJ5FjhPYsSMUFXm6jFPyb9fO0yWIiIicdxSeRY4TMmCAc0GHqspqJfjii/EKC/N0JSIiIucdhWeR43iFhRFx002uhRmqHLudmg+caEI7ERERqUh6YFCkDLlr1rCpig6L8K5bl9bJyRhW66kPFhERkXKlnmeRMgS0bUtgly5Q1QKqxULNBx5QcBYREfEQ9TyLnEDumjVs6tgR7HZPl+JkteLboAEt1q3DUpXHZIuIiJzD1PMscgIBbdtS59lnoaosD+xwUH/iRAVnETlvmaaJ+vzE09TzLHISjsJCNrVvT/7mzZ7tgbZYqPXoo9QbPdpzNYiIVJK83CJ2bk0leXsau7ansXNrKtmZBdiKHAB4eVsICvGlQdNI4hpHENc4ggZNI/ELqOJz9Ms5QeFZ5BTyNm5kc9euOHJyPBOgrVYCO3Wi6Zw56nUWkXParu1p/DlzG8vn78JW5MBiMTAB01F2VDEM5z9Mh4mXt4UuferT+7ImxDQMr9S65fyi8Czihpzly9mSkIBZWFi5Adpqxb9FC+IXLMAaGlp51xURqUTrV+7jx8nrSE5Mx2I1cNjPLJpYLAYOh0n9JhFceVMbWrStXc6Viig8i7gtZ/lytl1yCfbs7MoJ0FYrAR060OSXX/AKVy+KiJx7crML+frTVSydk4RhQHklkqNtdb+4Idfe2g5/DeeQcqTwLHIa8jdvZsfQoeT9/Xf5/S1/PIsFHA4iR4wg5v33sQYFVcx1REQ8aMOq/Xz+7jKyMgtOOCzjbBkWg5BQX0Y83IXm6oWWcqLwLHKazKIiDowezb4XXnAG6PLshbZa8apRg/rjxxN62WXl166ISBWy4NftTB7zV7n2Np/I0QmTht/fiW79GlbsxeS8oPAscoby1q0j+aGHyJ4717mUt812Zg0V/81u+PgQeeutRL/6Kl5hYeVXqIhIFTL3561M/WSVR6497J6O9Lq0sUeuLecOhWeRs5S/eTOHP/qIlE8/xZGd7QzSdvvJu1OsVud+hwOfBg2o+eCDRN5yi8Y2i8g5bemcnUx4d5lHa7jt0a5c2CvOozVI9abwLFJO7Dk5ZM2ZQ+7KleSsWEHO8uXYU1JKHmQY+DRoQFDXrgR06EBg584EdumCYdF6RSJybtu/+wivPPQL9jOcSaO8eHlbeO69y6hVN9ijdUj1pfAsUoHsWVk48vIwi4qw+PlhCQzUXM0ict5x2B28/vjv7NmZgaOCHg50l8ViENckgsdf64vFqo4LOX36qRGpQNbgYLxr1sQnOhqvyEgFZxE5L/0+fQvJiekeD84ADofJzi2pzJ2xzdOlSDWl8CwiIiIVJj0llx8n/e3pMkr54Yu1HEnP83QZUg0pPIuIiEiFWfBrIlWgw7kUu83Bot93eLoMqYYUnkVERKRC2IrszP9le4UtgnI2TBP+nLkNu93h6VKkmlF4FhERkQqxZtlesjMLPF3GCR1Jz2fdX/s8XYZUMwrPIiIiUiGWzUvCqMJJw2IxWD5vl6fLkGqmCv9Ii4iIiKckJCTg6+tLUFAQERERJCQksHLlytNqY+eWVMwKGBVxOGMn3/35NBNm3c74mbcxbcHz7Evd5NqflrmbGUte4/Nf7mDsjzdQUJRT4vy9KRv4adHLfPrzSB58/pJS7T/++OPEx8cTHBxMgwYNeO2111z7CgoKuOOOO2jQoAHBwcE0a9aMzz77rPxvUqoshWcREREp06hRo8jOzmbfvn20a9eOq666yu1zj6TllcuQDbvDVmpbcEANLrnwEW659BNGXDaONo0G8svS0djshQBYLFYa1e1CQtt7ymzT2+pLfGwCXVsOx3RQqk4/Pz++//57MjIymDVrFmPHjuXjjz8GwGazUadOHWbPnk1mZiYTJkzg0Ucf5bfffjvre5XqQeFZRERETsrPz4/bbruNvXv3kpqayn//+1+aNGlCcHAwjRo14oMPPnAdm5SUhGEY/Pet95n8+wNMmHUHC/7+tEQIPpyxk58WvcyEWbczZfbDbNr1h2vfX5u/Zday0SxY+ykTZt3O8o1TStfjE0xwQBSGYQAmFsNCkT2f3PwMAMKC6tIsrg8RITFl3k/N8MY0jelJSGAtAJJ3pJfY//LLL9OyZUusVivNmjVj8ODBLFy4EIDAwEBeeuklGjVqhGEYdOnShT59+rj2y7nPy9MFiIiISNWWm5vLuHHjiIuLIzIykri4OObMmUO9evWYN28eAwYMoF27dnTv3t11zqxZPzOkzygKbfnMWjqa1dum0TH+WnLzM5ix5FV6trmVBnU7k5G1lxlLXiU4oBb1oloBsPvQWnpfcCfdW48os+f5qPEzb6PIno9pOmharychgTXP6P72Jx+hRdvaZe4zTZP58+czdOjQMvfn5+ezfPlyhg0bdkbXlupHPc8iIiJSpqeeeoqwsDAaNmzI5s2b+fHHHwG45ppriImJwTAM+vTpQ//+/Zk3b16Jc4cMuhs/30AC/SJo2+Qqtu129sxu3bOAOpHNaBTdFYthISIkhvjYBLbvXeQ6NyI4hvjY3lgsVry9fE9Y38gBn3LrgPH0aXcvtSObndlNGlCQf+KA/uyzz5Kbm8s995QeAmKaJrfffjtNmjRh8ODBZ3Z9qXbU8ywiIiJleu2113j44YdLbZ88eTJvvfUWSUlJOBwOcnNzadCgQYljIsNqs5PDAAT71yAnPw2ArNzDJB9aw/iZt7mONU1HifAb5B/pdo1eVh+axvTk67mPERZUlzpnEKJtRWU/1fj666/z1Vdf8eeffxIYGFhin2ma3HvvvWzZsoXZs2djsag/8nyh8CwiIiJuS05O5pZbbuGXX34hISEBLy8vBg0ahGmWXAglNeMAYAUgOy+FQL8IwBmMG9S+kH4dHzzhNYwzmN/O4bBzJOfAGYVnL+/S13v99df56KOP+PPPP6lXr16JfaZpct9997Fs2TL++OMPQkNDT/uaUn3pY5KIiIi4LTs7G9M0qVmzJhaLhZkzZ5Y508Q30z8mvyCHnPw0Vm+bTuN6zvHQTev1ZG/KBnbsW4bdYcPusJFyJIlD6Ylu17DrwCpSj+zC4bBTZCtg1dZp5OSnUSeyOeAMtzZ7IXZHEQB2exE2e6Er4Jumo3i/DUzAsJOfn+9qf/To0YwZM4a5c+cSFxdX6vr3338/ixYt4vfffyc8PNztuuXcoJ5nERERcVuLFi145plnuOiii7Db7Vx55ZVceeWVpY7rf8kA3n73CYpseTSs24X2TQYBEOgfweVdn2TZxiks+HscpmkSFhRNx2bXuV1DfmEWSzZMIic/DS+rNxHBsVza+f8ILZ49IzsvhS9n/9OzPfE353jlYf3eIzggiv2pm/lp8cuu/ZcPuQDAFa6feOIJvL29ad26teuYnj17MmvWLHbt2sWYMWPw9fUtEaxvuukmPvroI7fvQaovwzz+9ywiIiIiZygpKYkGDRqwM3Evrz8y39PluOXNiVcTFHLiBxNFjqVhGyIiIlLuQsP9CQz28XQZpxQa7qfgLKdF4VlERETKnWEYNGgaiWF4upITMywGDeLdn9lDBBSeRUREpBzVr1/fOY45LIxOveOoyoNDTYdJp971PV2GVDMKzyIiIlIh2nWLISCo6g7dCA715YJO0Z4uQ6oZhWcRERGpEN7eVnpd2gjDUvXGbhgWg94DmmD1UhSS06OfGBEREakwPfs3pupFZ7AY0OPiRp4uQ6ohhWcRERGpMJE1Axl4QytPl1HKlTe1ISzS39NlSDWk8CwiIiIVqv/g5kTXD8NSBYZvWCwGsY3C6XdVvKdLkWpK4VlEREQqlNXLwq2PdPF0GQAYBox8pAtWqyKQnBn95IiIiEiFi64fxg33dPR0Gdx434XUiQn1dBlSjSk8i4iISKXoeUkjrr7lAo9d/7rb2tGtb0OPXV/ODYZpVuXpy0VERORc88dPW/hm3GoMgwpfROXoNW64uwO9L2tSsReT84LCs4iIiFS6Ncv2MPG95eTlFuFwVEwUsVgMAoJ8uPnBTrS5UIuhSPlQeBYRERGPyM4sYMrYv1i5cHe59kIfbatT7ziuv6M9gcG+5dOwCArPIiIi4mGrl+xh+qS/ObAnE4vFOOOe6KPn1o0NZdDwNrTR0ttSARSeRURExONM0yRxUwrzZm5j1aLdOBwmFquB6TBP2CNtGGAYhuvYjj1iSRjQhAbxkRiG5+eUlnOTwrOIiIhUKdmZBWzfdJjk7ens2p5K0rY0crIKSxwTGOxDg6aRxDWOILZRBI2a1yAoRMMzpOIpPIuIiEiVZ5omtiIHGODlZVHPsniMwrOIiIiIiJu0SIqIiIiIiJsUnkVERERE3KTwLCIiIiLiJi9PF+BJBbZ89qTv5EDmHgrt+dgcNrwsXvh6+VE7JIZ6YQ3w8dKTuyIiIiLidF6FZ4fpYNOBNaxMXsCOlM2kZB/AxPm8pGFYMHA+zevahkHN4Lo0rNGMjnG9iK/ZRk/3ioiIiJzHzovZNrILMlm6cw5/bptBWu5hLIYFh+lw+/yjx9cIrE1C04F0rp9AgE9QBVYsIiIiIlXROR2eHaaDhdt/4fu1E7DZi1w9ymfO2evsY/Xluva307VBX/VEi4iIiJxHztnwnJJ9kInL32P74Q0Vdo3mtdtx44X3ER5Qo8KuISIiIiJVxzkZnlcmL2Ti8vewO2ynNTzjdFkMC95WH0Z2fZTWdS+ssOuIiIiISNVwzoXnRYm/8eVfYyrxigYGcHPnh+lUv3clXldEREREKts5Nc/zkp1/VHJwBnDOzvHFsndYtXtxJV9bRERERCrTOROeNx1YzeTlH3js+iYm45e8SeLhTR6rQUREREQq1jkRnvMKc/h82bueLgMT+HzZ2xTaCjxdioiIiIhUgHMiPH+75jOyCzLLYSq6s2OaDtJyDvPjukkerUNEREREKka1D88b9q9i6c4/MCtwVo3TYWIyd+tPbD+80dOliIiIiEg5q/bh+ce/JxbPd1F1GIaFn9ZN9nQZIiIiIlLOqnV43pW6jT0ZOz0+XON4pulg++ENHMjc7elSRERERKQcVevw/Of2WViMqnkLFsPC/O2/eLoMERERESlHVTN5uiG3MJu/kudX6AqCZ8NhOliyYzYFtnxPlyIiIiIi5aTKhOcXXniBQYMGuX184uGN2B22E+5fOzORaS8uLIfKzlyhvYBdads8WoOIiIiIlJ/TCs8JCQkYhsHs2bNLbH/jjTcwDIOHH37YrXYmTJhA27ZtT+fSpSSnJ2IxrGd8/sT7f2PM0Olk7M8usX3+Z2sZM3Q6a2cmApB5KJcxQ6fz8S0/88nIGXx2xyymvbSIzfOSKWtl8zkfrWbM0Omk7c3CwCA5LfGMazwdCQkJvPPOO5VyLREREZHz1Wn3PMfHxzN+/PgS28aPH0+zZs3KrSh37ErbftbT04XVDWLzn8mu1/YiO9uX7CO0dmCpY28Z0587xl/OzR9eQtuBjVjx3Rb+/GRtiWMK84rYvmQvvkHebJqzC8MwSE6vnPAsIiIiIhXvtMPz0KFDmTVrFkeOHAFg2bJlAHTu3LnEcYmJiVxxxRVERUURFxfHK6+8gsPhYPXq1dx9992sW7eOoKAggoKCSE52Bli73c79999PWFgYsbGxTJ061dVeUVERTz31FLGxsURFRfHe05+Sm/nPeOK03Zl89+x8PhnxM9NeWkRO+qnHGjfrHcOW+bsxHc4e5B0rDlCzURiB4X4nPMfLx0r99rXpd38HNs7dRdqeTNe+7Uv24e3nRddhLdm6cA9FRTaSUre49r///vvExMQQGRnJs88+S9u2bZkwYYJr/+zZs+nUqRNhYWG0bNmSH3/80bVvxIgR3HHHHQwdOpTg4GDi4+OZN28eAI8++igLFizgiSeeICgoiMsuuwyA//73v8TGxhIcHEz9+vUZN27cKd8TERERETmx0w7PYWFhXHrppUyZMgWAzz77jJEjR5Y4Jjc3l759+9K3b1/27t3LggUL+Oqrrxg/fjzt2rXjo48+onXr1mRnZ5OdnU1sbCwAv/76K7169SI1NZVXXnmF22+/naysLABee+01fv75ZxYuXEjijkTsDhu/v78SAIfdwcw3lhHdqga3jhtAl6HN2TRn16nvpW4wQZH+7P77EACb5+2iWUKsW+9DnfgIAsP92Lcx1bVt09xdNO1ejybdoinKt5G06gCZ+RkA/PHHHzz//PN899137N+/H4vFwoYNG1zn/v3331x33XW8/vrrpKWlMXbsWIYPH86WLf+E76lTp3L33XeTkZHB8OHDGTFiBABvvfUWPXv2ZNSoUWRnZzNr1iy2bt3Ks88+y2+//UZWVhbLli2jU6dObt2biIiIiJTtjB4YHDlyJOPHjycvL4/vvvuO4cOHl9g/Y8YMwsPDefjhh/Hx8SE2NpaHHnqIL7/88qTttm/fniFDhmC1Whk+fDiFhYVs3boVgIkTJ/Lss88SGxuLn78v3Ye3ZM+6w+Sk5XFgazr5WYVceG0zrF4WajeNoHHXaLfupXlCLJvmJZOdmkdK0hEadKjt9vsQGO5Hfk4hAGl7Mjm4LZ343jF4+3nR8MI6bJqbjM1eBMCXX37JjTfeSKdOnfDx8eG5554jMPCf4SFjx45lxIgRXHTRRVgsFnr06MHAgQP5+uuvXccMGDCAhIQErFYrI0eOZNeuXaSmplIWq9WKaZps2LCBvLw8atWqRZs2bdy+NxEREREp7YzCc9++fdm/fz8vv/wyXbt2pXbtkoEzKSmJ9evXExYW5vp69NFHOXDgwEnbPbYdwzDw9/d39Tzv2bOH+vXru/YHRvhj9baQnZZPTno+AeF+WL3+uZ3gKH+37qVx12j2rDvM2pmJNO4ajdXb/YcQc9Lz8Qv0AWDT3GQi40KoERcKQHzvGHavPUR2Wh6mabJv3z5iYmJc53p7e1OnTh3X66SkJD766KMS79n06dPZt29fme/P0eB99P05XqNGjfj888/54IMPqFWrFpdccglr1qxx+95EREREpDSvMznJYrFwyy238J///Idvv/221P6YmBg6dOjA0qVLT3j+6apXrx5JSUl07twZL4sXuRn52IscBEX4YS9ykJuej93mcAXorJQ8t9r1CfAmrl0t1s5M5LpXe7tdz4GtaeSk51O3RSR2m4OtC3ZTlG9n/F3/LIxiOky2zt+LcY9B3bp12b37nxUHbTYb+/fvd72OiYnhoYce4vXXX3e7hmOV9Z4OGTKEIUOGkJeXx/PPP8/w4cNZt27dGbUvIiIiImcxz/MjjzzCb7/9xhVXXFFq38CBAzl48CBjxowhPz8fu93Oli1bXA+41apVi/3795OX517ABbjpppt49dVX2b17N7m5eSydtJl6raMIjPCnVpNwfIN8WPn9Fuw2Bwe3pbF9yV632+46rAVXPdudqAZhpzzWVmhn1+qDzP5gJS36xBFRL4SklQcozLNx3Wu9uX5Uguur4+CmbJq3C9M0ueGGG/jyyy/566+/KCoq4pVXXiEnJ8fV7l133cX48eOZO3cudrudgoIClixZwqZNm9y6h1q1apGY+M/MHlu2bOH3338nLy8PHx8fgoKC8PI6o89KIiIiIlLsjMNzREQE/fr1w9vbu9S+oKAgZs+ezR9//EH9+vWJjIxk2LBhrmEbF110EV26dCE6OpqwsDDXbBsn89RTT9G/f3+6du1K/fr18bME0O++9gBYvSwMeLwzyWsP8eltM1kyZRPNE+LcvpfACH+iW9Y46TGf3/srn4ycwRf3/cbqH7fRYXBTet9xAeB8ULBJt2jCo4MJCPNzfbW5rDE5aXnMnTuXfv368e9//5tBgwZRu3ZtbDYbTZs2xdfXF4B27doxZcoUnn32WaKiooiOjua5556joKDArXt4+OGHmT17NmFhYQwcOJDCwkKee+45atWqRWRkJHPmzCkxs4eIiIiInD7DLGulj2pg+t8Tmb15Gg7T7ulSTsgwLAxsdQOXtriu1L7CwkIiIyP55Zdf6N69uweqExEREZHTVWWW5z5dseGNqnRwBjBNB3ERjV2vv//+e/Ly8sjJyeGJJ54gMjKSCy+80IMVioiIiMjpqLbhuWGN5hgYni7jpKyGlbiIJq7XEydOpE6dOtStW5dVq1bx448/4uPj48EKRUREROR0VNsnyEL9w2kT3Yl1+1bgOMtluiuCxbDQMa4XAT5Brm0//PCDBysSERERkbNVbXueAXo1GVAlgzOAw3TQq/Flni5DRERERMpRtQ7P8TXbUCPQ/RUBK4uBQXRY/RJDNkRERESk+qvW4dkwDC5vdYOnyyjFxGRgq2EYRtUeky0iIiIip6dah2eAC+N60apOByxG1bgVi2GhY2xP2kR38nQpIiIiIlLOqkbiPAuGYXDDhffhbfX1dCkYGPh7B3Jd+zs8XYqIiIiIVIBqH54BwvwjGNrhbk+XgYnJTZ0eIMg3xNOliIiIiEgFOCfCM0Cn+r0Z2GqYR2u4tt1tGq4hIiIicg47Z8IzwKUtritzKezKcFWbm+nT9AqPXFtEREREKodhmqbp6SLK2+zN05i29nMwDMwKnAfaYlgwgevb30nPxpdW2HVEREREpGo4J8MzwI6UzXy+7B1Ssw9iUhG3aFA7JJoRXf5FTHjDCmhfRERERKqaczY8AxTaCvh5/RTmbJmOYRjlshqhxbBgmqZriIiX1bscKhURERGR6uCcDs9HHcjczfxts1iy8w8K7QUYGKfVG330eF8vf7o3vJiejS+lZnDdCqxYRERERKqi8yI8H5VflMeKXfNZsWseyemJFNkLAbAaVjAMOOatsJt2AHy9/IgNb0Sn+n3oGNsTHy/PzyctIiIiIp5xXoXnYzlMB4ez9pOcvp0DmXsosOVjd9jwsnjj4+VLndBYYsMbUSOodpVZvVBEREREPOu8Dc8iIiIiIqdLXaoiIiIiIm5SeBYRERERcZPCs4iIiIiImxSeRURERETcpPAsIiIiIuImhWcRERERETcpPIuIiIiIuEnhWURERETETQrPIiIiIiJuUngWEREREXGTwrOIiIiIiJsUnkVERERE3KTwLCIiIiLiJoVnERERERE3KTyLiIiIiLhJ4VlERERExE0KzyIiIiIiblJ4FhERERFxk8KziIiIiIibFJ5FRERERNyk8CwiIiIi4iaFZxERERERNyk8i4iIiIi4SeFZRERERMRNCs8iIiIiIm5SeBYRERERcZPCs4iIiIiImxSeRURERETcpPAsIiIiIuImhWcRERERETcpPIuIiIiIuEnhWURERETETQrPIiIiIiJuUngWEREREXGTwrOIiIiIiJsUnkVERERE3KTwLCIiIiLiJoVnERERERE3KTyLiIiIiLhJ4VlERERExE0KzyIiIiIiblJ4FhERERFxk8KziIiIiIibFJ5FRERERNyk8CwiIiIi4iaFZxERERERNyk8i4iIiIi4SeFZRERERMRNCs8iIiIiIm5SeBYRERERcZPCs4iIiIiImxSeRURERETcpPAsIiIiIuImhWcRERERETcpPIuIiIiIuEnhWURERETETQrPIiIiIiJuUngWEREREXGTwrOIiIiIiJsUnkVERERE3KTwLCIiIiLiJoVnERERERE3KTyLiIiIiLhJ4VlERERExE0KzyIiIiIiblJ4FhERERFxk8KziIiIiIibvDxdwIlkZuSze0c6OVkFFBU5sFgMvL2tRNYMJLp+KD6+VbZ0ERERETlHVZkEmpmRz9I5O9m+6TA7t6aRlZF/wmMNA2pFh9AwPpKWHerStnM0Vi91oouIiIhIxTJM0zQ9dXHTNNm+8TB/ztzGqsV7OFqKuxVZrAYOu0lQiC+9BzSmxyWNCI8MqMCKRUREROR85rHwfGBPJp+/t4ydW1KxWAwcjrMrw7AAJvQb1Iwrh7XG28daPoWKiIiIiBSr9PDssDv446etTJv4N6bDPOvQfDzDgBq1gxj5SBcaxtco17ZFRERE5PxWqeE5MyOfMf+ZT9LWtAq9jsViYJomA65vycChrTAMo0KvJyIiIiLnh0oLz2mHc3jrmTmkH84t997mk+l5aSNuuKsjFosCtIiIiIicnUqZbeNIWh5vPvUHGal5lRqcARb8kogB3HB3R/VAi4iIiMhZqfD53QoLbLz93FyPBOej5v+SyMypGzxybRERERE5d1R4eJ4+aR0H92Z6LDgf9fNX69m1rWLHWouIiIjIua1Cw/P2TYf548ctbs/bXJEMw+Czt5dQVGT3dCkiIiIiUk1VWHguLLAx/u2lVeZBPYfD5NC+LGZ8td7TpYiIiIhINVVh4XnR7B2kHcrx+HCNY5km/PbDZjJPsvS3iIiIiMiJVEh4Nk2TuT9vrRLDNY5nOkwWz97h6TJEREREpBqqkPC8df0hDu3Lroimz5ppwryZ23DYHZ4uRURERESqmQoJz/NmbsNirRpjncuSkZrH+pX7PV2GiIiIiFQz5R6eHQ6TDSv347BXwTEbxSxWgw2rD3i6DBERERGpZsp1ee6EhASWLFmCw25gtXgRERJL15Y3ERXWsLwucVZs9kJWbf2BxH1LyStIp2atGnTp0oWnnnqKDh06eLo8EREREaniyj08t2/di9zkFtjshSzf9BU79i3jpks+LK9LuMXusGG1eJXa9vPiVwDo1uoWoiLiGP35Ffz8809s3LiRV155pVJrFBEREZHqx+vUh5ye9JRcAqwG4EOz2D6s2zGL/MIstu6ez4ak2eTlZ+DvG0LrRgNo1aA/AFm5h/ly9oP0uuB2Vm2dRpEtn0bRXejW6hZXCD6csZOlGyaRmrkLX+8g2ja5guZxfQH4a/O3HD6ygyC/SBL3LSE+pjddWw0vUdf2PYvIyN7L0L7v4OsdCA7ITLMzbNiw8n4LREREROQcVe7hOTe7CD+rSZGtgE3Jcwjyr4GfTzBB/jW4otuzBPpFsC91I7OWjqJGSH1qR8a7zt25/y+uTXgdm72AWUtHs3rbNDrGX0tufgYzlrxKzza30qBuZzKy9jJjyasEB9SiXlQrAHYfWkvvC+6ke+sR2B22UnXtOfw3MTXbOoNzsfSUXOrVDyvvt0BEREREzlHlHp5/+OUjDMZhtXoTGRLHpZ0eA6Bh3c6uY6JrtCSm5gXsS91YIjx3jL8GX+9AfL0DadvkKlZsmkrH+GvZumcBdSKb0Si6KwARITHExyawfe8iV3iOCI4hPrY3ABaLtVRdeQWZpcZeFxVqqW4RERERcV+5h+cr+91JDZ/upbZv27OQvxNnkJV7GNM0sdkLCA6IKnFMkH8N1/fB/jXIyU8DnMM6kg+tYfzM21z7TdNB7chmx5wbedK6/HyDXe0dVZVWPxQRERGRqq/cw7PFy4JhUGJ1wazcFOau/h8DujxJ3cgWWCxWfl3+FiYlw2t2XgoBfmGu7wP9IgBnMG5Q+0L6dXzwhNc1jJPPuhcTdQFLN06moCgXX+8AAHx8SvdQi4iIiIicSLnP8+zrZ8WwlFwgxWbPB9PE3ycEwzBIPriaPYf/LnXuyq3fU1CUQ05+Gqu3TadxPWcPdtN6PdmbsoEd+5Zhd9iwO2ykHEniUHqi23U1rtedsKC6/LLsDVKO7MThsGP1dvD111/z3HPPnd1Ni4iIiMh5odx7nsMi/SnaX7JHOTy4Hu2aXs3PS17BYZrUr9WeuFql51WuX7sD3857kiJbHg3rdqF9k0EABPpHcHnXJ1m2cQoL/h6HaZqEBUXTsdl1btdltXgxoMtTrN42jd9XvEtuQTozVkXRtWsXnn766bO6ZxERERE5P5TrPM8Au7an8dqjv53WOUenqhtx2bgSs2FUpJp1g3jpfwMr5VoiIiIicm4o92EbdeNCsRw3bKOqsVgMGjStceoDRURERESOUe7h2dvbSlyTCIwqnJ8dDpPGLaNOfaCIiIiIyDHKPTwD9L6sMaczGCQ4IIq7rpxSaUM2vH2sXNgjtlKuJSIiIiLnjgoJzx26x+If6F0RTZ81i9WgW98G+AVUzfpEREREpOqqkPDs7WOlZ/9GVXLss8Nu0uuyxp4uQ0RERESqoQoJzwB9Lm+K1avCmj8jFotB6451iY4L83QpIiIiIlINVVi6Da8RwHW3tauo5s+It6+VG++90NNliIiIiEg1VaFdwz37NyK+dc0qM3zjhjs7EBbp7+kyRERERKSaqtDwbBgGNz/YGS8fq0enrjMsBq061qFzn/qeK0JEREREqr0KH5QcWTOQ+5/rhcVq8UiAtlgM6tUP5fZHu2FU5cmnRURERKTKq5Qn+pq2qsk9T/eo9ABtsRjUiQnhoRf7aGo6ERERETlrhmmeznImZ2fr+kN8+PJ8igrtOBwVf9lGzWpw//O98A/0qfBriYiIiMi5r1LDM0Da4RwmfrCCTWsOVEj7FosBBlw5rDUXX90Mq7VqTZcnIiIiItVXpYdnANM0WfzHTqZ+vJKiIgdmOfRCGxYD02ES0zCMkY90pW5saDlUKiIiIiLyD4+E56OOpOXx56ztzP9lO9mZBRgWMB2n14bFauCwm9RvGkGfy5vSsWeseptFREREpEJ4NDwfZSuys2bZXhb8up0dm1MpKrQDYLUamCY4CzSds2WYuMZLh4T5cUHnaHpd2piYhuEeq19EREREzg9VIjwfy2F3cHBfFsmJ6ezekU5udiGFhXYsFgNvbyuRtQKJbRRObKMIQsL8PF2uiIiIiJxHqlx4FhERERGpqrw8XYCIiIiIlK/8ojz2H0kmryiXIkchVsOKt9WHGkG1iQiI0sJxZ0HhWURERKSayynIYmXyAnakbiEpdSuHs/ef8Fh/70DiIhoTF9GENtGdqB/ZtBIrrf40bENERESkmtqVtp3522ayInk+docdi2HgcGvqMgOLYcFh2qkX1oCEJgPpENsDHy/fCq+5ulN4FhEREalmDmXtY9LyD0hM2YjFsOIw7WfcloGBiYmfdwCDLxhBt4YXa1jHSSg8i4iIiFQTDoededtmMO3vLzBNh5u9zKcnvmYbbur0ABGBUeXe9rlA4VlERESkGjiSl8bHi0aRlLqlQq9jMSx4Wby5sdP9dIztWaHXqo4UnkVERESquJTsg7wz91mO5KVWSG/ziVzX/g4SmlxeaderDjTbhoiIiEgVlp57mP/OeYqs/IxKDc4A36z6BAsGvZoMqNTrVmUWTxcgIiIiImXLK8zhnbnPeSQ4HzV11ces2r3II9euihSeRURERKqo79aMJzXn0FkH5+XfbGbWm8vO+PxJyz8gIy/N9fqdd94hISHhrGqqrhSeRURERKqIhIQEDMNg9uzZbNy/miU7Z2OaDlb/tI0xQ6ez8PN1p2xj87xkpj4xt1zrKrIX8OWKD9GjcgrPIiIiIlVKfHw84z79hInL38PAOd/y5nnJhNUN8lhNDtPBhv0rWb5rnsdqqCr0wKCIiIhIFTJ06FDeevtNhvbvg4+/lYPbnMMlajUOdx1z5EAOC79Yx8Ft6Xj5WmlxURwdBjUlZdcR/vx0LQ6bg49v+RmAG966CACHw2T+Z3+zdeFufPy96XpjS5p0iwbAbnOw/JvNbFu4B1uhneiWNeh1axv8Q5wrDqbtzmTu2DV8smcG3Tp3p1OnTpX5llQpCs8iIiIiVUhIaDAxF0SxdWEyrS5uwKZ5yTRLiCVtdxYARQU2fnxlEW0GNOLSf3UiNyOfGa8vJSDMjxYXxdH7tgtYOyuR60f1KdHu7rWH6Hd/B3qMaM3WhbuZ9/Fq4trVxMffm1XTt7Fr1QGufrEHfkE+zB27ht/fX8mVz3TDYXcw841lNO4WzaAXetA5YACP3vY0TZo1YvbmaSSnbWd/5m4K7QU4HHa8LN74+wRSL6wBcRGNiQlvRN3QWLys3p54O8udwrOIiIhIFbL/yG4a96zDsqmbaNY7lh3L9zP0jT4s+XIjALtWH8Q3yIcLBjQCILhGAG0ua8i2RXtocVHcCduNahBG467Onub4njHM+3gtGftzqNkwjK0LdtNpSHOCawQA0H14Sz6/9zdy0vI4cjCX/KxCLry2GVYvK+vN+dTtEMqevTuZtvZzDMMo84HG3ek7WLTjNwC8LN5cGNebXo0vIzaiUbm+X5VN4VlERESkCtl+eCOxPWsxd+wa/vpuC7WahBMQ5ufan3U4l7TdmYy7dYZrm2lCUKT/SdsNCPN1fW8YBl4+ForybABkp+YREvXP+YER/li9LWSn5ZOTnk9AuB9WLwtgkl+US3CUP+l7szAxT/gQocO0u763OYpYljSXJTtnExvemH7NBtE+pjuGYZzWe1MVKDyLiIiIVBEO00FK9n7qGo2I7xXDymlb6f/IhSWOCYr0J6pBGNe80qvsRs5gOoigSH8yD+dRq4nzdW5GPvYiB0ERftiLHOSm52O3OYoDNGSl5J32NY6G6d3piXy25E2W7pzDjZ3uJ8w/4vQL9iDNtiEiIiJSRRTY8jnaj3vB5Y244umu1G9fu8Qx9dvXJvdIAet/24mt0I7DYZK+L4u9G1IACAj1IzejAFuhHXc17VGPVdO2kpWSR1G+jUVfrKde6ygCI/yp1SQc3yAfVn6/BbvNwcFtaWxfsveM79EsvsPNB9fw0sz7WLpzTrWaAk89zyIiIiJVRIEtD5/i7/2CfIhpXbPUMd5+Xlz5TDeWfLmBFd9twV5kJ7RWIG2vaAxAdMsa1Goczuf3/Ippmlw/uk+pNo7XflBTigrsfP/8fOxFDqJb1KDffe0BsHpZGPB4Z+aOXc2aGYnUbBRO84Q4DiWmn9W9OkwHBbY8Ji5/j+T0RK5tdxsWo+r36xpmdYr6IiIiIuewz5e+w1/J8z22FLcndal/ETd2ur/KB+iqXZ2IiIjIeeRw9v7zMjgDLE2aw7erPq3yQzgUnkVERESqiEJ7gadLOCvLv9nMrDeXnfH5f26fwbKkf5YWf+edd0hISCiHysqPwrOIiIhIFeFwVE6v87QXFzJm6HR2rztUYvvqn7YxZuh0Fn6+7pRtbJ6XzNQn5p7yuNP19aqPychNLfd2y4vCs4iIiEgV4e3lc+qDyklY3SA2z9tdYtvmecmE1Q2qtBrKUmQvZPKKD6rs8A3NtiEiIiJSRQT5hAAGUH7BceoTc7ngskY0S4hl68LdrPt1J1YvC427RrPulx0U5BbhG+DNwW1pANRqHO4698iBHBZ+sY6D29Lx8rXS4qI4Ogxqyq7VB5nz0WoMAz6+5WcAbnjrIgAcDpP5n/3N1oW78fH3puuNLWnSzbmyod3mYPk3m9m2cA+2QjvRLWvQ69Y2+Ic4F3BJ253J3LFrSNvzI+PbTuOinheX2/tQXtTzLCIiIlIFJCQkcH/Cv9m7LqXE9tMZSnEqTXvEcM3LzsVVfAO9ib2gJtsW7QFg07xkmiXEuo4tKrDx4yuLqNcqilv+15+rX+jB9sV72TQvmYiYEADC6wVz5+cDufPzga6lvXevPUTd5pHcOm4Ana5vxryPV1OYVwTAqunb2LXqAFe/2IPh71+MYRj8/v5KABx2BzPfWEZ0qxrcNu5yLhzSlHHjxp31PZc3hWcRERGRKqJ+o1g2zksqsW3LvGQi6wbi73AQVVREuM2GbzmNjW6WEMvmecnYCu3sWL6f+J4xrn27Vh/EN8iHCwY0wuplIbhGAG0ua+gK2ycS1SCMxl2jsVgM4nvGYLeZZOzPAWDrgt10uDqe4BoBePt50X14S/asO0xOWh4HtqaTn1XIhdc2w+JlYKlbwICrLi2X+yxPGrYhIiIiUkUMGTKE9957l1aHj9DIB1K3pfG7zU67uEBC8vO5Ky2NpJR8nv8+iZW7svH2sdKzVzRtro7noI83GAbrftnBqh+3YSu007Jf/RLtb56XzNpZifgGeANQs2EY6XuzGH/XL9gK7MwYvYzgqAB8A73ZMn83KUlHGDN0OhjOxVIsXhaCIv1Peg8BYb6u7w3DwMvHQlGeDYDs1DxCov45PzDCH6u3hey0fHLS8wkI93MtAW4xLBQGZJXH21quFJ5FRETknFVoK+BA5h4KbHnYHEVYLV74Wv2oGRKNv3eAp8tzMW2FOHLTCN67gEvjQzgyfxctetTiqcUHuL5zFFsP5AKQW2hnyIcbuaN3Hcbd2pSDmUXcPHYzXf3g9p51mZiUx7Kpmxj4VFeiGoax4tvNpO0+cQCdM3YN/iG+ZB7Opf9DHQmpFcjqH7cBEFo7kBr1Q7nutd7s25jCz68vZeCTXagTH0nmodwzus+gSH8yD+dRq4nzdW5GPvYiB0ERftiLHOSm52O3ObB6WXCYDjZv30i4We+MrlVRFJ5FRETknJGVn8Gq3YvZlbaNnalbOJy1H/MED99FBtakQWQ8sRGNaR/TjfCAqEquFkzTQe7ab8laNg5behKOrEiu7xzF6Jm7GdIpihlr05j7ZBte/SkZgD82ZBDq78UdCXUAiInw5fbetZm2MoWhXWuyf34yQzpEcmm0L0usBhde24z1v+4s89qFeUXsXLGfIaMSyMssoG7zGli9LFi9rQB0HtKcrx6fy4bfk2iWEEtMmyi2L92Lw2YSHOX84JF7pABboR0vH6tb99u0Rz1WTdtK7aYR+AV5s+iL9dRrHUVghD9+Ib74Bvmw8vstdBgcT8rODLYu3k3bNjXO9m0uVwrPIiIiUq2ZpsmOlM3M3z6TVbsXYZomhmHBYdpPel5qziHSc1NYmbyQH9ZMoFXdjvRucjnxtdpUyhLRtozdZPz+MkX7j30Q0KRn01AenbKDd37dQ4f6QdQM+Wf6ut1pBWzZn0ezJ1e4tjkcUDfcBwtwMLOQro1D6JOdTYv8fKaHhhIQ7lfm9QtyirB6W6gRF1rm/p1/7cfLx8LCz9cx/7O/AfAP8aF2kwhXeK7ZMIzP7/kV0zS5fnSfU95z+0FNKSqw8/3z87EXOYhuUYN+97UHnMNCBjzembljV7NmRiI1G4XTPCGOgkN5p2y3Mik8i4iISLWVnJbIpBXvszcjCYthdS1tbZ4iOB917FLYG/avZN2+FUQF1WHYhffStGbrCqkZIOfv78lc8I4z+R7HYjEY0imKd3/fyycjm5bYVzfchzYxgfz8r1ZltlsrxIe9aYUYQJTNxi2HDjM6PY/jp74b9O8e5Gbk8/fMHWSl5BFc459xyH3vbU9WSi6THpzNwKe6EN2iBharhVlvOsdDN+lWzzVs4+IHOuIb6O06t9N1zUrVdPtnl7u+t3pZ6HZjS7rd2LLM+iNjQ7j2P73/eS8MK53rJ5R5rKdotg0RERGpdmz2In5aN5nRvz/G/iPOIQ2n6mk+laNBOiX7AO/OfY6pK8eSX1S+vZ6maZK5aAyZ894AexGcoOY7Euow5Z7mXNwqrMT2i1uGcziriAkLDpBf5MDuMNl+MI/F244AMKhDDb5fmcKqpCzsNgfv/bKXwgI7rfPz4bhFRwLC/GjQsTZ/frqWnPR8TIfJ4Z0Z5GcVUpRvA9PEP8QXwzDYtfogu/8+XK7vhTscpp3k9B2Vft2TUc+ziIiIVCsHMvfwyaLXOZC5FzDLfSW6o2OkF2z/hXX7VnBHtyeIi2xy9u2aJlkL3ydn9ZRTHhse6EWv+NLDKQJ9rUy9rzmvTE/m7V/3UmBzEBfpxz19nWOge8WH8viAetzx2TbyixwM716T+DoBxBUWEp+ZyebjeqAvurc9S77cyLdP/0lhvo3wukFc+q9ORNQLocPVTfnxlcWYDpP6HWpTv0Pts34PzkR+UY5HrnsihllV1z4UEREROU5yWiLvzXueAlteiSEXFcUwLHhZvLin57PE12pzVm1l//UFWYv/V06VnZnFAQHMCQ72aA2nK8QvnNeuGu/pMlw0bENERESqhT3pO3ln7rPkV1JwBudsGDZ7ER/++SJbD535Cn+FBzaQtfijcqzszHTLzaVhQYGnyzgtVot7M3lUFoVnERERqfJScw7x3rznKbIXYFZScD7KxMRhOhgz/2X2pJc97dtJz7cVkPHbi2AYFVDd6XEAV2Rm4nOKFQozD+UyZuh0CnKcy2rPG7eWJZM3uHWNfZtS+fzeX8+2VADGDJ1OSlJmubRVXjTmWURERKo0h+ngi2XvkleUU2k9zsczMbE7bIxf+l+euuS/eFm9T31Ssaxln2LP2MPxM154ggUIdDjol5XFzNBQ9m9OZeUPWzm4PR3TNAmuEUDTHvVo0LFOifMSbr/A7WvUbR7JLWP6l1vNNYJqlVtb5UHhWURERKq0hYm/sv2we72eFclhOjiQuYdfNn7DwNbD3DrHdmQvOSsnURWC81EWoH1+Pl9uyeLnMWvoNKQ5fe9rj3+IL+l7s1g1fRs1G4d7ukyXOiGxni6hBIVnERERqbJSsg/y/Zqq87AYmPyy8RvaRHcmNqLRKY/OXT/NOVyjis3PYDNN/vhiA+2ubMIFA/65j/DoYPre277U8tt/jFmFb6A3PW5pTeahXCY9+Dt9723Piu+2kJ9VQIOOdUi4sy1WLwt7N6Qw661lrvmd7TYHK7/fwtaFe8jLLCA4KoC+97YnqkEYWxbsZvX0bWSl5OIb6EN8rxg6DWmGccwQl9ohMZXzprhJ4VlERESqrJ/WTcbusHm6jBIMw+C7NZ/yyEWvnvQ401ZA7rpp4KGhJiez63A+B1PyGdC17hm3kbzmIENe701Rno1vn53PtoV7aJZQupd46Zcb2bc5hYFPdSW0diAZ+7PxKl4C3C/Ih0v/1YnQOoGk7srkp9eWEB4dRNMe/wTm2iHRZ1xjRdADgyIiIlIlZeVnsGr3Io+Ncz4Rh+lg++GNHMjcfdLj8rbNwSzMrqSqTk9qtvMDSdeAM2+j4zXx+Ph7ExjhT2zbWhzemVHqGNM02fBHEt2HtyKsThCGYRBeN9i1vHdcu1qE1XVur1E/lCbdotm7MRVwThMIEOhbtabWU8+ziIiIVElLdv5R6TNruMtiWFiw/Veua3/7CY/JT5wLhqVK9jxHBDojYM1DuRhhoWf0PgeE+bm+9/a1umbmOFZeZiG2AjuhtYPKbCN57SFWfLuZIwdycNgc2G0OYtvWBKi6f/aeLkBERETkeA6HnXnbZrhW+6tqHKaDxTt+dy3ffeutt2IYBps2bXIdU3RgQ5UMzgCNavoRE+HL4mUHMB1nt6z5yfiH+ODla+XIgdKrBNptDn7573Ja9qvPLWMu4fbxl9OyX33Xs5UBPmUHbk9TeBYREZFK9cILLzBo0KCTHrP3yC6O5KW51d7amYlMe3FhOVR2egrtBSSmbCQrK4uvv/6aiIgIPv30UwDsuWk4ct2r/2zZ7Kf/AcMwDF65pj4f/p7MobkpFGQ7e40z9mUz56PVZKXknqIF96/T4qI4Fk9az5ED2ZimSfq+LLIO52IvsmMvtOMX5IPV28rBbWlsW7THeR4WejW+rFxqKG8KzyIiIlJKQkIChmEwe/bsEtvfeOMNDMPg4YcfdqudCRMm0LZt29O+/u70xNM+51Smv7yIscN/Ij+7sFzasxgWktMSmTp1KoGBgYwaNYqJEydSVFRE0aEtOBwm4/7cT8//rKHJ/y2n+8trmLspA+Ck+655fwOfzNvvus76PTnUfWip6/U172/g5em7GDpmE40eX86cTRnM25zBpW+uI/6JFbR9biVPfb2TvMJ/er2z8m08/e1OLnxhFU3/bwWXvbmOFtEBXN85it+nrGHig78x7tYZ/PLOCuxFduaOXV0u7xFA12EtiG4VxY+vLGbcyBn8+s5f5GcX4uPvTc9b2zBv3Bo+GTmDldO20bir8+HAEP8wLm42uNxqKE8a8ywiIiJlio+PZ/z48fTr18+1bfz48TRr1qzCr52clojFsOIwy2dIwZGDOezdmIJvoDfbFu6h9aUNz7pN0zTZlbaNzz/9mRtvvJGhQ4fy8MMP89NPP9G/oY3PFhxk3J8H+HhkE1pFB7AnvZCCImcv8WcLDrj2ta4XyN70whJh91S+Xn6YL+5sRtvYQPKLTP7enc0bQxvSom4Ae9IKGP7xZj6et5+HLnGG0YcnJ5JX6OCnR1pRM9ibDfty8fO28K8B9fly6Sre+f4/bMhegonJT68upmW/+rS7oonren3vbe/6PqRmAPd+dVWJenrc0tr1fXTLGq5p6gCs3la63tCCrje0KHUfrS5uQKuLG5TafnOnh/Dz9sesYlP8gXqeRURE5ASGDh3KrFmzOHLkCADLli0DoHPnziWOS0xM5IorriAqKoq4uDheeeUVHA4Hq1ev5u6772bdunUEBQURFBREcnIyAHa7nfvvv5+wsDBiY2OZOnWqq72ioiI+euMzJtw7i8/umMWv76wgL7PAtT9tdybfPTufT0b8zLSXFpGTnn/Ke9k8L5kacaG07t+QTfOSS+yz2xws/3oTkx78nU9G/MxXj89xzRxxsn1f3P8r337xI0uXLuWWW25x9dJ/+umnmLZ8Xpq2izYxgTz9TRKN/28FOQUO/t6dTZ/X1vLvH3aRmWfjl7/TAagX4UuT2v4cyixk24E83pi5m2ZPruDq9zZQUOQM1Q9P3u6q+eoONVi49Qg3jd2Mv4+Fzo1CaF0vEKvFIK6GH8O71WLxNuey1oczC5n1dzqjr29I7VAfLBaD1vUCiQzyJiLQm8t6tGbfskwiA2uSk17Avk2pxPf0zNzKBha6N7yEZrXdX9Gwsik8i4iISJnCwsK49NJLmTJlCgCfffYZI0eOLHFMbm4uffv2pW/fvuzdu5cFCxbw1VdfMX78eNq1a8dHH31E69atyc7OJjs7m9hY5zzAv/76K7169SI1NZVXXnmF22+/naysLABee+01NizdytUv9mD4+xdjGAa/v78SAIfdwcw3lhHdqga3jhtAl6HN2TRn10nvw+Ew2fxnMs16xxDfK4aUXUdKTKu29MuN7FpzkIFPdeX28ZfT/5EL8QvyOeU+gMTVybRo2YJatWqRmpqKv78/v/76Kxs2b8PuMFm1K5t3bmzEttGdaFTTj/BAL8bd1hRfLwsvDIpj0uJD/LAy1VXnLR9vwTDg/n7RrP9PR568PAZLcVqbsTaNnAJnT3x0uC9fLz/M0M7OmSnW7MpmyIcbafPMSpr+3wpe+3k3acWzX+xJL8TXy6BehG+Z789Nl13I5ElfcmvXx9i2YA8xbWqWmEmjslgMC7VCohncduSpD/YghWcRERE5oZEjRzJ+/Hjy8vL47rvvGD58eIn9M2bMIDw8nIcffhgfHx9iY2N56KGH+PLLL0/abvv27RkyZAhWq5Xhw4dTWFjI1q1bAZg4cSIXDm5OcI0AvP286D68JXvWHSYnLY8DW9PJzyrkwmubYfWyULtphGuc7InsXnuIvCMFNOlej9BagdRpGsGmuc7AfbJ5iE81R7FpmuzbmELi9kRatGjBQw89RFpaGna7nUkzlmC1GHRpGEzjWv5YLQY+XhYuahFOo5r+1Itw9gAP6hDJ4u3OHuI1ydlsO5hHq3qBAHhZDTo3CiE9xzknc9Pa/vy8xhm096QVkJZt45LWzmW07/1iG92bhLL0+bZsHX0hTw2Mcc1TUi/chwKbyd70f3rvj3VRl9bYbDZ2rtvL/uXZtEiIw8Ao89iKYjEshAfU4KE+L+Pn7V+p1z5dCs8iIiJyQn379mX//v28/PLLdO3aldq1a5fYn5SUxPr16wkLC3N9Pfrooxw4cOCk7R7bjmEY+Pv7u3qe9+zZQ1CNf3o+AyP8sXpbyE7LJyc9n4BwP6xe/0SY4KiTh61Nc3cR164W/iHOntf43rFsW7QXW6H9pPMQn2qOYluhA1uhnT8XzWPdunW899571K1bl+eee44ZC9cT6GdhwdZM1u/JwTRN9qQVMHnxQa54ez170wt5aHIiny88QGpWIXvSClixM5vaYT5cEBvEzL/TyMyzkZJVxJg/nA8PDu1ck6+XHwacQfvqjjXwLX4fsvLthPhbCfC1su1AHp8vOuiqMyrEh/6tw3ny650cPFKIw2Gybk+Os2faNPHyDWLkyJE8/PDDZB/J4fWHP8DL6u1apKSiWQwLUUF1+Fff1wnxC6uUa54NhWcRERE5IYvFwi233MLrr79easgGQExMDB06dCAjI8P1lZmZyYYNG1znn6569eqRnfpPL2luRj72IgdBEX4EhvuRm56P3XbMTBIpeSdsKy+zgKSVB9iz/jDj7/qF8Xf9wtIpGynIKWLH8n0nnYf4ZPsA7EV2ajWJoN0F7YiOjqagoACr1cqDDz7I/pQMvK0GfZqHcveEbTT5vxVc/+FGnvkuieHda7H+Px147spY/H2szN6YwdAxm7AYcCCjkFu616JmiDcd/72a6z7YyJXtIgEY1CGStck55BY6WL83h+s7R7lqGT2kIR/N2U/jx5fzxNc7uKr4nKPevbERdcN8uOytdTR78i+e/HoH+UUOMO14RTVh5MiR/P3339x00020rteRRy56lYiAqErpgW5eux3/6vsaYf4RFX6t8qDZNkREROSkHnnkEXr37k3v3r1L7Rs4cCBPPfUUY8aM4dZbb8Xb25vt27ezf/9+EhISqFWrFvv37ycvLw9/f/d+HX/TTTcxZsK71GgUjF+QN4u+WE+91lEERvjjF+KLb5APK7/fQofB8aTszGD7kr1ExoSU2daW+bvxDfLhutd6Y7H8EwSXTtnIprnJNO0R45qH+OIHOhBSK5CM/dl4eVsJjgo46b4GF9ahMMeGrcjOnuQdfPjhhwDUqFGD3MwMYmuH0b91BO/c2BiA7Hw7zZ5cQXiAF/4+Vjo1DMbbatCvZTjjb4/H4TD5dkUKr8/Yzfs3NSbQ18rKpCzaxgVxc49aAFx+QQQb9uXSuKY/rYuHdwBcdkEEl11QMnw+PuCfh/5C/L0YdX1DRpXxHnnXbEZN/AgICODWW28FIC6iMc9e+h4/rZvMnK0/YhiWcl3xz2JY8Lb6cn2HO+kU55wWsbpQz7OIiIicVEREBP369cPb27vUvqCgIGbPns0ff/xB/fr1iYyMZNiwYa5hGxdddBFdunQhOjqasLAw12wbJ/PUU09xQdeWfP/8fCY+8DsOu0m/+5xTpVm9LAx4vDPJaw/x6W0zWTJlE80T4k7Y1qa5u2h1cX2CIvwJCPNzfV0wsDF7N6Zw5EDOCechhhPPUQzQ+frmUGChVs1aDBs2jJtvvtl1XcPbD8NiLfle+Vl59doG/N/UHTT9vxW8+/s+V68ygMVi8Pmd8eQVOuj5n7W0euYvRs/cjeOYzHpDl5ps3Jtbotf5bFj8w7H4h/P+++/Trl07WrVq5drn4+XLNe1u5V8XvUadEGcQt5zlUA7DsGBg0Ca6M88P+JDO9ftUq+AMYJhVcQI9EREROa/N2jCVmRum4qiiy1sDWA0r3RpezNCOd5e5P+OP18jbOAPKaa5qcD4o2OOVNax6uT0RgaU/zJwWwwLBzWhy92TCvb35sFs3WgYGYnh7YwkMxK9ZMwI6dCCgfXusYWHsTN3C/O2zWJm8EIdpx2pYsbtxb0fn6w7wDqJn40vp3ugSIgNrnl3tHqRhGyIiIlLlxIQ3qtLBGcBu2omNaHTC/QHNB5C34cfyu57D5MM/9nFFu8izD84ApoPUN7/nz6IisNth9mwyTBMMAywWME3ndsAnNpaIG2/khrvu4pq2t7Ju3wp2pyeSlLqNvUeSsDtspZr39w4gNqIJcRGNqR/ZlBa12+NtLYe6PUzhWURERKqc+pFNMTAwqdq/IG8QeeLVFr3rtMErogG2tCQ4y/tITs2nz+t/Exvhy8S7zn6FR9M0cRwppGBjhrO0Y8eGmGbJ10BhcjIHRo/mwOuvEzJgAK0efJCuF9+JYRjYHXbSc1MoshdicxThbfXBzzuAUL/wajckwx0atiEiIiJV0tiFr7J+319VsgfawCAmvBFPXPLmSY/LWfcDmXNHV1JV7jMdJlkzdpM9e9/pn2y1gt1O2KBBxH70Ed61apV/gVWYHhgUERGRKql348urZHAGMDFJaHr5KY/zj++P4RcClbzoyMmYpolZ5CB36aEza6B4KEfGTz+xoVkz0qZO5Xzqi1V4FhERkSqpaa3W1Aismr2a/t4BtI/pfsrjLD4BhF70FGc7bKM8GYZB5vdJOLJLj1M+LXY79iNH2Dl0KMl3341pL78HI6syhWcRERGpkiyGhUuaX+PpMspgcFH8VXhbfdw62r9xAn5N+jpnt/Aw026SvyWD3KWHy6lB54eClE8+YeeNN2LazjKQVwOe/1MUEREROYGuDfvRKKrFWc8vXF4shoXaIdFc0mzwaZ0XmvAYhm+QRwO06TAxbQ6OTNlRAY2bpH/9NUm3347pqJpDbcpL1fhJFBERESmDxbBwc6eHsFqqxgRhpmlyS+dH8DrNKdcs/mFEXPEGWKx4Yvyz6TDBhPTPtmJPL6ygi5ikff45B0aVtY7huUOzbYiIiEiVN3/bTKau+tjTZXBZiyEMbD3sjM8v2LWMtJ8edy6ccoYPQ17z/gZW7szG28vAYhjUDfMhoVkY919cl8ig0qHedDijXvqEbeSvTWMssBV464zv4hS8vGixejX+rVoxYsQIwsLCeOeddyrqapVOPc8iIiJS5fVsfBk9G13qsesbGFwQ3YUBLa8/q3Z84zoTMegdDKsPGNZTn3ACz1wZy7bRndj8ekfGjmzC/iOF9H9jHYczS/Yqm3YHOEzSxm0hf23aWdV+OnbedBNmUVGlXa8yqedZREREqgWH6WDisvdYvmtepV7XwKB57bbc2ePpclshz5aeTMZvL1F0cMNpn3vN+xu4tHUEdyTU+ac9u8nFo//mohZh/OvSetz3xXZWJmVRWOigCfBYoYOmwDzgSZxzfxx93HEBsBT4EEgG/IA+wMPF3wNMAqYAWUAocBswqHjfsmPOrQncB/Q2DGZdfjkv/vILhmHg4+NDXFwcGzac/v1WNep5FhERkWrBYlgY3vlBejUeULylcsYOt4vpxl3lGJwBvMJjibxuLME9HgSL91n1QgN4WQ0ubRPB0sRM7DYHg9pH8nvnKH4pctC00OEKzAnASKAHztC8oPh8X+AZYA7wKfAXMLl43y7gfzgD8nxgAtCyeN82nGH8geJznwaeB5JMk4ELFzJs6FDuvfdesrOzz4ngDArPIiIiUo1YDAtD2t/ByC7/ws/bv8Jm4bAYFrytPlzf4S5Gdn30tB8QdIdhsRLU/gaibpyEf7P+YPHiTD8QmKZJrRBv0nNseG0+QrfZ+3D8sR9fE+7C2St8ssnp2gHNACtQDxgMrCzedzTWJwL5QCTQpHjb98BA4EKcobIt0BOYDdgzMijcteuM7qcqqxqProqIiIi4yTAMOsb1ommtNkz5awx/712OxbCUy2qEBgYmJg1rNGd4pwepEVTxi7R4hccSdvFzhPR4kNxNM8hZ+w2OrAPOnRYrOByUuciKxQoO58IkjswiklemEJRWyIEvtvMOsAjI5J84noFzWEVZNuDsWd4OFAA2IK54Xz3gBeBr4CWgFfAgEA/sw9lL/dMxbdmBQACLhfxNm6B9e/ffjGpA4VlERESqpRC/MO7s/hTr961gztaf2Hpo3RmHaIthxWHaiY1ozEVNr6B9bI9Kn1va4h9KUPthBLa7AXvWAYoObaHo0CaKDm7Cnn0IbAUAGF6JWMPqEdDyWvY+8TqFOzIoSC3gd6A7zvHJm4BxQC2c45T78E/8LuuungGuwDkDhz/wJfDzMfsvLv7KB8biHJoxtbj9oTiHbZTicGBPScGWmnrG70lVpPAsIiIi1ZZhGLSO7kTr6E4czNzLgu2zWJo0l7yiHOCfUHw8q2HFXrzdx+rLhXG96Nn4MmLCG1Zq/WUxDAOvkDp4hdTBv3FCqf3enycQ2GYQFq8O5K04SBLOoJwN3IgzPPsCIUAuzh7lY0UA+3H2Lh8NgjlAMM7gvBP4ln8eFkwCDuAckuFdfMzRoRyDcfZCd8U59MMObC5uqwHOIR7b//4b0zQxjMqf37oiaLYNEREROaeYpklqziGS0xPZnZ7Ivowk8oryKHIU4m3xxtfLj9ohMcRFNCYmohFRQXWqzAqG7khISGDJkiV4GwYUFFAT6AbcgjMYpwDP4hyKEQbcDfwb5wOA8cAR4HGcD/uZOGfgmAu8DaQDzYEOwJ84e6C3A6/gDNUG0BR4rPjfACtwPlCYVLw/Hnio+N97rFaeDQ9nj81GTEwMf//9d4W8J5VJ4VlERESkGto5fDhpU6aAvXTPelXi36YNLdau9XQZ5ab6fMwSEREREZecZcuqfHAGyNu48ZxaMEXhWURERKQaqjYP4tls2HNyPF1FuVF4FhEREamGzMLCUx9URZh5eZ4uodxotg032LOzyVuzhvzNm3Hk5uIoLMTi64slOBj/Vq3wb90ai6+vp8sUERGR84mlGvWBWs9uBcWqROG5DKbdzpFZs0j/+mtyliyhIDERjj5XabGAYThfO4rnkfTywr9FCwK7dSNy+HACu3Y9Z6ZjERERkarJ4uvL2S8LUzksfn6nPqia0Gwbxyg6dIjUzz7j0AcfULR3L3h5gc3mfgPFx/u1bEnNBx8kYtgwrEFBFVewiIiInLc2depE7ooVZ91Oz2O+L8A5h/PR3tV2wHtn2b4lOJi2R46cMx2LCs84e5oPvfMOe59+GtNm+6dH+UwV/3BYw8KI/egjIoYMKYcqRURERP6R/MADHB47FspxJos7gQRgWLm1CEEJCcTPnVuOLXpWNRosUzHyt2xhc7du7HnsMefA+7MNzuAc0mGa2DMy2Hn99SRecw1Fhw6dfbsiIiIixQI6dCjX4FwhvL0J7NTJ01WUq/M6PKdMmMDGNm3IXbWqYi5Q3KmfMX06G+LjyTqHPnWJiIiIZwV26ODpEk6tqMgZ8s8h5214Pvj22+waOdLZ23w645rPhN2OPTOTbf37k/HjjxV7LRERETkv+LVogXfdup4u4+S8vAju08fTVZSr8zI8H/rgA/b861+Ve1GHA9NmI/Gaazjy66+Ve20RERE55xhWK1H33Vd1p6zz8iLi+uvxjorydCXlqoq+2xUn/Ycf2P3AA565ePH0dolXXUXuObTGu4iIiHhGjdtuq7rh2WZzhvtzTBV9tytG0aFD7LrtNtdsGB5R3AO988Ybz6l13kVERKTyedeqRfi11zqny61KLBb8WrYksEsXT1dS7s6r8Jx8zz3YMzP/WfDEU+x28jduZP+rr3q2DhEREan26r74IkZV6312OIh5++1zZm7nY5038zynf/MNO6rafMtWK81XriTgggs8XYmIiIhUYwfffrvyn+c6EauVyJEjqf/JJ56upEKcF+HZdDhY37gxhUlJnu91PpbVSujAgTSeNs3TlYiIiEg1ZtrtbOnRg5y//qr4WcROxmLBu3ZtWm7ahDUkxHN1VKAq1sdfMbLmzKFw586qFZwB7HaO/PgjhXv2eLoSERERqcYMq5X6X3yBxd8frFYPFWGAYdBg0qRzNjjDeRKeD33wQdUbSH+UxULKxx97ugoRERGp5vyaNKHJb79heHt7bAaOBpMnn3PzOh/vnB+2Ubh3L+tiYqper/MxrJGRXLB/v/OHXUREROQsZC1YwPbLLsORnw92e8VfsLjHuf7nnxN5000Vfz0PO+d7nrPmzq3SwRnAnppK3vr1ni5DREREzgHBPXsSP38+3nXqVHwPtNWKJTiYRtOnnxfBGapxeE5ISMDX15fg4GBCQ0Np1aoVjz76KIcPHy5xXO7KlXCCHt2xwKOVUCvAC8BbJ9mfu3IlI0aMwDAMxo0bV2LfN998g2EYDBo0qAIrFBERkXNFQPv2tNy0iRp33eXcUN4hungKutDLLqPV5s2EDRxYvu1XYdU2PAOMGjWKrKwsMjIy+Prrr9m7dy8dOnTg4MGDrmNyli+HKr4YieHtTc7KlQDEx8czfvz4EvvHjx9Ps2bNPFGaiIiIVFPWoCDixoyh6Zw5+MTEFG88y4cJi0O4NTSU+pMm0ejHH5093OeRah2ejzIMgxYtWjBp0iRCQkJ46y1nH29WZiZ3L13KxUBv4A5ga/E584DxwEKgZ/EXwFJgePHx/YHXgfxjrjUJuBzoBVwBTDtm3zLgZiABGAL8Wbz9K2AW8E3xdY6fbdosKiJn2TIALr74Ynbt2sXWrc5K9+3bx4oVK9TrLCIiImckuE8fWm3fTqNp0/55mM9qPb0Vl4snXvC/4ALixo+nzb59RN544zm5CMqpVNEpKM6Ml5cXgwYN4vfffwegKCOD/g4Hr+D8lPA+8CTwHc6AOxJnmD52OIUv8AzQBNgPPAxMBm4DdgH/K35dH0gF0orP21bc9migA/A38BDwOTAU2AwEc+JhIkV790KbNlitVm6++WY+++wzXn/9dT7//HOGDBmCr6/vWbwzIiIicj4zvLwIu+oqwq66ivxt20ibNInspUvJXbECe3r6Cc+zBAYS0L49gZ07Ez5kCIEXXliJVVdN51R4BoiOjiYtzRlpg729ueSYfXfh7AU+DNQ8wfntjvm+HjAYZ+/0bcDRX3QkArWByOIvgO+BgcDRH6m2OHuZZwO3u1G3I/+f/u0RI0bQp08fXnnlFSZMmMDkyZP5+eef3WhFRERE5OT8mjSh7osvAmCaJkV79pC3fj2O7Gwc+fkYPj5YAgLwa9YM30aNqt7S3x52zoXnvXv3EhERAUBubi6vA4uATODoLxYyOHF43gB8CGwHCgAbEFe8rx7OB/++Bl4CWgEPAvHAPuAv4Kdj2rIDge4WfsyMIE2bNqVBgwY8//zz+Pj40LFjR4VnERERKXeGYeATE/PPmGg5pXMqPNtsNqZPn86AAQMAePezz9gEjANqAVlAH+BoTC3rc9QzOMcyvwX4A18Cx8bWi4u/8nHO1vE8MLW4/aHAAyeo7VQjgix+fiVejxw5kjvuuIM333zzFGeKiIiISGU5Z8Lz5s2befnllzly5Aj/+te/AMgqKMDXMAgxTXJx9igfKwLnuGYb/7wROTjHJvsDO4FvgaOxNgk4gHNIhnfxMUeHcgzG2QvdFefQDzv/jHNugHN4RyLO4F5WkPaKiirxetiwYcTFxdG5c+fTeRtEREREpAJV60EsTzzxhGue58GDB1O7dm3++usvatWqBcCjjz2Gd2AglwDXA62PO78fEISzJzmheNvTOGfU6Am8hnPGjaNswEfF2/riHKbxQvG+ZsB/gDHF7V1WfGxh8f5BOMdaX4Szh7oELy8CjwvJ/v7+9OvXj+DgYDffDRERERGpaOf88tzJ995LyrhxmFV5rmfDIOb996l5332erkRERERETqJa9zy7I6BDh6odnAFMk4AOHTxdhYiIiIicwjkfnoN69fJ0CadkCQwkoG1bT5chIiIiIqdwzodnvyZNCOrd++yXo6woXl7UuO22UrNtiIiIiEjVc86PeQZI/+47dlx7rafLOKGWmzbh16yZp8sQkXJimiZpuYfIzs+kyFEIGPhYfQgLqEGIX5inyxMRkbNwXoRns6iIv+vVw3bokKdLKclqJahHD+LnzfN0JSJyFhwOO5sOrmHboQ3sSttGcloi+bbcMo8N8g2lfmRT4iIa07xWW+pHNsUwTjUTvIiIVBXnRXgGOPTBB+x+4ERLmHhO0zlzCO7Tx9NliMgZyMrPYPGO2fy5fSZH8tKwGFYcpv2U5xkYGIaBw3RQJzSWPk0G0jGuF75eGr4lIlLVnTfh2XQ42NKrFznLloHN5ulywGKhxh13EPfRR56uREROU35RHj/+PYkFibMwTROTM/9r1MDAxMTH6suAVkPp2/RKLJYq+oyGiIicP+EZoCAxkQ2tWmHm53u2EIsF7zp1aLlpE1YtgiJSrWw9tI4vlr1LRm7qWYXmE4kLb8zNXR6mdki9cm9bRETO3nkVngEOjRnDbk8vRmIYNJk9m5CLLvJsHSLiNrvDxnerx/Pn9hmu3uKKYDEsGBhc3XYkfZoOrJBriIjImTvvwrNpmux57DEO/fe/Hqsh7tNPqXHrrR67voicniJ7IeMWjWLD/lUVFprLcnGzq7mqzc16oFBEpAo578IzOAP07vvv5/CYMZV+7Zj336fm/fdX+nVF5MzY7EV8tPBVNh9YU6nB+aiLml7J4LYjFaBFRKoIL08X4AmGYRDzwQd4RUay/+WXnQuo2E/9hPwZ83K+zfU/+4zI4cMr7joiUq5M0+SLZe96LDgDzNn6I8F+YVzSfLBbx2flHyE5PZHd6YkcyUuj0F4Ipom31Ycgv1BiwhsSG96IMP9IBXIRkTNwXvY8HytzzhySbrmFon37wOEo/wsYBv5t29Jg4kT8W7Ys//ZFpMIs3TmHicvf83QZGBg8fvEbxEU0LnP/rrTtLNg+iw37V5GZnw44x06DAa7Q7/zeYTr/ngv0CaZpzdb0bHwpTWu2VpAWEXHTeR+eAezZ2ez5v/8j5X//c/YSl8dUdlYrGAZ1X3qJ2o8/juF1Xnbyi1RbGbmpvDTrPgpsHp6dB2cQjgqqw1P938Hb6g1Akb2Iv5Ln8+fWGezO2OH2HNPHt+swHUQF1aF3k8vp2qAvft7+FXELIiLnDIXnY+SuXs2hDz8kbdIkzELnrzpPi8UCDgfW0FBq3HUXUXffjW+DBhVTrIhUGNM0GTP/ZTYfXOPqqfU0A4P+La7litY3sit1GxOWvc2hrH3lNPOHs9c5xC+U4Z0eokWddmdfsIjIOUrhuQy29HRSv/iCtC+/JG/tWsyCAucOb2/XL0FdvwwtKgLAEhRE4IUXEjlyJOHXXYfFTyuFiVRXmw6s4YM/X/B0GWXq1XgAC7bPcq1QWJ6OBvGuDfpyTdtb8fcJLNf2RUTOBQrPp2DabORv3kzuypXkb96MIzcXR2EhFj8/LEFB+LdqRUCHDvg2aqQxgyLniI8W/IcN+1dWmV7nymYYFkL9wnkg4UUt1iIichyFZxGRY6TlHOb5n+/02OwaVYXFsODr5c9DfV4mJryhp8sREakyLJ4uQESkKlm04zfQb5FwmA4KbHm8N+95Dmbu9XQ5IiJVhsKziMgxVu9ejHmeDtc4nsN0kF+Uy3vznievKNfT5YiIVAkKzyJy3nvhhRcYNGgQhbYCDmXtO+v21s5MZNqLC8uhstM37tYZ7N2QUm7tOUwHR/LS+WHNhHJrU0SkOlN4FpFqISEhAcMwmD17dontb7zxBoZh8PDDD7vVzoQJE2jbtm2Z+/Zk7KyUsc4T7/+NMUOnk7E/u8T2+Z+tZczQ6aydmVhu18rNyGfOR6sZf9cvfHzzz0x5bA7rf9tZ5rGb5yczZuj0UvtNHCza8RubDqwpt7pERKorhWcRqTbi4+MZP358iW3jx4+nWbNm5dJ+cnoiBpUz3jmsbhCb/0x2vbYX2dm+ZB+htctveriCnCK+//dCivJsXPtKL277bAC9RrZm1Y/bWDJlY6njN89NxjfIm03zdpXaZ2DwxbJ3yS/KK7f6RP6/vTuPrqq89z/+3vvkZDxkIiGBkBDIAAgIgoLIWCIiCC2UtlIUESlWf7YV1s/h1nt/3lbrdK3V1l6sKCD4K3RQLrSAggxhBhEoMicEyAQJCRlISE7IOWffPyKBGEgOChwgn9daWYucvffzfHdY6+Rznjz7eURuRNr2TkRuGBMmTOAPf/gD5eXlhIWFsW3bNgD69evX4LysrCymT5/O1q1bCQ4OZtq0aTz33HPs3r2bxx57jNraWhwOBwD799eFSLfbzav/8QZrl27EP8iP/g90I+WuuLpjLg+f//0gmRvzcJ11E9ctisGP3EpQaAAAJbmnWfvuvyjJO010pwjaJIU3ey9dhsSzZ8VR+v2oK4ZpcGR7AW2SwnHVNNwlsOhoGZs+3Mep7HICHP70/m4yt6QlAmB5LD7/+0H2r8nGMKHP2NQG1+5enoVpMxj+5O2YZt2Hgrhu0dz9RB+WvLiJbmmJhLYJBqDsRCXHD5xi5FN9+eSNzynOLieqQ1h9WxYWp52lbM9ez6DkEc3en4jIzUojzyJywwgPD+fee+9l4cKFAMyZM4cpU6Y0OKeqqoq0tDTS0tLIz89nw4YN/OUvf2Hu3Lncdttt/OlPf6JHjx5UVlZSWVlJQkICACtWrKBzr05Mmz2avvd3IX3WLs5W122CtHNJJtk7Cxj364FMens4hmHw2ds7APC4PSx/fRtx3aN45P1R3DmhKwfWNB65bXQv7VrhaB1E7pcnATiYnk2XoQkN76XMyT9e2kz34YlMeW8kI5/qy+cfHSRvT1HdNetyOLguh7HPD+CBt+6m6EgZtdWu+utzvzxJ8p3t6oPzOe26tiY4PIC8vUX1rx1IzyEqMYyOt7elXZfWHFh78dHn9MylaIVTEWnJFJ5F5IYyZcoU5s6dS3V1NR9//DGTJk1qcHzZsmVEREQwffp0/P39SUhI4Mknn2TBggVNttu7d2/6pvXCMA06D4rH7bIoO3EGgIwNufQZ15lWUcHYA/0YMKkbeXuKOFNSTUFGKc6Ks9zxgy7Y/ExiUyNJ7h/n1b10HZrAgfQcKk9VU3ysnI59YhscP7Qhl3ZdW5PcPw7TNGgdH0qXIQlkbMqrq2tjHrfe24mIuFbYA/y488e3cGGudVacJTji4rudhkQEUl1et3uqx2NxaH0OnQfHA5A6KJ6MjXm4axuOgltYFJzO5UjxQa/uT0TkZqRpGyJyQ0lLS2Pq1Km8+OKL9O/fn9jYhoHz2LFj7N27l/Dw8PrXPB4P8fHxTbYbGxuLn82OARiGgZ+/WT+KW3mqmtDooPpzQyKDsNlNKkucnCl1EhwRiM3v/FhEq+ggSvMrmr2X5P5xbFmwn93Ls0juH4fNbmtwvKKoiuxdJ3n/kWX1r1kei7ZdWgNwptSJIyq4/lhweCA2+/k6Alv5U1XqvGjfZ0qdBIX6A5CzqxDn6bOkDqzbTTD5znZs/GAPR7afIOWuhjsMmobJhqxPSYru2uz9iYjcjBSeReSGYpomkydP5qWXXuKjjz5qdDw+Pp4+ffqwdevWS15/KUH2kIuuteFoHcTpompiUuq+rypz4q714IgMxF3roarUidvlqQ/QFcVNP1RXc6aWnYsz6HRHWzrcFsPu5Vn88OUhF+230x1tuefJ2y/aTkhEIJXFVexensXR7Se4Z/oduGvPr1Ed3yOaw1uPc/sPujSYunHi4CnOlDppd0sUAAfWZmNZ8Jen19af43F7OLA2p1F49lgeMk7uafL+mvLwww8THh7OW2+99Y3bEBHxJU3bEJEbzowZM1i5ciVjxoxpdGz06NEUFhYyc+ZMnE4nbrebQ4cOkZ6eDkBMTAwnTpygurpxwI0LT8Rj1U1VqHW6WPLiJnL3nCR1YHt2Ls6goriaLxYd4oPHVuBoHURIZBAxKREEOPzZsegQbpeHwswSDm85vyPfwfQc/vrs2kZ9ndN/4i187z8GEN0xvNGx1EHx5O8rImvbcdwuD26Xh+Jj5RRmlQKQMqA9e1Yepaq8Bsuy2Lpwf4PNEW8dlYTH5eH9Kct4d9I/ee/hpcyavJQlL24iqkMYIRGBVJU5yd5VyG3fTaG6vIZapwtXjQu/AD/y9hSxb9Wx+vYsy2LxC5tY+k46lTWnG/x/DB48GI+nLrgbhkFwcDAOh4OYmBgmTpxIUdH5+dUiIjcyhWcRueFERkZy9913Y7fbGx1zOBysWrWK1atXk5iYSOvWrZk4cSIFBQUADBs2jDvvvJO4uDjCw8PJyTm/XFyHyOSGbbUO4mB6Lr3HphLfsw2Lnl/P9o8OYg/0I75nNAA2P5NRT/cjZ/dJZk9dzpaFB+g6tIPX9xISGURct6iLHnNEBjH6l/3Zt+oY8x5fwQc//ZT1c76ktqpuOknXoQmkDmzP3k+PUJhZSnRiGPag839QDHT48/0XBtXX6TrrwVXjJmVAHP7Bdhb95wb2r8nG0TqI9t2j8A/249F5o5n2wWge+u/htGoTxKb5e9jxPxlAXSge9lgvDq3L5eNlfwUgPT2d2bNn88EHHzQY1d+8eTOVlZXs2bOH/Px8r9fhFhG53hmWHpsWEQHqRlafWjQRp6uaxb/eSLtbotjz6REefHs4AcF2CjNLWPOnXbRJiiAgxM7AyT0AKC84w8b5eyjMLMUvwMYtwzrQZ2wqxdnlLHp+Ax6XB5t/3XzmH78xjANrcyg6WkarqGAyNubiH2T/1kvjnTxcytj/HHjR+/rwZysZMLkHib1j+OSNzwH4zk9v46/PrOWO8Z3pfk9H8vcV88kb2/jJnPsaXJu5OY817+xi8jsjCHTUzZHevyqbA58cZ+fnuxgwYADPPPMMjz/+eP01hmGwa9eu+s1o/vjHP/Luu++yZ8+eBtM20tPTGTt2LGVlZfXXjh07ll69evGrX/2KkpISpk6dSnp6OpZlkZSUxKJFi+jQwfsPJyIiV5pGnkVEvmIYBh2jumAYdW+NASF2Enq2IfOr1S0OpOc0Wk6utsbFP36zifbdo5n8zgjG/WoghzfncyA9h+iO4QyZ2pPIhFAenTeaR+eNptVXD/jl7j5Ju66teeT9UddkaTwA02Zyz5O3E5MSQcXJM7TvEU3+/qa38u7Utx0el4fCzNL613oMTyI2PpqePXuSkpLSIDh/XUFBAX/729/o3bu3VzVe6Le//S0ul4v8/HxOnTrF7NmzadWq1WW3IyJyJSk8i4hc4K5Ow7Gs8w/ddRmawMH0HFxn3Rz5/ASdBzVctSN7VyEBDn96jkrC5mfSKiqYW0d2qg/clxLdMbx+CbprtTQegD3Aj9vHdSYmJRJHRCA1Z2qbPN/mZxIYGkDNmbMXvGrR+bYkiouLmTx58kWvGzRoEBEREfTt25ekpCTefPNNr2usr9Vu59SpU2RmZmKz2ejVqxeRkZGX3Y6IyJWk1TZERC7QM64vjoDQ+u/bd49m7bv/4ouPDxGTEkFweMN1kyuKqijJPd1wOTmrbr50U4LDA+r/fa2Wxvu6ylInASGN541fyO3y4DxdQ0CIf/1rJXmn+ce8DcyYMYNnn32W0aNHExYW1uC6DRs21E/b+KaefvppnE4nP/rRjygvL+f+++/n1VdfJSio6Z+tiMjVpJFnEZEL2Ew/BieP5NyyFYZp0HlwPDv/kdloygbUheTojuH8ZM599V/T5t7Hj387rO6Eb/Aue25pvHMuXBovJCKwfmm8c5pbGu9iaqpqydtTRNwtF39Y8Zwjnx/H9DOJSYkA6qaNfDZzB6MfGMHvfvc7evXq9Y0fBnQ4HFRXVzfYsfDEiRMNjr/22mscOnSILVu2sHr1ambOnPmN+hIRuVIUnkVEvmZg0ggMzq/51vO+JMY815/E3rGNzk3sHUtVeQ17Vx7FddaNx2NReryC/H11c4mDwwKpKqvBddbd6NpLuXBpvFqni03z99K+R7RXS+M1x/JYFB0tY8Wb2wkOD7joBwKoW6ovc3M+G+bu4fbvd65/WHDnkkzcLjePPlm3LfqsWbNYsmQJy5cv97qG+vtMTcVut7NgwQLcbjcLFy5k165d9ceXLl1KRkYGHo+H0NBQ7HY7fn76g6mI+JbehUREviYsKJJoRyx8tWVKoMOf+B5tLnquPdCP7/77XWxZsI/tHx/CXesmLCaEXmPqlr2L6xZFTHIE8x5fgWVZ3P9f32m2/95jU6mtcbPo+fW4az3E3RLF3U/UPXB3bmm8te/u4l/LsmiTFEHXoR04mVXaZJuf/eELTJuBYRiExoSQ2CeW28Yk4+d/flfDs1UuZk1eimEa+PnbiEoMY+ijveh0R1sAirPL2bkkk++/MIhObToD0K5dO9566y2mTZvGvn37Guzs2JzQ0FDee+89nnnmGZ544gkefPBBRowYUX/88OHD/OIXv6CwsBCHw8H48eObfDhRRORa0FJ1IiIXYVkWv1/7/8gq3o/nggcIBQwM3hi/kAC/wOZPFhG5yWjahojIRRiGwaR+P8dm+jWYwiEQ5YhVcBaRFkvhWUTkElqHxDBtwL9hGAYoQANgGibd293u6zJERHxG4VlEpAnd2vZm6l1PYxiGRqABj+VhUNK9vi5DRMRnFJ5FRJrRq31/Hhv4HKZpwzRa7tumaZiktulBTKj3m7KIiNxsWu5vARGRy9C93e388p43iQtPvIq91I1sp0R3u4p9fHMey8OQlFG+LkNExKcUnkVEvNQ2LJ6n736d7936EKZxZUehTcMkyB7MlP7/lye/8xt6xw+4rka5TcMkISKJW9v19XUpIiI+paXqRES+gYLTeaw6uJjt2em4PW4sLv+t1MDAwiLIHszApBEM6/w9QgPDAahwlvPCJ09QffbMN2r7SrMZNp679/fEhrb3dSkiIj6l8Cwi8i1Una1k69E1rD+8nKLKAqBulNayrEah9+uvd4hMYWjKfdwWfxd2m3+jtnflbub9zf919W/CC2N7TmZ4l3G+LkNExOcUnkVErpAKZzk5pVnklmaRU5LFaWcpZ901GJj4+/kTGdyGhMgkEiKSaB/RiSB7cLNtztv6Ftuz1/ls9Nk0TBJbpzLjOy9hmrbmLxARuckpPIuIXMdc7lre2fAShwp3X/MAbRomsaHtmTHsZYL9Hde0bxGR69X18zSKiIg04mez89OBv6RLbK9rus60aZi0DU3gF0NfVHAWEbmARp5FRG4ALnctC76YybZja+sfNLyaOre5lWkDniXIP+Sq9iMicqNReBYRuYHsztvKn7f/N1W1Z7AszxVt2zRMbKYf43tNZWDSPV9tSy4iIhdSeBYRucFU1pzm7zvf44ucDZiGiedbhmjTsOGx3KS26cGDfX9O65A2V6hSEZGbj8KziMgN6nh5DhsPf8qWo6u/WtXD++kc5+ZPm4ZJn4RBDE4ZRWJkikabRUSaofAsInKDc9ZW80XOeg4U/ItjpzIoqz7V5PmOgDASI1NIadOdOzsOwxEQeo0qFRG58Sk8i4jcZM7UVJBbeoSy6lPUumux8GC3+dMqIIz4iCTCgiJ8XaKIyA1L4VlERERExEta51lERERExEsKzyIiIiIiXlJ4FhERERHxksKziIiIiIiXFJ5FRERERLyk8CwiIiIi4iWFZxERERERLyk8i4iIiIh4SeFZRERERMRLCs8iIiIiIl5SeBYRERER8ZLCs4iIiIiIlxSeRURERES8pPAsIiIiIuIlhWcRERERES8pPIuIiIiIeEnhWURERETESwrPIiIiIiJeUngWEREREfGSwrOIiIiIiJcUnkVEREREvKTwLCIiIiLiJYVnEREREREvKTyLiIiIiHhJ4VlERERExEsKzyIiIiIiXlJ4FhERERHxksKziIiIiIiXFJ5FRERERLyk8CwiIiIi4iWFZxERERERLyk8i4iIiIh4SeFZRERERMRLCs8iIiIiIl5SeBYRERER8ZKfrwsQkZufZVmUVBVR4Syj1l2LYYDd9CcsKJKwoEgMw/B1iSIiIl5ReBaRK85jeThUuJuMk3vJLjlMTkkm1bVVFz03xL8VHSKT6RCZQueYniRH36IwLSIi1y3DsizL10WIyM2hsuY0W46uZl3mMkqrijENGx7L3ex1BgaGYeCxPLRp1Y6hKaPpmziUIHvwNahaRETEewrPIvKtnXXVsHTvAtIzluKxPFh8+7cVu82fe7qOZ0TX8dhM/ZFMRESuDwrPIvKtZBUdYN62tyg5c/KKhOavaxeWwOR+M2gf0fGKty0iInK5FJ5F5BvxeNws/vJDVh9ajGmYeCzPVenHNEwsYEz3idzTdbzmQ4uIiE8pPIvIZXO5a5m79Xf8K2/LNe13UNK9/KjPo5iGVtkUERHfUHgWkcvi9rh5f/Nr7MnfflWmaTTnrk7DmXj7/9EItIiI+ISGb0Tksiz8YiZf+ig4A2w+8hlL9y7wSd8iIiIKzyLitZ25m9lydDX4KDif8+n+v5N5cp9PaxARkZZJ4VlEvFLhLGfhFzMx8P10CQOT+dveosbl9HUpIiLSwig8i4hX/rrjXZy1VT6brnEhCw+lVcX848v/7+tSRESkhVF4FpFmHSk+yK68zVdtObpvwsIiPXMpJyuO+7oUERFpQRSeRaRZ6zKXX5fLw5mGyYbDn/q6DBERaUGuv9+GInJdqXCWsTN303U16nyOx/Kw+chnnHXV+LoUERFpIRSeRaRJW46uxroOg/M5Tlc1X+Rs8HUZIiLSQig8i0iTduVuafIhwc//fpBPfrvtivW3e3kWi3+90evzDQy+zL9y/YuIiDRF4VmkBRg6dCiGYbBq1aoGr7/++usYhsH06dMvep3b4+Z4eXb99wfTc/jrs2uvZqleW/LiJt6d9E+qK2s4VpLp63JERKSFUHgWaSE6d+7M3LlzG7w2d+5cunTpcslrCk7n4vLUXu3SLlt54Rny9xfjF2Ajc2MeFc4yTjvLfF2WiIi0AArPIi3EhAkT+OSTTygvLwdg27a6qQ79+vVrcF5WVhZjxowhOjqaPt368cWiQ1gei6KjZaybvZuSnNPMmryUWZOXUlFcBYDHY7F+zpe8/8gy5j+xkszN+fXtuV0etizcz/wnVjJn2ieseGs71afPP+BXknuaj/9jPe89vJTFL2ziTGnzG58cTM8hqkMYPUZ04kB6DgA5JYfp2bMn8+fPb3DuyJEjeeWVVwDIy8tj+PDhhIaG0qdPH15++WUSExMv8ycpIiItmcKzSAsRHh7Ovffey8KFCwGYM2cOU6ZMaXBOVVUVaWlppKWlkZ+fz8tzn+Pw5uMcSM8humM4Q6b2JDIhlEfnjebReaNpFRUMQO7uk7Tr2ppH3h9F3/u7kD5rF2er60asdy7JJHtnAeN+PZBJbw/HMAw+e3sHAB63h+WvbyOuexSPvD+KOyd05cCabJri8VgcXJdDlyHxdB4cT3F2OcVHyymuLGDSpEl8+OGH9ecWFBSwevVqHnzwQQAmTpxIhw4dKCwsZOHChcyePfvK/HBFRKTFUHgWaUGmTJnC3Llzqa6u5uOPP2bSpEkNji9btoyIiAimT5+Ov78/YdGt6DkqmcxNeU22G90xnOT+cZimQedB8bhdFmUnzgCQsSGXPuM60yoqGHugHwMmdSNvTxFnSqopyCjFWXGWO37QBZufSWxqJMn945rsK3f3SarLa0gZ0J6wmBDapkZyYG02tZ5aHnjgAdatW0d+ft3I98KFCxk0aBDx8fHk5uayYcMGXn31VYKCgkhNTeWxxx77Fj9NERFpiRSeRVqQtLQ0Tpw4wYsvvkj//v2JjY1tcPzYsWPs3buX8PBwwsPDeWjwz9j44ZdUlTe9jnJweED9vw3DwM/fpLbaBUDlqWpCo4Pqj4dEBmGzm1SWODlT6iQ4IhCb3/m3olYXnHsxB9Zm0+G2GIJC6/rsPCSBjE15VFdX0bZtW4YNG8af//xnAObPn89DDz0EwPHjxwkMDCQqKqq+rYSEhCb7EhER+To/XxcgIteOaZpMnjyZl156iY8++qjR8fj4ePr06cPWrVsBWLx7Hmsy/onb4/qqgcvv09E6iNNF1cSk1H1fVebEXevBERmIu9ZDVakTt8tTH6Ariqsv2Vb16RqO7SjAZjeZ+9O6nQUtj0XNmVq2rt7Bd297gEmTJvHqq68yatQoMjIyGD9+PADt2rXD6XRSXFxcH6BzcnIu/4ZERKRF08izSAszY8YMVq5cyZgxYxodGz16NIWFhcycOROn04m/GUhJfjn5+4oBCA4LpKqsBtdZt9f9pQ5sz87FGVQUV1PrdLFp/l7a94gmJDKImJQIAhz+7Fh0CLfLQ2FmCYe35F+yrUPrcwlw+PPj36Vx/2tDuf+1oUx4/Tt0GRLPio/rltAbN24c2dnZPPXUU4wbNw6HwwHUfTAYMGAAzz33HNXV1WRmZjJr1qzL+dGJiIgoPIu0NJGRkdx9993Y7fZGxxwOB6tWrWL16tUkJiYyYeBPWPmH7VSV162AEdctipjkCOY9voL3H1lWv9pGU3qPTSW+ZxsWPb+eD3/+GR63xd1P9AbA5mcy6ul+5Ow+yeypy9my8ABdh3a4ZFsH1mbTfXgijsgggsMD6796jk5m17bdZGVlERwczPjx41mxYkX9lI1zFixYwJEjR4iJiWHChAk8+OCDBAQEXKI3ERGRxgzLsi69dZiItGgVzjL+bcnDvi6jWaZh483xf8HP1vgDQVNeeeUV1qxZw2effXaVKhMRkZuNRp5F5JJaBYbTKjDc12U0q21ovFfBeefOnRw8eBDLstixYwdvv/02P/zhD69BhSIicrNQeBaRJiVHdcU0rt+3CtOwkRx9i1fnFhUVMXLkSEJCQhg/fjzTpk1j6tSpV7lCERG5mWi1DRFpUv9Ow9mVt8XXZVySx3JzZ8c0r84dMWIER48evcoViYjIzez6HU4SketC19heRARHNX+iDxgYJEQkkxCZ5OtSRESkhVB4FpEmmYbJ0JTRGBi+LqURC4uhqff5ugwREWlBFJ5FpFn9O6YR4BcI11GANjAIC4qkd/wAX5ciIiItiMKziDQrJKAV9/d5DLh+Vra0sHio75PYbf6+LkVERFoQhWcR8codHQbTve3t18XKGwYmAzrdQ5fYnr4uRUREWhjf/xYUkRuCYRhMvOMJ/P0CfTr/2TRMQoPCGdfrYZ/VICIiLZfCs4h4LSwogscH/TumafNJgDYwsdsCeGLw8wTZg695/yIiIgrPInJZkqO78dOBz2Ea5jUN0KZhYrfZ+fmQXxEXnnjN+hUREbmQYVnW9fMEkIjcMDJO7uGd9b/B5anFY3mual+mYRJoD+YXQ18gPqLTVe1LRESkKQrPIvKNFVWeYP6233Ok+OBV7eeW2N480PdnhAdFXtV+REREmqPwLCLfisfysD5zOf+zex4ey33FRqHrpmkEcH/vR+mbOBTDuH7WmBYRkZZL4VlErojiygJWHVzM1mNrqHWfpW5Dlct7ezEwsLAI9AtiQNI9DOv8PY02i4jIdUXhWUSuqOraKrYfS2dd5nIKKvIAMAwTLAvra2HaNEysC16Pj+jEkJT76BM/EH+/gGteu4iISHMUnkXkqqmsOU1u6RFyS7PIKcmi3FnKWVcNhgF2WwARwVEkRCSREJlEfHgngvxDfF2yiIhIkxSeRURERES8pHWeRURERES8pPAsIiIiIuIlhWcRERERES8pPIuIiIiIeEnhWURERETESwrPIiIiIiJeUngWEREREfGSwrOIiIiIiJcUnkVEREREvKTwLCIiIiLipf8F7fcLOHYZP8wAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "deg = dict(G.degree())\n",
        "top = sorted(deg.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "df_deg = pd.DataFrame(top, columns=[\"entity\",\"degree\"])\n",
        "\n",
        "ax = df_deg.plot.bar(x=\"entity\", y=\"degree\", figsize=(8,4), legend=False, rot=30)\n",
        "ax.set_title(\"Top-degree entities\")\n",
        "ax.set_xlabel(\"Entity\")\n",
        "ax.set_ylabel(\"Degree\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "jAAEXUwKa2tH",
        "outputId": "eb16079a-527f-4406-c5f3-8ada1a9c04ec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [\"doc_id\",\"head\",\"type1\",\"tail\",\"type2\",\"sentence\"]\n",
        "display(edges_df[cols].head(12))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "oNYkmMmTa5Np",
        "outputId": "899cece0-2a73-49e3-bbfd-fe19436576a6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                               doc_id              head  \\\n",
              "0   NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method\\nMedical   \n",
              "1   NeurIPS-2024-mdagents-an-adaptive-collaboratio...        Dataset\\nT   \n",
              "2   NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method Accuracy   \n",
              "3   NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method Accuracy   \n",
              "4   NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method\\nMedical   \n",
              "5   NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method\\nMedical   \n",
              "6   NeurIPS-2024-mdagents-an-adaptive-collaboratio...  Method\\nClinical   \n",
              "7   NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method\\nMedical   \n",
              "8   NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method\\nMedical   \n",
              "9   NeurIPS-2024-mdagents-an-adaptive-collaboratio...  Method\\nClinical   \n",
              "10  NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method\\nMedical   \n",
              "11  NeurIPS-2024-mdagents-an-adaptive-collaboratio...   Method\\nMedical   \n",
              "\n",
              "      type1              tail   type2  \\\n",
              "0    METHOD  Method\\nClinical  METHOD   \n",
              "1   DATASET          Accuracy  METRIC   \n",
              "2    METHOD          Accuracy  METRIC   \n",
              "3    METHOD          Accuracy  METRIC   \n",
              "4    METHOD  Method\\nClinical  METHOD   \n",
              "5    METHOD          Accuracy  METRIC   \n",
              "6    METHOD          Accuracy  METRIC   \n",
              "7    METHOD  Method\\nClinical  METHOD   \n",
              "8    METHOD          Accuracy  METRIC   \n",
              "9    METHOD          Accuracy  METRIC   \n",
              "10   METHOD  Method\\nClinical  METHOD   \n",
              "11   METHOD          Accuracy  METRIC   \n",
              "\n",
              "                                             sentence  \n",
              "0   Category Method\\nMedical Knowledge Retrieval D...  \n",
              "1   Report Generation\\nD.1 Accuracy on entire MedQ...  \n",
              "2   Category Method Accuracy (%)\\nSingle-agent\\nZe...  \n",
              "3   23\\nD.3 Impact of Knowledge Enhancement with R...  \n",
              "4   Category Method\\nMedical Knowledge Retrieval D...  \n",
              "5   Category Method\\nMedical Knowledge Retrieval D...  \n",
              "6   Category Method\\nMedical Knowledge Retrieval D...  \n",
              "7   Category Method\\nMedical Knowledge Retrieval D...  \n",
              "8   Category Method\\nMedical Knowledge Retrieval D...  \n",
              "9   Category Method\\nMedical Knowledge Retrieval D...  \n",
              "10  Method\\nMedical Knowledge Retrieval Datasets\\n...  \n",
              "11  Method\\nMedical Knowledge Retrieval Datasets\\n...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-24e9cde0-c84b-4904-ac0c-28d709e4b9ef\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc_id</th>\n",
              "      <th>head</th>\n",
              "      <th>type1</th>\n",
              "      <th>tail</th>\n",
              "      <th>type2</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nMedical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Method\\nClinical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Category Method\\nMedical Knowledge Retrieval D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Dataset\\nT</td>\n",
              "      <td>DATASET</td>\n",
              "      <td>Accuracy</td>\n",
              "      <td>METRIC</td>\n",
              "      <td>Report Generation\\nD.1 Accuracy on entire MedQ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method Accuracy</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Accuracy</td>\n",
              "      <td>METRIC</td>\n",
              "      <td>Category Method Accuracy (%)\\nSingle-agent\\nZe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method Accuracy</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Accuracy</td>\n",
              "      <td>METRIC</td>\n",
              "      <td>23\\nD.3 Impact of Knowledge Enhancement with R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nMedical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Method\\nClinical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Category Method\\nMedical Knowledge Retrieval D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nMedical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Accuracy</td>\n",
              "      <td>METRIC</td>\n",
              "      <td>Category Method\\nMedical Knowledge Retrieval D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nClinical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Accuracy</td>\n",
              "      <td>METRIC</td>\n",
              "      <td>Category Method\\nMedical Knowledge Retrieval D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nMedical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Method\\nClinical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Category Method\\nMedical Knowledge Retrieval D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nMedical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Accuracy</td>\n",
              "      <td>METRIC</td>\n",
              "      <td>Category Method\\nMedical Knowledge Retrieval D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nClinical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Accuracy</td>\n",
              "      <td>METRIC</td>\n",
              "      <td>Category Method\\nMedical Knowledge Retrieval D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nMedical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Method\\nClinical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Method\\nMedical Knowledge Retrieval Datasets\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>NeurIPS-2024-mdagents-an-adaptive-collaboratio...</td>\n",
              "      <td>Method\\nMedical</td>\n",
              "      <td>METHOD</td>\n",
              "      <td>Accuracy</td>\n",
              "      <td>METRIC</td>\n",
              "      <td>Method\\nMedical Knowledge Retrieval Datasets\\n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24e9cde0-c84b-4904-ac0c-28d709e4b9ef')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-24e9cde0-c84b-4904-ac0c-28d709e4b9ef button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-24e9cde0-c84b-4904-ac0c-28d709e4b9ef');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e3405125-5176-48d8-bc04-6528d1345afd\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e3405125-5176-48d8-bc04-6528d1345afd')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e3405125-5176-48d8-bc04-6528d1345afd button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(edges_df[cols]\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"doc_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"head\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Dataset\\nT\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type1\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"DATASET\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tail\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Accuracy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type2\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"METRIC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Category Method\\nMedical Knowledge Retrieval Datasets\\nMedQA\\nT\\nPubMedQA\\nT\\nPath-VQA\\nI T\\nPMC-VQA\\nI T\\nMedVidQA\\nV T\\nSingle-agent\\nZero-shot 75.0 \\u00b11.3 61.5 \\u00b12.2 57.9 \\u00b11.6 49.0 \\u00b13.7 37.9 \\u00b18.4\\nFew-shot 72.9 \\u00b111.4 63.1 \\u00b111.7 57.5 \\u00b14.5 52.2 \\u00b12.0 47.1 \\u00b18.6\\n+ CoT [83] 82.5 \\u00b14.9 57.6 \\u00b19.2 58.6 \\u00b13.1 51.3 \\u00b11.5 48.6 \\u00b15.5\\n+ CoT-SC [82] 83.9 \\u00b12.7 58.7 \\u00b15.0 61.2 \\u00b12.1 50.5 \\u00b15.2 49.2 \\u00b18.2\\nER [67] 81.9 \\u00b12.1 56.0 \\u00b17.0 61.4 \\u00b14.1 52.7 \\u00b12.9 48.5 \\u00b14.1\\nMedprompt [59] 82.4 \\u00b15.1 51.8 \\u00b14.6 59.2 \\u00b15.7 53.4 \\u00b17.9 44.5 \\u00b12.0\\nMulti-agent\\n(Single-model)\\nMajority V oting 80.6 \\u00b12.9 72.2 \\u00b16.9 56.9 \\u00b119.7 36.8 \\u00b16.7 50.8 \\u00b17.4\\nWeighted V oting 78.8 \\u00b11.1 72.2 \\u00b16.9 62.1 \\u00b113.9 25.4 \\u00b19.0 57.8 \\u00b12.1\\nBorda Count 70.3 \\u00b18.5 66.9 \\u00b13.0 61.9 \\u00b18.1 27.9 \\u00b15.3 54.5 \\u00b14.7\\nMedAgents [72] 79.1 \\u00b17.4 69.7 \\u00b14.7 45.4 \\u00b18.1 39.6 \\u00b13.0 51.6 \\u00b14.8\\nMeta-Prompting [70] 80.6 \\u00b11.2 73.3 \\u00b12.3 55.3 \\u00b12.3 42.6 \\u00b14.2 -\\nMulti-agent\\n(Multi-model)\\nReconcile [10] 81.3 \\u00b13.0 79.7 \\u00b13.2 57.5 \\u00b13.3 31.4 \\u00b11.2 -\\nAutoGen [86] 60.6 \\u00b15.0 77.3 \\u00b12.3 43.0 \\u00b18.9 37.3 \\u00b16.1 -\\nDyLAN [51] 64.2 \\u00b12.3 73.6 \\u00b14.2 41.3 \\u00b11.2 34.0 \\u00b13.5 -\\nAdaptive MDAgents (Ours) 88.7\\u00b14.0 75.0 \\u00b11.0 65.3 \\u00b13.9 56.4 \\u00b14.5 56.2 \\u00b16.7\\nCategory Method\\nClinical Reasoning and Diagnostic Datasets\\nDDXPlus\\nT\\nSymCat\\nT\\nJAMA\\nT\\nMedBullets\\nT\\nMIMIC-CXR\\nI T\\nSingle-agent\\nZero-shot 70.3 \\u00b12.0 88.7 \\u00b12.3 62.0 \\u00b12.0 67.0 \\u00b11.4 40.0 \\u00b15.3\\nFew-shot 69.4 \\u00b11.0 86.7 \\u00b13.1 69.0 \\u00b14.2 72.0 \\u00b12.8 35.3 \\u00b15.0\\n+ CoT [83] 72.7 \\u00b17.7 78.0 \\u00b12.0 66.0 \\u00b15.7 70.0 \\u00b10.0 36.2 \\u00b15.2\\n+ CoT-SC [82] 52.1 \\u00b16.4 83.3 \\u00b13.1 68.0 \\u00b12.8 76.0 \\u00b12.8 51.7 \\u00b14.0\\nER [67] 61.3 \\u00b12.4 82.7 \\u00b12.3 71.0 \\u00b11.4 76.0 \\u00b15.7 50.0 \\u00b10.0\\nMedprompt [59] 59.5 \\u00b117.7 87.3 \\u00b11.2 70.7 \\u00b14.3 71.0 \\u00b11.4 53.4 \\u00b14.3\\nMulti-agent\\n(Single-model)\\nMajority V oting 67.8 \\u00b14.9 91.9 \\u00b12.2 70.0 \\u00b15.7 70.0 \\u00b10.0 49.5 \\u00b110.7\\nWeighted V oting 65.9 \\u00b13.3 90.5 \\u00b12.9 66.1 \\u00b14.1 66.0 \\u00b15.7 53.5 \\u00b12.2\\nBorda Count 67.1 \\u00b16.7 78.0 \\u00b111.8 61.0 \\u00b15.6 66.0 \\u00b12.8 45.3 \\u00b16.8\\nMedAgents [72] 62.8 \\u00b15.6 90.0 \\u00b10.0 66.0 \\u00b15.7 77.0 \\u00b11.4 43.3 \\u00b17.0\\nMeta-Prompting [70] 52.6 \\u00b16.1 77.3 \\u00b12.3 64.7 \\u00b13.1 49.3 \\u00b11.2 42.0 \\u00b14.0\\nMulti-agent\\n(Multi-model)\\nReconcile [10] 68.4 \\u00b17.4 90.6 \\u00b12.5 60.7 \\u00b15.7 59.5 \\u00b18.7 33.3 \\u00b13.4\\nAutoGen [86] 67.3 \\u00b111.8 73.3 \\u00b13.1 64.6 \\u00b11.2 55.3 \\u00b13.1 43.3 \\u00b14.2\\nDyLAN [51] 56.4 \\u00b12.9 75.3 \\u00b14.6 60.1 \\u00b13.1 57.3 \\u00b16.1 38.7 \\u00b11.2\\nAdaptive MDAgents (Ours) 77.9 \\u00b12.1 93.1 \\u00b11.0 70.9 \\u00b10.3 80.8 \\u00b11.7 55.9 \\u00b19.1\\n* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement\\n* T : text-only, I : image+text, V : video+text\\n* All experiments were tested with 3 random seeds\\n4 Experiments and Results\\nIn this section, we evaluate our framework and baseline methods across different medical benchmarks\\nin Solo, Group, and Aaptive settings.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "type_to_color = {\"METHOD\":\"#6aa84f\",\"AUTHOR\":\"#3c78d8\",\"DATASET\":\"#cc0000\",\"PAPER\":\"#674ea7\",\"METRIC\":\"#e69138\"}\n",
        "colors = [type_to_color.get(G.nodes[n].get(\"type\",\"\"), \"#999\") for n in G.nodes()]\n",
        "pos = nx.kamada_kawai_layout(G, weight=None)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "nx.draw_networkx(\n",
        "    G, pos,\n",
        "    with_labels=True, node_color=colors, node_size=900,\n",
        "    edge_color=\"#bbb\", font_size=9\n",
        ")\n",
        "from matplotlib.patches import Patch\n",
        "handles = [Patch(color=c, label=t) for t, c in type_to_color.items()]\n",
        "plt.legend(handles=handles, loc=\"best\", frameon=True)\n",
        "plt.title(\"Graph-RAG — Kamada–Kawai layout\")\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "SccRSaaga_3h",
        "outputId": "c065f85f-e68f-4bd8-8d51-e0265c14263c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.schema import Document\n",
        "\n",
        "lc_docs = [Document(page_content=row[\"text\"], metadata={\"doc_id\": row[\"doc_id\"]}) for _, row in corpus.iterrows()]\n",
        "\n",
        "emb = SentenceTransformerEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectordb = Chroma.from_documents(lc_docs, emb, persist_directory=\"chroma_week6_graph\")\n",
        "retriever_dense = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "def baseline_dense(query: str, k: int = 5):\n",
        "    docs = retriever_dense.get_relevant_documents(query)[:k]\n",
        "    return [{\"doc_id\": d.metadata.get(\"doc_id\"), \"snippet\": d.page_content[:220]} for d in docs]\n",
        "\n",
        "q_demo = \"Which dataset was used to evaluate Method X?\"\n",
        "print(\"\\n[Baseline Dense]\")\n",
        "for e in baseline_dense(q_demo, 5):\n",
        "    print(f\"({e['doc_id']}) {e['snippet']}\")\n",
        "\n",
        "print(\"\\n[Graph-RAG hops=2]\")\n",
        "gr = graph_rag(q_demo, hops=2)\n",
        "for s in gr[\"spans\"][:5]:\n",
        "    print(f\"({s['doc_id']}) {s['sentence']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7f23718fa08d4961bbc7b15326608840",
            "8d04472afb08411ab8a5faef688ba304",
            "5bd3920f8b154956be13e3293e19d047",
            "53f815c2a4934d7ba53e6f654d1fe24b",
            "acf6499983e94ebf8bc7a0dd6ecee7fe",
            "48a6810331214b849490a4258cff238a",
            "0d99a2dbf7534841ba8928c6ec627389",
            "f1e2c4b300ce4688abc3f3d2ec4f7776",
            "697b3c127d7c48849adad34d03f2658c",
            "0c7c88d3b1c04c599f9fa2608c80b39a",
            "ee63ef443011435a9290d7b4fc429be4",
            "81757c8aa4ef434a96e778de2bf027cb",
            "9630a8f3bb2040ae917cfc28bacff368",
            "d7fb75b9b28f4f6c920a058f5e655a24",
            "f608a4b6949e48a68544ceb954622f67",
            "339718118b0f45b691395783af8c903a",
            "00d49b74fde84373a88c47976185a511",
            "8a3ccc81aee14dd3a4d85b297ec26892",
            "bef0930708874c8d88216e2a042f7013",
            "f01a396a9148421c91dbc3daa4f61876",
            "5e0c13b9aa7e495ca52c7f9b2697e10f",
            "9b386594b7f14be6bd22cdf521881065",
            "55c8e3f8377d48de9905df7f5f0a987b",
            "426a1b6ad6d64822b79afb83f70bb9b3",
            "e76fd7c008454acf90777eb1fd99d434",
            "d3b6a19e37894cd4941d885e52d9b526",
            "5e4f38528f424120a77182e8b5c26f2c",
            "79abbaadb4a4485fadd8a5ced47376b1",
            "11d4722a337148adb7301a0a2a24991c",
            "13a7d60304484b6990ae4f25ff1d8d42",
            "d5821ce374744fdcba8ed8bfba28ff9e",
            "8a1a7692aedb448eb87e1fc18cf9d5fc",
            "ba7859b627964556a21b6300b0d01f36",
            "27df6fd930c848ac92c9afbe3943ef04",
            "02d4d6cfd36346ee9f74d45f39ec7353",
            "87b3b6b8f9cf40159abe7177380c6812",
            "579b8b70f9bf4bf28d4d13b338976c12",
            "a2cf6b2c2f9f4d5db84a4fd47d2feab7",
            "0cda6b9f35e5489e9bca24aaac602e50",
            "5ae583732b7b435e92fe490f066de6be",
            "9dabc122c4204393b9b43ddd71abb58c",
            "3f5485eb9f8e4db4be0504dac858a64d",
            "01523f1b31274e8aa9f9e99018f15687",
            "e35b89b848394b78b538299b76d6a013",
            "c0372025423942d7a1c448741eebf2ea",
            "b077969df15140d0b4ffc2b7b72d8bd2",
            "43176ec262ab4c28bbebcbe50281a968",
            "c9cf319e6a434d94912a65ffb783fa5f",
            "5459959ffcbb4573ad99671b15931bae",
            "261364d7bb28417fb46700baa57675fc",
            "3e2c4c862c10407e8dd2a90fbc56c9b7",
            "4732078b57fc43388f63f806503a35df",
            "48da6ec8ce5d43fb97dd11472f40c9b1",
            "0b06875d4ef24cea92bcfeba6dcf0499",
            "6f8676ceb1e94ae7bd93a4065e907334",
            "2a93a69d63094fe4a80fa8bc847eea67",
            "3a153bc0efe540e3963672d164dd4e08",
            "661752b4a62f496091df1eb481dfb2c4",
            "2a88a86923004dd3b1818eca14ada5a2",
            "7a4e93e98b514d129141aa9e87c5e625",
            "e68fb863947b4f38abefdcaebe571f4f",
            "350c75696595456fba2f2d21b5ae2977",
            "7878dba07e07414c96bb60a452e00d7b",
            "187dfdac58a44d1ca6b5f6513208993a",
            "0c0bbe356539406c87837f1763b6c981",
            "a198ab981e5e42d69cd85034ee0cf383",
            "9e9be35aa25d4f07b0af8a2f7cf0f024",
            "868b4032f39c48f897f6b2122bc66d67",
            "3ef46e0a37e4474280e98b32c11614c7",
            "266301831422459fb14c6b80a74bde93",
            "e58f369b96f54b9e86b13c16270eb033",
            "e9080fccbbe7476a9d562c1cd5166ad3",
            "2d6d7f89915947c1aebaded1d4fef351",
            "a8591251b8c44011a5cc82098957e448",
            "96e45c20b577460fbb1c6bf333713dc9",
            "7492457c1e0841438e96420c2f60e028",
            "22563f8216394a7baf4781e85b3d76e1",
            "27f660e6c3fa49b0bb6a5f0f774f56c7",
            "6fba63dfc2b4410bb9f72c5f01842752",
            "7f90db41ecde413fb9d54299c21b7b13",
            "45938068781743e0bd0d303be934dc64",
            "e57cace810b84f068b8871b3b361004b",
            "d81434e941ad4abc97570bd4c1bf4314",
            "9ee5d91f61304656a340a534157337e2",
            "fd1914e460d849d3bb4a701b783ab184",
            "3837584b19e54ddd980c0ac30501b624",
            "bb789b8f7d464b71b97ed8d795e73b76",
            "b2b11e66c36d4b9d8f761fbf9177a966",
            "9fe340919971436e8647b509bb059e6c",
            "4625079ee6094e15b15165088da9a01e",
            "6c7d5d04ac8845bf9c81f81e6e21931f",
            "fdf9eec965f64b7496e69a5ce850568f",
            "fd8fa08ed15c4f189b85033b043b800d",
            "f0dfc0d9d44f4aa78f22f24c08763950",
            "11151c0abf224f83a9947b8a977033f7",
            "ae9dd7008bf340999e61dbf49448efb3",
            "390cfe34b5bd4ce9a3825ba02ddd422b",
            "c97e6bfceca5439dbf808eb2f4d080a4",
            "5176af9dd0024b918786bb3cae47c436",
            "e4196eaee97d4d5bba73637f42360396",
            "e6d26e973d9c4f36b21d206351b2cdb2",
            "d9b785ec3f2e44f79ef31dbbddc75ee1",
            "34f91ba9fecb49baa851496eff9a3bac",
            "6338b68301264d70a125a87aa998ebf9",
            "218bb6538f9c405bad356b56132bdc13",
            "eac8aae02fb6437f9f53d4ac994a2189",
            "ca4fa4e937a1412c8eb85d1155592fc2",
            "7ba5fdd0cc4b422d8512303b693fdd0b",
            "ebc4a68fc10c4926936737dbe77cf7be",
            "99a8925382ff491e83abed4d2b375682",
            "7d401872910a4375a625233c56510c91",
            "04d0c3fda9dd4996a395b542781e807c",
            "bedbcaf04efe42c6829a36c522454f2e",
            "3e45beca46384e8e93411c37d575dff3",
            "c4158b08b68e490ebfd1a62e14381ea8",
            "7364c55f6907405ebeb14c2687f45afd",
            "0b56ae4ddcad495e97ec933af623b965",
            "012e350d7e27498eaf5c91f042192ce4",
            "f4c86ce7e20b47ec802162238eea6a1b",
            "7343c5f5c5d3480a9afc52fa07b3e5ed",
            "996dc67723584fc0b4bf9bd827334a1c"
          ]
        },
        "id": "WJ1TRX93bFw6",
        "outputId": "1de12354-85ed-4dec-92de-2607ed3e21e4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-427457341.py:13: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  emb = SentenceTransformerEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f23718fa08d4961bbc7b15326608840"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81757c8aa4ef434a96e778de2bf027cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55c8e3f8377d48de9905df7f5f0a987b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27df6fd930c848ac92c9afbe3943ef04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0372025423942d7a1c448741eebf2ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a93a69d63094fe4a80fa8bc847eea67"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e9be35aa25d4f07b0af8a2f7cf0f024"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27f660e6c3fa49b0bb6a5f0f774f56c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fe340919971436e8647b509bb059e6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4196eaee97d4d5bba73637f42360396"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d401872910a4375a625233c56510c91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Baseline Dense]\n",
            "(NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference) MDAgents: An Adaptive Collaboration of LLMs for\n",
            "Medical Decision-Making\n",
            "Yubin Kim1 Chanwoo Park1 Hyewon Jeong1♮ Yik Siu Chan1\n",
            "Xuhai Xu1 Daniel McDuff2 Hyeonhoon Lee3\n",
            "Marzyeh Ghassemi1 Cynthia Breazeal1 Hae Won Park1\n",
            "1Mas\n",
            "(NeurIPS-2024-can-large-language-model-agents-simulate-human-trust-behavior-Paper-Conference) Can Large Language Model Agents Simulate\n",
            "Human Trust Behavior?\n",
            "Chengxing Xie∗1, 11 Canyu Chen∗2\n",
            "Feiran Jia4 Ziyu Ye5 Shiyang Lai5 Kai Shu6 Jindong Gu3 Adel Bibi3 Ziniu Hu7\n",
            "David Jurgens8 James Evans5, 9, 10 Philip H.S. T\n",
            "(NeurIPS-2024-richelieu-self-evolving-llm-based-agents-for-ai-diplomacy-Paper-Conference) Richelieu: Self-Evolving LLM-Based Agents for AI\n",
            "Diplomacy\n",
            "Zhenyu Guan ♢, Xiangyu Kong♣†B, Fangwei Zhong♠†B, Yizhou Wang♡♢\n",
            "♢ Institute for Artificial Intelligence, Peking University\n",
            "♣ College of Computer Science, Beijing\n",
            "\n",
            "[Graph-RAG hops=2]\n",
            "(NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference) Report Generation\n",
            "D.1 Accuracy on entire MedQA 5-options Dataset\n",
            "To provide a comprehensive evaluation of our approach, we conducted experiments on the entire\n",
            "MedQA 5-options dataset using GPT-4o mini.\n",
            "(NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference) 23\n",
            "D.3 Impact of Knowledge Enhancement with RAG\n",
            "Method Accuracy (%)\n",
            "MDAgents (baseline) 71.8\n",
            "+ MedRAG 75.2\n",
            "+ Medical Knowledge Initialization 76.0\n",
            "+ Moderator’s Review 77.6\n",
            "+ Moderator’s Review & MedRAG 80.3\n",
            "Table 7: Impact of knowledge enhancement on MDAgents performance\n",
            "We investigated whether simply assigning roles to agents is sufficient for expert-like performance,\n",
            "and explored the impact of equipping agents with different knowledge using Retrieval-Augmented\n",
            "Generation (RAG).\n",
            "(NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference) Method\n",
            "Medical Knowledge Retrieval Datasets\n",
            "MedQA\n",
            "T\n",
            "PubMedQA\n",
            "T\n",
            "Path-VQA\n",
            "I T\n",
            "PMC-VQA\n",
            "I T\n",
            "MedVidQA\n",
            "V T\n",
            "GPT-3.5 64.0 ±1.6 66.0 ±5.7 - - -\n",
            "GPT-4(V) 88.7 ±4.0 75.0 ±1.0 65.3 ±3.9 56.4 ±4.5 -\n",
            "Gemini-Pro(Vision) 57.4 ±1.8 71.0 ±1.6 72.0 ±2.3 62.2 ±7.6 56.2 ±6.7\n",
            "Method\n",
            "Clinical Reasoning and Diagnostic Datasets\n",
            "DDXPlus\n",
            "T\n",
            "SymCat\n",
            "T\n",
            "JAMA\n",
            "T\n",
            "MedBullets\n",
            "T\n",
            "MIMIC-CXR\n",
            "I T\n",
            "GPT-3.5 62.5 ±6.7 85.7 ±3.0 48.6 ±6.5 55.3 ±4.3 -\n",
            "GPT-4(V) 77.9 ±2.1 93.1 ±1.0 70.9 ±0.3 80.8 ±1.7 55.9 ±9.1\n",
            "Gemini-Pro(Vision) 59.2 ±1.2 65.0 ±6.1 47.0 ±2.2 42.9 ±3.4 48.1 ±5.8\n",
            "* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement\n",
            "* T : text-only, I : image+text, V : video+text\n",
            "* All experiments were tested with 3 random seeds\n",
            "26\n",
            "Table 12: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive settings with increased\n",
            "number of samples (N=100).\n",
            "(NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference) Method\n",
            "Medical Knowledge Retrieval Datasets\n",
            "MedQA\n",
            "T\n",
            "PubMedQA\n",
            "T\n",
            "Path-VQA\n",
            "I T\n",
            "PMC-VQA\n",
            "I T\n",
            "MedVidQA\n",
            "V T\n",
            "GPT-3.5 64.0 ±1.6 66.0 ±5.7 - - -\n",
            "GPT-4(V) 88.7 ±4.0 75.0 ±1.0 65.3 ±3.9 56.4 ±4.5 -\n",
            "Gemini-Pro(Vision) 57.4 ±1.8 71.0 ±1.6 72.0 ±2.3 62.2 ±7.6 56.2 ±6.7\n",
            "Method\n",
            "Clinical Reasoning and Diagnostic Datasets\n",
            "DDXPlus\n",
            "T\n",
            "SymCat\n",
            "T\n",
            "JAMA\n",
            "T\n",
            "MedBullets\n",
            "T\n",
            "MIMIC-CXR\n",
            "I T\n",
            "GPT-3.5 62.5 ±6.7 85.7 ±3.0 48.6 ±6.5 55.3 ±4.3 -\n",
            "GPT-4(V) 77.9 ±2.1 93.1 ±1.0 70.9 ±0.3 80.8 ±1.7 55.9 ±9.1\n",
            "Gemini-Pro(Vision) 59.2 ±1.2 65.0 ±6.1 47.0 ±2.2 42.9 ±3.4 48.1 ±5.8\n",
            "* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement\n",
            "* T : text-only, I : image+text, V : video+text\n",
            "* All experiments were tested with 3 random seeds\n",
            "26\n",
            "Table 12: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive settings with increased\n",
            "number of samples (N=100).\n",
            "(NeurIPS-2024-mdagents-an-adaptive-collaboration-of-llms-for-medical-decision-making-Paper-Conference) Method\n",
            "Medical Knowledge Retrieval Datasets\n",
            "MedQA\n",
            "T\n",
            "PubMedQA\n",
            "T\n",
            "Path-VQA\n",
            "I T\n",
            "PMC-VQA\n",
            "I T\n",
            "MedVidQA\n",
            "V T\n",
            "GPT-3.5 64.0 ±1.6 66.0 ±5.7 - - -\n",
            "GPT-4(V) 88.7 ±4.0 75.0 ±1.0 65.3 ±3.9 56.4 ±4.5 -\n",
            "Gemini-Pro(Vision) 57.4 ±1.8 71.0 ±1.6 72.0 ±2.3 62.2 ±7.6 56.2 ±6.7\n",
            "Method\n",
            "Clinical Reasoning and Diagnostic Datasets\n",
            "DDXPlus\n",
            "T\n",
            "SymCat\n",
            "T\n",
            "JAMA\n",
            "T\n",
            "MedBullets\n",
            "T\n",
            "MIMIC-CXR\n",
            "I T\n",
            "GPT-3.5 62.5 ±6.7 85.7 ±3.0 48.6 ±6.5 55.3 ±4.3 -\n",
            "GPT-4(V) 77.9 ±2.1 93.1 ±1.0 70.9 ±0.3 80.8 ±1.7 55.9 ±9.1\n",
            "Gemini-Pro(Vision) 59.2 ±1.2 65.0 ±6.1 47.0 ±2.2 42.9 ±3.4 48.1 ±5.8\n",
            "* CoT: Chain-of-Thought, SC: Self-Consistency, ER: Ensemble Refinement\n",
            "* T : text-only, I : image+text, V : video+text\n",
            "* All experiments were tested with 3 random seeds\n",
            "26\n",
            "Table 12: Accuracy (%) on Medical benchmarks with Solo/Group/Adaptive settings with increased\n",
            "number of samples (N=100).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-427457341.py:18: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever_dense.get_relevant_documents(query)[:k]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = {\n",
        "    \"timestamp\": datetime.datetime.now().isoformat(timespec=\"seconds\"),\n",
        "    \"python\": platform.python_version(),\n",
        "    \"platform\": platform.platform(),\n",
        "    \"packages\": {\n",
        "        \"networkx\": nx.__version__,\n",
        "        \"numpy\": np.__version__,\n",
        "        \"pandas\": pd.__version__\n",
        "    },\n",
        "    \"notes\": \"Regex-based entity extraction; sentence co-occurrence edges; NetworkX graph; graph neighborhood retrieval.\"\n",
        "}\n",
        "with open(\"env_rag_graph.json\",\"w\") as f: json.dump(env, f, indent=2)\n",
        "print(\"✅ Saved env_rag_graph.json\")\n",
        "\n",
        "run_cfg = {\n",
        "    \"data_dir\": DATA_DIR,\n",
        "    \"corpus_csv\": corpus_csv_path,\n",
        "    \"entity_patterns\": ENTITY_PATTERNS,\n",
        "    \"graph\": {\"nodes\": int(len(G.nodes())), \"edges\": int(len(G.edges()))},\n",
        "    \"retrieval\": {\"hops_default\": 1, \"max_spans_default\": 12},\n",
        "    \"baseline_dense\": {\"enabled\": True, \"k\": 5},\n",
        "}\n",
        "with open(\"rag_graph_run_config.json\",\"w\") as f: json.dump(run_cfg, f, indent=2)\n",
        "print(\"✅ Saved rag_graph_run_config.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCNR1S_6bOkf",
        "outputId": "069a856b-9b68-4eeb-f962-731a2c43fe7a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved env_rag_graph.json\n",
            "✅ Saved rag_graph_run_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "DATA_DIR = \"./data_week6\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "ENT_CSV = os.path.join(DATA_DIR, \"entities.csv\")\n",
        "REL_CSV = os.path.join(DATA_DIR, \"relations.csv\")\n",
        "\n",
        "ents_to_save = ents_df[[\"doc_id\",\"entity\",\"type\",\"start\",\"end\",\"span\"]].copy()\n",
        "rels_to_save = edges_df[[\"doc_id\",\"head\",\"type1\",\"tail\",\"type2\",\"sentence\"]].copy()\n",
        "\n",
        "ents_to_save.to_csv(ENT_CSV, index=False)\n",
        "rels_to_save.to_csv(REL_CSV, index=False)\n",
        "\n",
        "print(f\"✅ Saved entities → {ENT_CSV} ({len(ents_to_save)} rows)\")\n",
        "print(f\"✅ Saved relations → {REL_CSV} ({len(rels_to_save)} rows)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJBcJusSbYnI",
        "outputId": "8bba3024-c568-4542-ca85-63317085eaa0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved entities → ./data_week6/entities.csv (38 rows)\n",
            "✅ Saved relations → ./data_week6/relations.csv (14 rows)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx, os\n",
        "GRAPHML = os.path.join(DATA_DIR, \"graph_week6.graphml\")\n",
        "EDGELIST = os.path.join(DATA_DIR, \"graph_week6.edgelist\")\n",
        "\n",
        "nx.write_graphml(G, GRAPHML)\n",
        "nx.write_edgelist(G, EDGELIST, data=[\"doc_id\",\"sentence\"])\n",
        "print(f\"📦 graph exports saved:\\n- {GRAPHML}\\n- {EDGELIST}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfaD3JqVuEjP",
        "outputId": "6609330c-6734-4eef-9b35-41aad15adb50"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 graph exports saved:\n",
            "- ./data_week6/graph_week6.graphml\n",
            "- ./data_week6/graph_week6.edgelist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQo731BpuUXl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}