{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RRAW1Ks_Xml"
   },
   "source": [
    "# 1) Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers accelerate datasets sentencepiece pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMnWDzzS_eMG"
   },
   "source": [
    "# 2) Imports & device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPBqwCSF_sKd"
   },
   "source": [
    "# 3) Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0648f6b97b54779af4d2ed82935cc07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a42c88d2e54885aca3ebf50d56023a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b147344aa1493da786b801d23103dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0d0036df3748a08d76581fcb9fc5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf349a186fe4e928e139c18f99298f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af28a7b6a1640708103b60793170e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36dcb16e94ec427288f3abd586bde168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20c9e39be7b4fb7bf552de6780a3b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9490a909dac14d0eb198b77e9903a718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11cbca11624842cd906deff7367bbb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b56af459af3480796eb860faef06fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436863489ca94349a0e39d68251311b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57638dfeb5a54b6d8f841ace84828120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fc2dfb17f545ea9b53619f5325f8c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f487743695bf48bb88034834485c1e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/gpt-neo-125M output:\n",
      " Explain what a Multi-Agent AI App is, how agents collaborate, and why building it from scratch without frameworks is unique, in 3 concise sentences.\n",
      "\n",
      "In this post, we’ll cover the best ways to implement AI in a multi-agent AI App, how to do that, and what you can expect from it.\n",
      "\n",
      "The key thing you’ll learn in this post is that AI is all about the ability to be the best at what you do. We’ll cover how to do it and what you can expect from it.\n",
      "\n",
      "AI in a Multi-Agent AI App\n",
      "\n",
      "AI is about the ability to be the best at what you do. We’ll explain how to do it\n",
      "\n",
      "distilgpt2 output:\n",
      " Explain what a Multi-Agent AI App is, how agents collaborate, and why building it from scratch without frameworks is unique, in 3 concise sentences.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Two lightweight, public models\n",
    "model_name_alt = \"EleutherAI/gpt-neo-125M\"  # Small GPT-Neo model\n",
    "model_name_base = \"distilgpt2\"              # Distilled GPT-2 baseline\n",
    "\n",
    "# Load pipelines\n",
    "gen_neo = pipeline(\"text-generation\", model=model_name_alt, device=0 if device == \"cuda\" else -1)\n",
    "gen_distil = pipeline(\"text-generation\", model=model_name_base, device=0 if device == \"cuda\" else -1)\n",
    "\n",
    "# Prompt\n",
    "prompt = \"Explain what a Multi-Agent AI App is, how agents collaborate, and why building it from scratch without frameworks is unique, in 3 concise sentences.\"\n",
    "\n",
    "# Generate outputs\n",
    "output_neo = gen_neo(prompt, max_new_tokens=120, do_sample=True, temperature=0.7, top_p=0.9)[0][\"generated_text\"]\n",
    "print(\"EleutherAI/gpt-neo-125M output:\\n\", output_neo)\n",
    "\n",
    "output_distil = gen_distil(prompt, max_new_tokens=120, do_sample=True, temperature=0.7, top_p=0.9)[0][\"generated_text\"]\n",
    "print(\"\\ndistilgpt2 output:\\n\", output_distil)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_K6oEH1BoTF"
   },
   "source": [
    "When I compared the two models, GPT-Neo-125M gave a longer answer but it didn’t really stick to the prompt. It started repeating itself and even added phrases like “In this post,” which made it sound more like a blog. DistilGPT2, on the other hand, was much shorter and stopped too early without really explaining the idea fully. From this, I noticed that smaller models often either add random filler or cut off before finishing. Both were fast, but the quality of the answers wasn’t very reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYCNHztlBtam"
   },
   "source": [
    "# 5) Decoding parameter experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Variant 1 | temp=0.2 top_p=0.95 top_k=50 ---\n",
      "Give 3 short tips for writing reproducible data science code:\n",
      "\n",
      "1. Write a data science code that is easy to read and understand.\n",
      "\n",
      "2. Write a data science code that is easy to understand and understand.\n",
      "\n",
      "3. Write a data science code that is easy to read and understand.\n",
      "\n",
      "4. Write a data science code that is easy to understand and understand.\n",
      "\n",
      "5. Write a data science code that is easy to read and understand.\n",
      "\n",
      "6. Write a data science code that is easy to understand and\n",
      "\n",
      "--- Variant 2 | temp=0.8 top_p=0.9 top_k=50 ---\n",
      "Give 3 short tips for writing reproducible data science code:\n",
      "\n",
      "1) Be careful and be flexible. When writing a new data science code, be flexible, so that the compiler and compiler-base team can help you.\n",
      "\n",
      "2) Be honest. If you're writing a data science code that is not reproducible, that code may be a lot of work. It's best to keep your code to the point that it can be reproducible, so that you can write your code to test your code in a reproducible way.\n",
      "\n",
      "\n",
      "\n",
      "--- Variant 3 | temp=1.1 top_p=0.85 top_k=50 ---\n",
      "Give 3 short tips for writing reproducible data science code:\n",
      "\n",
      "1) Write your code as follows\n",
      "\n",
      "1. The following will create the 3 rows\n",
      "\n",
      "2. This will give the number of rows of each row, plus\n",
      "\n",
      "3. The 2nd row.\n",
      "\n",
      "3. The 3rd row, containing the 3rd line of the file.\n",
      "\n",
      "4. The 3rd row will contain the 3rd line of the file.\n",
      "\n",
      "5. The 3rd line will contain the data that is written to\n",
      "\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "base_prompt = \"Give 3 short tips for writing reproducible data science code:\"\n",
    "settings = [\n",
    "    {\"temperature\": 0.2, \"top_p\": 0.95, \"top_k\": 50},\n",
    "    {\"temperature\": 0.8, \"top_p\": 0.9, \"top_k\": 50},\n",
    "    {\"temperature\": 1.1, \"top_p\": 0.85, \"top_k\": 50},\n",
    "]\n",
    "\n",
    "for i, s in enumerate(settings, 1):\n",
    "    out = gen_neo(base_prompt, max_new_tokens=100, do_sample=True,\n",
    "                   temperature=s[\"temperature\"], top_p=s[\"top_p\"], top_k=s[\"top_k\"],\n",
    "                   pad_token_id=gen_neo.tokenizer.eos_token_id)[0][\"generated_text\"]\n",
    "    print(f\"\\n--- Variant {i} | temp={s['temperature']} top_p={s['top_p']} top_k={s['top_k']} ---\")\n",
    "    print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMl59_i-Ckp8"
   },
   "source": [
    "**Explanation of Decoding Parameters**\n",
    "\n",
    "I saw that temperature mainly controls the randomness of the text. With low temperature like 0.2, the model kept repeating the same line, while with high temperature like 1.1 it became too random and gave extra lines that were not needed. Top-p decides how much of the probability space is used, so lower value makes the model more safe, and higher value makes it more open and creative. Top-k fixes how many word options the model can pick from, which helps in keeping the text focused. In my outputs, low values gave safe but boring tips, while higher values gave more variety but less reliable answers. I would keep lower values when I want correct and stable outputs, and higher ones when I just want creative ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SVUfcd6C3sE"
   },
   "source": [
    "# 6) Hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Examples of Hallucinations:\n",
      "1. Multi-Agent AI Apps were first created by Google in 2010 for healthcare.\n",
      "2. Every Multi-Agent system always has a blockchain layer for security.\n"
     ]
    }
   ],
   "source": [
    "hallucination_examples = [\n",
    "    \"Multi-Agent AI Apps were first created by Google in 2010 for healthcare.\",\n",
    "    \"Every Multi-Agent system always has a blockchain layer for security.\"\n",
    "]\n",
    "\n",
    "print(\"\\n# Examples of Hallucinations:\")\n",
    "for i, example in enumerate(hallucination_examples, 1):\n",
    "    print(f\"{i}. {example}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1Tfw2XODGVU"
   },
   "source": [
    "When I tested the model, I saw that sometimes it gave information that was not true. For example, it said Multi-Agent AI Apps were created by Google in 2010, which is completely wrong. It also claimed that every Multi-Agent system always has a blockchain layer, which is not correct. These kinds of mistakes are called hallucinations and can mislead users if we trust them blindly. To reduce this, we can use real data sources or retrieval grounding so the model is not just guessing. Another way is to keep the temperature lower or add a fact-checking step before showing the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63-J27uKJKz5"
   },
   "source": [
    "# 7) Minimal Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3ddfa053e74caa9da930ad1644dac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c560c597d1464a629987060e6bcfc587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b6ebb0aed14523a26c90e74a5c12ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad1c1a11c5244e8b0fa48947702c200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf87c4649db4fd4af16dd26e4daa53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb69100e87bf4ba79d8ce1abb3158688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56a6ca806864b3fa47cf96f5db3dcc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: In one sentence, what is transfer learning?\n",
      "Assistant: Transfer learning involves using a pre-trained model as the starting point for training a new model. This allows the original model to be reused in a new task without the need for extensive retraining. It helps speed up the development process and reduces the computational cost of the new model compared to building a new one from scratch.\n",
      "\n",
      "User: Name two risks when fine-tuning small LLMs on tiny datasets.\n",
      "Assistant: When fine-tuning small language models (LLMs) on tiny datasets, there are several potential risks that should be considered:\n",
      "\n",
      "1. Overfitting: If the model is not trained properly, it may overfit the tiny dataset and perform poorly on large-scale tasks. This can lead to poor generalization and reduced effectiveness.\n",
      "\n",
      "2. Underfitting: The model may also underfit the tiny dataset, resulting in poor performance even on smaller datasets. This can lead to a worse result than if the\n",
      "\n",
      "User: Suggest one mitigation for each risk.\n",
      "Assistant: One possible mitigation for the risk of overfitting is to use techniques like early stopping or regularization to prevent the model from getting too complex too quickly. Another mitigation could be to increase the number of epochs during training to help avoid overfitting. Additionally, by choosing the right hyperparameters for the model, you can reduce its likelihood of overfitting. Finally, ensuring that the data used to train the model is representative of the actual dataset will help mitigate the risk of underfitting.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "gen_qwen = pipeline(\"text-generation\", model=\"Qwen/Qwen2-0.5B-Instruct\", device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "history = []\n",
    "\n",
    "def build_prompt(history, user_msg):\n",
    "    convo = []\n",
    "    for u, a in history[-3:]:\n",
    "        convo += [f\"User: {u}\", f\"Assistant: {a}\"]\n",
    "    convo.append(f\"User: {user_msg}\\nAssistant:\")\n",
    "    return \"\\n\".join(convo)\n",
    "\n",
    "def chat_once(user_msg, max_new_tokens=100, temperature=0.7, top_p=0.9):\n",
    "    prompt = build_prompt(history, user_msg)\n",
    "    out = gen_qwen(prompt, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, top_p=top_p)[0][\"generated_text\"]\n",
    "    reply = out.split(\"Assistant:\")[-1].strip()\n",
    "    history.append((user_msg, reply))\n",
    "    print(f\"\\nUser: {user_msg}\\nAssistant: {reply}\")\n",
    "\n",
    "chat_once(\"In one sentence, what is transfer learning?\")\n",
    "chat_once(\"Name two risks when fine-tuning small LLMs on tiny datasets.\")\n",
    "chat_once(\"Suggest one mitigation for each risk.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Gg_EQSEKckH"
   },
   "source": [
    "In this small chat loop, I saw that the model could keep some context from the last few turns and answer in a natural way. Unlike the smaller base models, the instruction-tuned model followed my questions more directly and gave proper answers. It was able to explain transfer learning, point out risks, and suggest mitigations in a simple flow. This shows how even a small instruction-tuned model can behave like a chatbot if we manage the history properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEVMARJhKhws"
   },
   "source": [
    "# 8) Batch over prompts + save CSV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: hf_llm_batch_outputs.csv\n"
     ]
    }
   ],
   "source": [
    "# 8) Batch prompts and save\n",
    "import pandas as pd, time\n",
    "\n",
    "prompts = [\n",
    "    \"Write a tweet (<=200 chars) about reproducible ML.\",\n",
    "    \"One sentence: why eval metrics matter beyond accuracy.\",\n",
    "    \"List 3 checks before deploying a model to production.\",\n",
    "    \"Explain temperature vs. top-p to a PM.\"\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for p in prompts:\n",
    "    t0 = time.time()\n",
    "    out = gen_neo(\n",
    "        p,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=gen_neo.tokenizer.eos_token_id\n",
    "    )[0][\"generated_text\"]\n",
    "    rows.append({\n",
    "        \"prompt\": p,\n",
    "        \"output\": out,\n",
    "        \"latency_s\": round(time.time() - t0, 2)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "out_path = \"hf_llm_batch_outputs.csv\"\n",
    "df.to_csv(out_path, index=False)\n",
    "\n",
    "df  # display dataframe\n",
    "print(\"Saved to:\", out_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
